<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Practical MongoDB Aggregations Book</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="Learn about MongoDB Aggregations to develop effective and optimal data manipulation and analytics aggregation pipelines with this book, using the MongoDB Aggregation Framework (aggregate)">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-D0T2GQ8R19"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-D0T2GQ8R19');
        </script>
    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="front-cover.html">Practical MongoDB Aggregations</a></li><li class="chapter-item expanded affix "><a href="credits.html">Credits</a></li><li class="chapter-item expanded affix "><a href="foreword.html">Foreword</a></li><li class="chapter-item expanded affix "><a href="who-this-is-for.html">Who This Book Is For</a></li><li class="chapter-item expanded "><a href="intro/introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="intro/introducing-aggregations.html"><strong aria-hidden="true">1.1.</strong> Introducing MongoDB Aggregations</a></li><li class="chapter-item expanded "><a href="intro/history.html"><strong aria-hidden="true">1.2.</strong> History of MongoDB Aggregations</a></li><li class="chapter-item expanded "><a href="intro/getting-started.html"><strong aria-hidden="true">1.3.</strong> Getting Started</a></li><li class="chapter-item expanded "><a href="intro/getting-help.html"><strong aria-hidden="true">1.4.</strong> Getting Help</a></li></ol></li><li class="chapter-item expanded "><a href="guides/guides.html"><strong aria-hidden="true">2.</strong> Guiding Tips & Principles</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="guides/composibility.html"><strong aria-hidden="true">2.1.</strong> Embrace Composability For Increased Productivity</a></li><li class="chapter-item expanded "><a href="guides/project.html"><strong aria-hidden="true">2.2.</strong> Better Alternatives To A Project Stage</a></li><li class="chapter-item expanded "><a href="guides/explain.html"><strong aria-hidden="true">2.3.</strong> Using Explain Plans</a></li><li class="chapter-item expanded "><a href="guides/performance.html"><strong aria-hidden="true">2.4.</strong> Pipeline Performance Considerations</a></li><li class="chapter-item expanded "><a href="guides/expressions.html"><strong aria-hidden="true">2.5.</strong> Expressions Explained</a></li><li class="chapter-item expanded "><a href="guides/sharding.html"><strong aria-hidden="true">2.6.</strong> Sharding Considerations</a></li><li class="chapter-item expanded "><a href="guides/advanced-arrays.html"><strong aria-hidden="true">2.7.</strong> Advanced Use Of Expressions For Array Processing</a></li></ol></li><li class="chapter-item expanded "><a href="examples/examples.html"><strong aria-hidden="true">3.</strong> Aggregations By Example</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="examples/foundational/foundational.html"><strong aria-hidden="true">3.1.</strong> Foundational Examples</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="examples/foundational/filtered-top-subset.html"><strong aria-hidden="true">3.1.1.</strong> Filtered Top Subset</a></li><li class="chapter-item expanded "><a href="examples/foundational/group-and-total.html"><strong aria-hidden="true">3.1.2.</strong> Group & Total</a></li><li class="chapter-item expanded "><a href="examples/foundational/unpack-array-group-differently.html"><strong aria-hidden="true">3.1.3.</strong> Unpack Arrays & Group Differently</a></li><li class="chapter-item expanded "><a href="examples/foundational/distinct-values.html"><strong aria-hidden="true">3.1.4.</strong> Distinct List Of Values</a></li></ol></li><li class="chapter-item expanded "><a href="examples/joining/joining.html"><strong aria-hidden="true">3.2.</strong> Joining Data Examples</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="examples/joining/one-to-one-join.html"><strong aria-hidden="true">3.2.1.</strong> One-to-One Join</a></li><li class="chapter-item expanded "><a href="examples/joining/multi-one-to-many.html"><strong aria-hidden="true">3.2.2.</strong> Multi-Field Join & One-to-Many</a></li></ol></li><li class="chapter-item expanded "><a href="examples/type-convert/type-convert.html"><strong aria-hidden="true">3.3.</strong> Data Types Conversion Examples</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="examples/type-convert/convert-to-strongly-typed.html"><strong aria-hidden="true">3.3.1.</strong> Strongly-Typed Conversion</a></li><li class="chapter-item expanded "><a href="examples/type-convert/convert-incomplete-dates.html"><strong aria-hidden="true">3.3.2.</strong> Convert Incomplete Date Strings</a></li></ol></li><li class="chapter-item expanded "><a href="examples/trend-analysis/trend-analysis.html"><strong aria-hidden="true">3.4.</strong> Trend Analysis Examples</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="examples/trend-analysis/faceted-classifications.html"><strong aria-hidden="true">3.4.1.</strong> Faceted Classification</a></li><li class="chapter-item expanded "><a href="examples/trend-analysis/largest-graph-network.html"><strong aria-hidden="true">3.4.2.</strong> Largest Graph Network</a></li><li class="chapter-item expanded "><a href="examples/trend-analysis/incremental-analytics.html"><strong aria-hidden="true">3.4.3.</strong> Incremental Analytics</a></li></ol></li><li class="chapter-item expanded "><a href="examples/securing-data/securing-data.html"><strong aria-hidden="true">3.5.</strong> Securing Data Examples</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="examples/securing-data/redacted-view.html"><strong aria-hidden="true">3.5.1.</strong> Redacted View</a></li><li class="chapter-item expanded "><a href="examples/securing-data/mask-sensitive-fields.html"><strong aria-hidden="true">3.5.2.</strong> Mask Sensitive Fields</a></li><li class="chapter-item expanded "><a href="examples/securing-data/role-programmatic-restricted-view.html"><strong aria-hidden="true">3.5.3.</strong> Role Programmatic Restricted View</a></li></ol></li><li class="chapter-item expanded "><a href="examples/time-series/time-series.html"><strong aria-hidden="true">3.6.</strong> Time-Series Examples</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="examples/time-series/iot-power-consumption.html"><strong aria-hidden="true">3.6.1.</strong> IoT Power Consumption</a></li><li class="chapter-item expanded "><a href="examples/time-series/state-change-boundaries.html"><strong aria-hidden="true">3.6.2.</strong> State Change Boundaries</a></li></ol></li><li class="chapter-item expanded "><a href="examples/array-manipulations/array-manipulations.html"><strong aria-hidden="true">3.7.</strong> Array Manipulation Examples</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="examples/array-manipulations/array-high-low-avg.html"><strong aria-hidden="true">3.7.1.</strong> Summarising Arrays For First, Last, Min, Max & Average</a></li><li class="chapter-item expanded "><a href="examples/array-manipulations/pivot-array-items.html"><strong aria-hidden="true">3.7.2.</strong> Pivot Array Items By A Key</a></li><li class="chapter-item expanded "><a href="examples/array-manipulations/array-sort-percentiles.html"><strong aria-hidden="true">3.7.3.</strong> Array Sorting & Percentiles</a></li><li class="chapter-item expanded "><a href="examples/array-manipulations/array-element-grouping.html"><strong aria-hidden="true">3.7.4.</strong> Array Element Grouping</a></li><li class="chapter-item expanded "><a href="examples/array-manipulations/array-fields-joining.html"><strong aria-hidden="true">3.7.5.</strong> Array Fields Joining</a></li><li class="chapter-item expanded "><a href="examples/array-manipulations/comparison-of-two-arrays.html"><strong aria-hidden="true">3.7.6.</strong> Comparison Of Two Arrays</a></li></ol></li><li class="chapter-item expanded "><a href="examples/full-text-search/full-text-search.html"><strong aria-hidden="true">3.8.</strong> Full Text Search Examples</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="examples/full-text-search/compound-text-search.html"><strong aria-hidden="true">3.8.1.</strong> Compound Text Search Criteria</a></li><li class="chapter-item expanded "><a href="examples/full-text-search/facets-and-counts-text-search.html"><strong aria-hidden="true">3.8.2.</strong> Facets And Counts Text Search</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="appendices/appendices.html"><strong aria-hidden="true">4.</strong> Appendices</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="appendices/cheatsheet.html"><strong aria-hidden="true">4.1.</strong> Appendix: Stages Cheatsheet</a></li><li class="chapter-item expanded "><a href="appendices/cheatsheet-source.html"><strong aria-hidden="true">4.2.</strong> Appendix: Stages Cheatsheet Source</a></li><li class="chapter-item expanded "><a href="appendices/create-search-index.html"><strong aria-hidden="true">4.3.</strong> Appendix: Create Atlas Search Index</a></li><li class="chapter-item expanded "><a href="appendices/book-history.html"><strong aria-hidden="true">4.4.</strong> Appendix: Book Version History</a></li></ol></li><li class="chapter-item expanded "><a href="back-cover.html"></a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Practical MongoDB Aggregations Book</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <p><img src="./pics/cover.png" alt="Practical MongoDB Aggregations book front cover" /></p>
<div style="break-before: page; page-break-before: always;"></div><p> </p>
<h1 id="practical-mongodb-aggregations"><a class="header" href="#practical-mongodb-aggregations">Practical MongoDB Aggregations</a></h1>
<p>Author: Paul Done (<a href="https://twitter.com/TheDonester">@TheDonester</a>)</p>
<p>Version: 7.00</p>
<p>Last updated: July 2023    (see <a href="appendices/book-history.html">Book Version History</a>)</p>
<p>MongoDB versions supported: 4.2 → 7.0</p>
<p>Book published at: <a href="https://www.practical-mongodb-aggregations.com">www.practical-mongodb-aggregations.com</a></p>
<p>Content created &amp; assembled at: <a href="https://github.com/pkdone/practical-mongodb-aggregations-book">github.com/pkdone/practical-mongodb-aggregations-book</a></p>
<hr />
<p>Acknowledgements - many thanks to the following people for their valuable feedback:</p>
<ul>
<li>Jake McInteer</li>
<li>John Page</li>
<li>Asya Kamsky</li>
<li>Mat Keep</li>
<li>Brian Leonard</li>
<li>Marcus Eagan</li>
<li>Elle Shwer</li>
<li>Ethan Steininger</li>
<li>Andrew Morgan</li>
</ul>
<hr />
<p>Front cover image adapted from a <a href="https://www.pexels.com/photo/red-steel-pipe-2420294/">Photo by Henry &amp; Co. from Pexels</a> under the <a href="https://www.pexels.com/license/">Pexels License</a> (free to use &amp; modify)</p>
<hr />
<p><a href="https://creativecommons.org/licenses/by-nc-sa/3.0/"><img src="https://img.shields.io/badge/License-CC%20BY--NC--SA%203.0-lightgrey.svg" alt="CC BY-NC-SA 3.0" /></a></p>
<p>This work is licensed under a <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/">Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License</a></p>
<p><a href="https://creativecommons.org/licenses/by-nc-sa/3.0/"><img src="https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png" alt="CC BY-NC-SA 3.0" /></a></p>
<p>Copyright © 2021-2023 MongoDB, Inc.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="foreword"><a class="header" href="#foreword">Foreword</a></h1>
<p><strong>By Asya Kamsky</strong> (<a href="https://twitter.com/asya999">@asya999</a>)</p>
<p>I've been involved with databases since the early 1990's when I &quot;accidentally&quot; got a job with a small database company. For the next two decades, databases were synonymous with SQL in my mind, until someone asked me what I thought about these new &quot;No SQL&quot; databases, and MongoDB in particular. I tried MongoDB for a small project I was doing on the side, and the rest, as they say, is history. </p>
<p>When I joined the company that created MongoDB in early 2012, the query language was simple and straightforward but didn't include options for easy data aggregation, because the general advice was &quot;store the data the way you expect to access the data&quot;, which was a fantastic approach for fast point queries. As time went on, though, it became clear that sometimes you want to answer questions that you didn't know you'd have when you were first designing the application, and the options for that within the database itself were limited. Map-Reduce was complicated to understand and get right, and required writing and running JavaScript, which was inefficient. This led to a new way to aggregate data natively in the server, which was called &quot;The Aggregation Framework&quot;. Since the stages of data processes were organized as a pipeline (familiarly evoking processing files on the Unix command line, for those of us who did such things a lot) we also referred to it as &quot;The Aggregation Pipeline&quot;. Very quickly &quot;Agg&quot; became my favorite feature for its flexibility, power and ease of debugging.</p>
<p>We've come a long away in the last nine years, starting with just seven stages and three dozen expressions operating on a single collection, to where we are now: over thirty stages, including special stages providing input to the pipeline, allowing powerful output from the pipeline, including data from other collections in a pipeline, and over one hundred and fifty expressions, available not just in the aggregation command but also in queries and updates. </p>
<p>The nature of data is such that we will never know up-front all the questions we will have about it in the future, so being able to construct complex queries (aka aggregations) about it is critical to success. While complex data processing can be performed in any programming language you are comfortable with, being able to analyze your data without having to move it from where it's currently stored provides a tremendous advantage over exporting and loading the data elsewhere just to be able to use it for your analytics. </p>
<p>For years, I've given talks about the power of the Aggregation Pipeline, answered questions from users about how to do complex analysis with it, and frequently fielded requests for a comprehensive &quot;Aggregation Cookbook&quot;. Of course it would be great to have a repository of &quot;recipes&quot; with which to solve common data tasks that involve more than a single stage or expression combination, but it's hard to find the time to sit down and write something like that. This is why I was so stoked to see that my colleague, Paul Done, had just written this book and laid the foundation for that cookbook.</p>
<p>I hope you find this collection of suggestions, general principles, and specific pipeline examples useful in your own application development and I look forward to seeing it grow over time to become the cookbook that will help everyone realize the full power of their data.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="who-this-book-is-for"><a class="header" href="#who-this-book-is-for">Who This Book Is For</a></h1>
<p>This book is for developers, architects, data analysts, data engineers and data scientists who have some familiarity with MongoDB and have already acquired a small amount of rudimentary experience using the MongoDB Aggregation Framework. If you do not yet have this entry level knowledge, don't worry because there are plenty of getting started guides out there. If you've never used the Aggregation Framework before, you should start with one or more of the following resources before using this book:</p>
<ul>
<li>The <a href="https://docs.mongodb.com/manual/">MongoDB Manual</a>, and specifically its <a href="https://docs.mongodb.com/manual/aggregation/">Aggregation</a> section</li>
<li>The <a href="https://university.mongodb.com/">MongoDB University</a> free online courses, and specifically the <a href="https://learn.mongodb.com/courses/mongodb-aggregation">MongoDB Aggregation</a> introduction course</li>
<li>The <a href="https://www.oreilly.com/library/view/mongodb-the-definitive/9781491954454/">MongoDB: The Definitive Guide</a> book by Bradshaw, Brazil &amp; Chodorow, and specifically its section <em>7. Introduction to the Aggregation Framework</em></li>
</ul>
<p>This book is not for complete novices, explaining how you should get started on your first MongoDB aggregation pipeline. Neither is this book a comprehensive programming language guide detailing every nuance of the Aggregation Framework and its syntax. This book intends to assist you with two key aspects:</p>
<ol>
<li>Providing a set of opinionated yet easy to digest principles and approaches for increasing your effectiveness in using the Aggregation Framework</li>
<li>Providing a set of examples for building aggregation pipelines to solve various types of data manipulation and analysis challenges</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>This introduction section of the book helps you understand what MongoDB Aggregations are, the philosophy of Aggregations, what people use them for, and the history of the framework.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introducing-mongodb-aggregations"><a class="header" href="#introducing-mongodb-aggregations">Introducing MongoDB Aggregations</a></h1>
<h2 id="what-is-mongodbs-aggregation-framework"><a class="header" href="#what-is-mongodbs-aggregation-framework">What Is MongoDB’s Aggregation Framework?</a></h2>
<p>MongoDB’s Aggregation Framework enables users to send an analytics or data processing workload, written using an aggregation language, to the database to execute the workload against the data it holds. You can think of the Aggregation Framework as having two parts:</p>
<ol>
<li>
<p>The Aggregations API provided by the MongoDB Driver embedded in each application to enable the application to define an aggregation task called a pipeline and send it to the database for the database to process</p>
</li>
<li>
<p>The Aggregation Runtime running in the database to receive the pipeline request from the application and execute the pipeline against the persisted data</p>
</li>
</ol>
<p>The following diagram illustrates these two elements and their inter-relationship:</p>
<p><img src="intro/./pics/aggregation-components.png" alt="MongoDB Aggregation Framework components - Driver API and Database Aggregation Runtime" /></p>
<p>The driver provides APIs to enable an application to use both the MongoDB Query Language (MQL) and the Aggregation Framework. In the database, the Aggregation Runtime reuses the Query Runtime to efficiently execute the query part of an aggregation workload that typically appears at the start of an aggregation pipeline.</p>
<h2 id="what-is-mongodbs-aggregations-language"><a class="header" href="#what-is-mongodbs-aggregations-language">What Is MongoDB's Aggregations Language?</a></h2>
<p>MongoDB's aggregation pipeline language is somewhat of a paradox. It can appear daunting, yet it is straightforward. It can seem verbose, yet it is lean and to the point. It is <a href="https://en.wikipedia.org/wiki/Turing_completeness">Turing complete</a> and able to solve any business problem <strong>*</strong>. Conversely, it is a strongly opinionated <a href="https://en.wikipedia.org/wiki/Domain-specific_language">Domain Specific Language (DSL)</a>, where, if you attempt to veer away from its core purpose of mass data manipulation, it will try its best to resist you.</p>
<blockquote>
<p><strong>*</strong> <em>As <a href="https://twitter.com/MakrOfAdventure">John Page</a> once showed, you can even code a <a href="https://github.com/johnlpage/MongoAggMiner">Bitcoin miner</a> using MongoDB aggregations, not that he (or hopefully anyone for that matter) would ever recommend you do this for real, for both the sake of your bank balance and the environment!</em></p>
</blockquote>
<p>Invariably, for beginners, the Aggregation Framework seems difficult to understand and comes with an initially steep learning curve that you must overcome to become productive. In some programming languages, you only need to master a small set of the language's aspects to be largely effective. With MongoDB aggregations, the initial effort you must invest is slightly greater. However, once mastered, users find it provides an elegant, natural and efficient solution to breaking down a complex set of data manipulations into a series of simple easy to understand steps. This is the point when users achieve the Zen of MongoDB Aggregations, and it is a lovely place to be.</p>
<p>MongoDB's aggregation pipeline language is focused on data-oriented problem-solving rather than business process problem-solving. Depending on how you squint, it can be regarded as a <a href="https://en.wikipedia.org/wiki/Functional_programming">functional programming language</a> rather than a <a href="https://en.wikipedia.org/wiki/Procedural_programming">procedural programming language</a>. Why? Well, an aggregation pipeline is an ordered series of statements, called stages, where the entire output of one stage forms the entire input of the next stage, and so on, with no side effects. This functional nature is probably why many users regard the Aggregation Framework as having a steeper learning curve than many languages. Not because it is inherently more difficult to understand but because most developers come from a procedural programming background and not a functional one. Most developers also have to learn how to think like a functional programmer to learn the Aggregation Framework.</p>
<p>The Aggregation Framework's functional characteristics ultimately make it especially powerful for processing massive data sets. Users focus more on defining &quot;the what&quot; in terms of the required outcome. Users focus less on &quot;the how&quot; of specifying the exact logic to apply to achieve each transformation. You provide one specific and clear advertised purpose for each stage in the pipeline. At runtime, the database engine can then understand the exact intent of each stage. For example, the database engine can obtain clear answers to the questions it asks, such as, &quot;is this stage for performing a filter or is this stage for grouping on some fields?&quot;. With this knowledge, the database engine has the opportunity to optimise the pipeline at runtime. The diagram below shows an example of the database performing a pipeline optimisation. It may decide to reorder stages to optimally leverage an index whilst ensuring that the output isn't changed. Or, it may choose to execute some steps in parallel against subsets of the data in different shards, reducing response time whilst again ensuring the output is never changed.</p>
<p><img src="intro/./pics/optimise.png" alt="MongoDB Aggregation Framework developer vs database engine optimizations comparison" /></p>
<p>Last and by far least in terms of importance is a discussion about syntax. So far, MongoDB aggregations have been described here as a programming language, which it is (a Domain Specific Language). However, with what syntax is a MongoDB aggregation pipeline constructed? The answer is &quot;it depends&quot;, and the answer is mostly irrelevant. This book will highlight pipeline examples using MongoDB's Shell and the JavaScript interpreter it runs in. The book will express aggregation pipelines using a <a href="https://en.wikipedia.org/wiki/JSON">JSON</a> based syntax. However, if you are using one of the many <a href="https://docs.mongodb.com/drivers/">programming language drivers</a> that MongoDB provides, you will be using that language to construct an aggregation pipeline, not JSON. An aggregation is specified as an array of objects, regardless of how the programming language may facilitate this. This programmatic rather than textual format has a couple of advantages compared to querying with a string. It has a low vulnerability to <a href="https://en.wikipedia.org/wiki/SQL_injection">injection attacks</a>, and it is highly <a href="https://en.wikipedia.org/wiki/Composability">composable</a>.</p>
<h2 id="whats-in-a-name"><a class="header" href="#whats-in-a-name">What's In A Name?</a></h2>
<p>You might have realised by now that there doesn't seem to be one single name for the subject of this book. You will often hear:</p>
<ul>
<li>Aggregation</li>
<li>Aggregations</li>
<li>Aggregation Framework</li>
<li>Aggregation Pipeline</li>
<li>Aggregation Pipelines</li>
<li>Aggregation Language</li>
<li>Agg</li>
<li><em>...and so on</em></li>
</ul>
<p>The reality is that any of these names are acceptable, and it doesn't matter which you use. This book uses most of these terms at some point. Just take it as a positive sign that this MongoDB capability (and its title) was not born in a marketing boardroom. It was built by database engineers, for data engineers, where the branding was an afterthought at best!</p>
<h2 id="what-do-people-use-the-aggregation-framework-for"><a class="header" href="#what-do-people-use-the-aggregation-framework-for">What Do People Use The Aggregation Framework For?</a></h2>
<p>The Aggregation Framework is versatile and used for many different data processing and manipulation tasks. Some typical example uses are for:</p>
<ul>
<li>Real-time analytics</li>
<li>Report generation with roll-ups, sums &amp; averages</li>
<li>Real-time dashboards</li>
<li>Redacting data to present via views</li>
<li>Joining data together from different collections on the &quot;server-side&quot;</li>
<li>Data science, including data discovery and data wrangling</li>
<li>Mass data analysis at scale (a la &quot;<a href="https://en.wikipedia.org/wiki/Big_data">big data</a>&quot;)</li>
<li>Real-time queries where deeper &quot;server-side&quot; data post-processing is required than provided by the MongoDB Query Language (<a href="https://docs.mongodb.com/manual/crud/">MQL</a>)</li>
<li>Copying and transforming subsets of data from one collection to another</li>
<li>Navigating relationships between records, looking for patterns</li>
<li>Data masking to obfuscate sensitive data</li>
<li>Performing the Transform (T) part of an Extract-Load-Transform (<a href="https://en.wikipedia.org/wiki/Extract,_load,_transform">ELT</a>) workload</li>
<li>Data quality reporting and cleansing</li>
<li>Updating a materialised view with the results of the most recent source data changes</li>
<li>Performing full-text search (using MongoDB's <a href="https://www.mongodb.com/docs/atlas/atlas-search/">Atlas Search</a>)</li>
<li>Representing data ready to be exposed via SQL/ODBC/JDBC (using MongoDB's <a href="https://docs.mongodb.com/bi-connector/">BI Connector</a>)</li>
<li>Supporting machine learning frameworks for efficient data analysis (e.g. via MongoDB's <a href="https://docs.mongodb.com/spark-connector">Spark Connector</a>)</li>
<li><em>...and many more</em></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="history-of-mongodb-aggregations"><a class="header" href="#history-of-mongodb-aggregations">History Of MongoDB Aggregations</a></h1>
<h2 id="the-emergence-of-aggregations"><a class="header" href="#the-emergence-of-aggregations">The Emergence Of Aggregations</a></h2>
<p>MongoDB's developers released the first major version of the database (version 1.0) in February 2009. Back then, both users and the predominant company behind the database, <a href="https://en.wikipedia.org/wiki/MongoDB_Inc.">MongoDB Inc.</a> (called <em>10gen</em> at the time) were still establishing the sort of use cases that the database would excel at and where the critical gaps were. Within half a year of this first major release, MongoDB's engineering team had identified a need to enable materialised views to be generated on-demand. Users needed this capability to maintain counts, sums, and averages for their real-time client applications to query. In December 2009, in time for the following major release (1.2), the database engineers introduced a quick tactical solution to address this gap. This solution involved embedding a JavaScript engine in the database and allowing client applications to submit and execute &quot;server-side&quot; logic using a simple <a href="https://docs.mongodb.com/manual/core/map-reduce/">Map-Reduce</a> API.</p>
<p>A <a href="https://en.wikipedia.org/wiki/MapReduce">Map-Reduce</a> workload essentially does two things. Firstly it scans the data set, looking for the matching subset of records required for the given scenario. This phase may also transform or exclude the fields of each record. This is the &quot;map&quot; action. Secondly, it condenses the subset of matched data into grouped, totalled, and averaged result summaries. This is the &quot;reduce&quot; action. Functionally, MongoDB's <em>Map-Reduce</em> capability provides a solution to users' typical data processing requirements, but it comes with the following drawbacks:</p>
<ol>
<li>The database has to bolt in an inherently slow JavaScript engine to execute users' Map-Reduce code.</li>
<li>Users have to provide two sets of JavaScript logic, a <em>map</em> (or matching) function and a <em>reduce</em> (or grouping) function. Neither is very intuitive to develop, lacking a solid data-oriented bias.</li>
<li>At runtime, the lack of ability to explicitly associate a specific intent to an arbitrary piece of logic means that the database engine has no opportunity to identify and apply optimisations. It is hard for it to target indexes or reorder some logic for more efficient processing. The database has to be conservative, executing the workload with minimal concurrency and employing locks at various times to prevent race conditions and inconsistent results.</li>
<li>If returning the response to the client application, rather than sending the output to a collection, the response payload must be less than 16MB.</li>
</ol>
<p>Over the following two years, as user behaviour with Map-Reduce became more understood, MongoDB engineers started to envision a better solution. Also, users were increasingly trying to use Map-Reduce to perform mass data processing given MongoDB's ability to hold large data sets. They were hitting the same Map-Reduce limitations. Users desired a more targeted capability leveraging a data-oriented Domain Specific Language (DSL). The engineers saw how to deliver a framework enabling a developer to define a series of data manipulation steps with valuable composability characteristics. Each step would have a clear advertised intent, allowing the database engine to apply optimisations at runtime. The engineers could also design a framework that would execute &quot;natively&quot; in the database and not require a JavaScript engine. In August 2012, this solution, called the Aggregation Framework, was introduced in the 2.2 version of MongoDB. MongoDB's Aggregation Framework provided a far more powerful, efficient, scalable and easy to use replacement to Map-Reduce.</p>
<p>Within its first year, the Aggregation Framework rapidly became the go-to tool for processing large volumes of data in MongoDB. Now, a decade on, it is like the Aggregation Framework has always been part of MongoDB. It feels like part of the database's core DNA. MongoDB still supports Map-Reduce, but developers rarely use it nowadays. MongoDB aggregation pipelines are always the correct answer for processing data in the database!</p>
<blockquote>
<p><em>It is not widely known, but MongoDB's engineering team re-implemented the Map-Reduce &quot;back-end&quot; in MongoDB 4.4 to execute within the aggregation's runtime. They had to develop additional aggregation stages and operators to fill some gaps. For the most part, these are internal-only stages or operators that the Aggregation Framework does not surface for developers to use in regular aggregations. The two exceptions are the new <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/function/"><code>$function</code></a> and <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/accumulator/"><code>$accumulator</code></a> 4.4 operators, which the refactoring work influenced and which now serve as two helpful operators for use in any aggregation pipeline. In MongoDB 4.4, each Map-Reduce &quot;aggregation&quot; still uses JavaScript for certain phases, and so it will not achieve the performance of a native aggregation for an equivalent workload. Nor does this change magically address the other drawbacks of Map-Reduce workloads concerning composability, concurrency, scalability and opportunities for runtime optimisation. The primary purpose of the change was for the database engineers to eliminate redundancy and promote resiliency in the database's codebase. MongoDB version 5.0 deprecated Map-Reduce, and it is likely to be removed in a future version of MongoDB.</em></p>
</blockquote>
<h2 id="key-releases--capabilities"><a class="header" href="#key-releases--capabilities">Key Releases &amp; Capabilities</a></h2>
<p>Below is a summary of the evolution of the Aggregation Framework in terms of significant capabilities added in each major release:</p>
<ul>
<li><strong>MongoDB 2.2 (August 2012)</strong>: Initial Release</li>
<li><strong>MongoDB 2.4 (March 2013)</strong>: Efficiency improvements (especially for sorts), a concat operator</li>
<li><strong>MongoDB 2.6 (April 2014)</strong>: Unlimited size result sets, explain plans, spill to disk for large sorts, an option to output to a new collection, a redact stage</li>
<li><strong>MongoDB 3.0 (March 2015)</strong>: Date-to-string operators</li>
<li><strong>MongoDB 3.2 (December 2015)</strong>: Sharded cluster optimisations, lookup (join) &amp; sample stages, many new arithmetic &amp; array operators</li>
<li><strong>MongoDB 3.4 (November 2016)</strong>: Graph-lookup, bucketing &amp; facets stages, many new array &amp; string operators </li>
<li><strong>MongoDB 3.6 (November 2017)</strong>: Array to/from object operators, more extensive date to/from string operators, a REMOVE variable</li>
<li><strong>MongoDB 4.0 (July 2018)</strong>: Number to/from string operators, string trimming operators</li>
<li><strong>MongoDB 4.2 (August 2019)</strong>: A merge stage to insert/update/replace records in existing non-sharded &amp; sharded collections, set &amp; unset stages to address the verbosity/rigidity of project stages, trigonometry operators, regular expression operators, Atlas Search integration</li>
<li><strong>MongoDB 4.4 (July 2020)</strong>: A union stage, custom JavaScript operator expressions (function &amp; accumulator), first &amp; last array element operators, string replacement operators, a random number operator</li>
<li><strong>MongoDB 5.0 (July 2021)</strong>: A setWindowFields stage, time-series/window operators, date manipulation operators</li>
<li><strong>MongoDB 6.0 (July 2022)</strong>: Support for lookup &amp; graph-lookup stages joining to sharded collections, new densify, documents &amp; fill stages, new array sorting &amp; linearFill operators, new operators to get a subset of ordered arrays or ordered grouped documents</li>
<li><strong>MongoDB 7.0 (August 2023)</strong>: A user roles system variable for use in pipelines, new median and percentile operators</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h1>
<p>For developing aggregation pipelines effectively, and also to try the examples in the second half of this book, you need the following two elements:</p>
<ol>
<li>A <strong>MongoDB database</strong>, <strong>version 4.2 or greater</strong>, running somewhere which is network accessible from your workstation</li>
<li>A <strong>MongoDB client tool</strong> running on your workstation with which to submit aggregation execution requests and to view the results</li>
</ol>
<p>Note that each example aggregation pipeline shown in the second major part of this book is marked with the minimum version of MongoDB that you must use to execute the pipeline. A few of the example pipelines use aggregation features that MongoDB introduced in releases following version 4.2.</p>
<h2 id="database"><a class="header" href="#database">Database</a></h2>
<p>The database deployment for you to connect to can be a single server, a replica set or a sharded cluster. You can run this deployment locally on your workstation or remotely on-prem or in the cloud. It doesn't matter which. You need to know the MongoDB URL for connecting to the database and, if authentication is enabled, the credentials required for full read and write access.</p>
<p>If you don't already have access to a MongoDB database, the two most accessible options for running a database for free are:</p>
<ol>
<li><a href="https://www.mongodb.com/cloud/atlas">Provision a Free Tier MongoDB Cluster</a> in MongoDB Atlas, which is MongoDB Inc.'s cloud-based Database-as-a-Service (once deployed, in the Atlas Console, there is a button you can click to copy the URL of the cluster)</li>
<li><a href="https://docs.mongodb.com/guides/server/install/">Install and run a MongoDB single server</a> locally on your workstation</li>
</ol>
<p>Note that the aggregation pipelines in the <a href="intro/../examples/full-text-search/full-text-search.html">Full-Text Search Examples</a> section leverage <a href="https://www.mongodb.com/docs/atlas/atlas-search/atlas-search-overview/">Atlas Search</a>. Consequently, you must use Atlas for your database deployment if you want to run those examples.</p>
<h2 id="client-tool"><a class="header" href="#client-tool">Client Tool</a></h2>
<p>There are many options for the client tool, some of which are:</p>
<ol>
<li><strong><em>Modern</em> Shell</strong>. Install the modern version of MongoDB's command-line tool, the <a href="https://www.mongodb.com/try/download/shell">MongoDB Shell</a>: <code>mongosh</code></li>
<li><strong><em>Legacy</em> Shell</strong>. Install the legacy version of MongoDB's command-line tool, the <a href="https://docs.mongodb.com/manual/mongo/">Mongo Shell</a>: <code>mongo</code> (you will often find this binary bundled with a MongoDB database installation)</li>
<li><strong>VS Code</strong>. <a href="https://www.mongodb.com/docs/mongodb-vscode/install/">Install MongoDB for VS Code</a>, and use the <a href="https://www.mongodb.com/docs/mongodb-vscode/playgrounds/">Playgrounds</a> feature</li>
<li><strong>Compass</strong>. Install the <em>official</em> MongoDB Inc. provided graphical user interface (GUI) tool, <a href="https://www.mongodb.com/products/compass">MongoDB Compass</a></li>
<li><strong>Studio 3T</strong>. Install the <em>3rd party</em> 3T Software Labs provided graphical user interface (GUI) tool, <a href="https://studio3t.com/download/">Studio 3T</a></li>
</ol>
<p>The book's examples present code in such a way to make it easy to copy and paste into MongoDB's Shell (<code>mongosh</code> or <code>mongo</code>) to execute. All subsequent instructions in this book assume you are using the Shell. However, you will find it straightforward to use one of the mentioned GUI tools instead to consume the code examples. Of the two Shell versions, is it is easier to use and view results with the <em>modern</em> Shell.</p>
<h3 id="mongodb-shell-with-atlas-database"><a class="header" href="#mongodb-shell-with-atlas-database">MongoDB Shell With Atlas Database</a></h3>
<p>Here is an example of how you can start the <em>modern</em> Shell to connect to an Atlas Free Tier MongoDB Cluster (change the text <code>mongosh</code> to <code>mongo</code> if you are using the <em>legacy</em> Shell):</p>
<pre><code class="language-bash">mongosh &quot;mongodb+srv://mycluster.a123b.mongodb.net/test&quot; --username myuser
</code></pre>
<p>Note before running the command above, ensure:</p>
<ol>
<li>You have <a href="https://docs.atlas.mongodb.com/security/add-ip-address-to-list/">added your workstation's IP address</a> to the Atlas Access List</li>
<li>You have <a href="https://docs.atlas.mongodb.com/tutorial/create-mongodb-user-for-cluster/">created a database user</a> for the deployed Atlas cluster, with rights to create, read and write to any database</li>
<li>You have changed the dummy URL and username text, shown in the above example command, to match your real cluster's details (these details are accessible via the cluster's <code>Connect</code> button in the Atlas Console)</li>
</ol>
<h3 id="mongodb-shell-with-local-database"><a class="header" href="#mongodb-shell-with-local-database">MongoDB Shell With Local Database</a></h3>
<p>Here is an example of how you can start the <em>modern</em> Shell to connect to a MongoDB single server database if you've installed one locally on your workstation (change the text <code>mongosh</code> to <code>mongo</code> if you are using the <em>legacy</em> Shell):</p>
<pre><code class="language-bash">mongosh &quot;mongodb://localhost:27017&quot;
</code></pre>
<h3 id="mongodb-for-vs-code"><a class="header" href="#mongodb-for-vs-code">MongoDB For VS Code</a></h3>
<p>Using the MongoDB <em>playground</em> tool in VS Code, you can quickly prototype queries and aggregation pipelines and then execute them against a MongoDB database with the results shown in an output tab. Below is a screenshot of the playground tool in action:</p>
<p><img src="intro/./pics/vscode.png" alt="MongoDB For VS Code playground for building and testing database aggregation pipelines" /></p>
<h3 id="mongodb-compass-gui"><a class="header" href="#mongodb-compass-gui">MongoDB Compass GUI</a></h3>
<p>MongoDB Compass provides an <em>Aggregation Pipeline Builder</em> tool to assist users in prototyping and debugging aggregation pipelines and exporting them to different programming languages. Below is a screenshot of the aggregation tool in Compass:</p>
<p><img src="intro/./pics/compass.png" alt="MongoDB Compass GUI tool for building database aggregation pipelines" /></p>
<h3 id="studio-3t-gui"><a class="header" href="#studio-3t-gui">Studio 3T GUI</a></h3>
<p>Studio 3T provides an <em>Aggregation Editor</em> tool to help users prototype and debug aggregation pipelines and translate them to different programming languages. Below is a screenshot of the aggregation tool in Studio 3T:</p>
<p><img src="intro/./pics/studio3t.png" alt="Studio 3T GUI tool for building database aggregation pipelines" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-help"><a class="header" href="#getting-help">Getting Help</a></h1>
<p>No one can hold the names and syntax of all the different aggregation stages and operators in their heads. I'd bet even <em>MongoDB Aggregations Royalty</em> (<a href="http://www.kamsky.org/stupid-tricks-with-mongodb">Asya Kamsky</a>) couldn't, although I'm sure she would give it a good go!</p>
<p>The good news is there is no need for you to try to remember all the stages &amp; operators. The MongoDB online documentation provides you with a set of excellent <em>references</em> here:</p>
<ul>
<li>MongoDB Aggregation Pipeline <a href="https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/">Stages reference</a></li>
<li>MongoDB Aggregation Pipeline <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/">Operators reference</a></li>
</ul>
<p>To help you get started with the purpose of each stage in the MongoDB Framework, consult the &quot;cheatsheets&quot; in the appendix of this book:</p>
<ul>
<li>MongoDB Aggregation <a href="intro/../appendices/cheatsheet.html">Stages Cheatsheet</a></li>
<li>MongoDB Aggregation <a href="intro/../appendices/cheatsheet-source.html">Stages Cheatsheet Source Code</a></li>
</ul>
<p>If you are getting stuck with an aggregation pipeline and want some help, an active online community will almost always have the answer. So pose your questions at either:</p>
<ul>
<li>The <a href="https://www.mongodb.com/community/forums/">MongoDB Community Forums</a></li>
<li>Stack Overflow - <a href="https://stackoverflow.com/questions/tagged/mongodb">MongoDB Questions</a></li>
</ul>
<p>You may be asking for just general advice. However, suppose you want to ask for help on a specific aggregation pipeline under development. In that case, you should provide a sample input document, a copy of your current pipeline code (in its JSON syntax format and not a programming language specific format) and an example of the output that you are trying to achieve. If you provide this extra information, you will have a far greater chance of receiving a timely and optimal response.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="guiding-tips--principles"><a class="header" href="#guiding-tips--principles">Guiding Tips &amp; Principles</a></h1>
<p>The following set of chapters provide opinionated yet easy-to-digest principles and approaches for increasing effectiveness, productivity, and performance when developing aggregation pipelines.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="embrace-composability-for-increased-productivity"><a class="header" href="#embrace-composability-for-increased-productivity">Embrace Composability For Increased Productivity</a></h1>
<p>An aggregation pipeline is an ordered series of instructions, called stages. The entire output of one stage forms the whole input of the next stage, and so on, with no side effects. Pipelines exhibit high <a href="https://en.wikipedia.org/wiki/Composability">composability</a> where stages are stateless self-contained components selected and assembled in various combinations (pipelines) to satisfy specific requirements. This composability promotes iterative prototyping, with straightforward testing after each increment.</p>
<p>With MongoDB's aggregations, you can take a complex problem, requiring a complex aggregation pipeline, and break it down into straightforward individual stages, where each step can be developed and tested in isolation first. To better comprehend this composability, it may be helpful to internalise the following visual model.</p>
<p><img src="guides/./pics/pipeline-equivalence.png" alt="Alternatives for MongoDB aggregation pipelines composability" /></p>
<p>Suppose you have two pipelines with one stage in each. After saving the intermediate results by running the first pipeline, you run the second pipeline against the saved intermediate data set. The final result is the same as running a single pipeline containing both stages serially. There is no difference between the two. As a developer, you can reduce the <a href="https://en.wikipedia.org/wiki/Cognitive_load">cognitive load</a> by understanding how a problem can be broken down in this way when building aggregation pipelines. Aggregation pipelines enable you to decompose a big challenge into lots of minor challenges. By embracing this approach of first developing each stage separately, you will find even the most complex challenges become surmountable.</p>
<h2 id="specific-tips-to-promote-composability"><a class="header" href="#specific-tips-to-promote-composability">Specific Tips To Promote Composability</a></h2>
<p>In reality, once most developers become adept at using the Aggregation Framework, they tend not to rely on temporary intermediate data sets whilst prototyping each stage. However, it is still a reasonable development approach if you prefer it. Instead, seasoned aggregation pipeline developers typically comment out one or more stages of an aggregation pipeline when using MongoDB's Shell (or they use the &quot;disable stage&quot; capability provided by the <a href="guides/../intro/getting-started.html#mongodb-compass-gui">GUI tools</a> for MongoDB).</p>
<p>To encourage composability and hence productivity, some of the principles to strive for are:</p>
<ul>
<li>Easy disabling of subsets of stages, whilst prototyping or debugging</li>
<li>Easy addition of new fields to a stage or new stages to a pipeline by performing a copy, a paste and then a modification without hitting cryptic error messages resulting from issues like missing a comma before the added element</li>
<li>Easy appreciation of each distinct stage's purpose, at a glance</li>
</ul>
<p>With these principles in mind, the following is an opinionated list of guidelines for how you should textually craft your pipelines in JavaScript to improve your pipeline development pace:</p>
<ol>
<li>Don't start or end a stage on the same line as another stage</li>
<li>For every field in a stage, and stage in a pipeline, include a trailing comma even if it is currently the last item</li>
<li>Include an empty newline between every stage</li>
<li>For complex stages include a <code>//</code> comment with an explanation on a newline before the stage</li>
<li>To &quot;disable&quot; some stages of a pipeline whilst prototyping or debugging another stage, use the multi-line comment <code>/*</code> prefix and <code>*/</code> suffix</li>
</ol>
<p>Below is an example of a poor pipeline layout if you have followed none of the guiding principles:</p>
<pre><code class="language-javascript">// BAD

var pipeline = [
  {&quot;$unset&quot;: [
    &quot;_id&quot;,
    &quot;address&quot;
  ]}, {&quot;$match&quot;: {
    &quot;dateofbirth&quot;: {&quot;$gte&quot;: ISODate(&quot;1970-01-01T00:00:00Z&quot;)}
  }}//, {&quot;$sort&quot;: {
  //  &quot;dateofbirth&quot;: -1
  //}}, {&quot;$limit&quot;: 2}
];
</code></pre>
<p>Whereas the following is an example of a far better pipeline layout, where you meet all of the guiding principles:</p>
<pre><code class="language-javascript">// GOOD

var pipeline = [
  {&quot;$unset&quot;: [
    &quot;_id&quot;,
    &quot;address&quot;,
  ]},    
    
  // Only match people born on or after 1st January 1970
  {&quot;$match&quot;: {
    &quot;dateofbirth&quot;: {&quot;$gte&quot;: ISODate(&quot;1970-01-01T00:00:00Z&quot;)},
  }},
  
  /*
  {&quot;$sort&quot;: {
    &quot;dateofbirth&quot;: -1,
  }},      
    
  {&quot;$limit&quot;: 2},  
  */
];
</code></pre>
<p>Notice trailing commas are included in the code snippet, at both the end of stage level and end of field level.</p>
<p>It is worth mentioning that some (but not all) developers take an alternative but an equally valid approach to constructing a pipeline. They decompose each stage in the pipeline into different JavaScript variables, where each stage's variable is defined separately, as shown in the example below:</p>
<pre><code class="language-javascript">// GOOD

var unsetStage = {
  &quot;$unset&quot;: [
    &quot;_id&quot;,
    &quot;address&quot;,
  ]};    

var matchStage = {
  &quot;$match&quot;: {
    &quot;dateofbirth&quot;: {&quot;$gte&quot;: ISODate(&quot;1970-01-01T00:00:00Z&quot;)},
  }};

var sortStage = {
   &quot;$sort&quot;: {
    &quot;dateofbirth&quot;: -1,
  }}; 


var limitStage = {&quot;$limit&quot;: 2};
    
var pipeline = [
  unsetStage,
  matchStage,
  sortStage,
  limitStage,
];
</code></pre>
<p>Furthermore, some developers may take additional steps if they do not intend to transfer the prototyped pipeline to a different programming language:</p>
<ul>
<li>They may choose to decompose elements inside a stage into additional JavaScript variables to avoid code &quot;typos&quot;. For instance, to prevent one part of a pipeline incorrectly referencing a field computed earlier in the pipeline due to a misspelling.</li>
<li>They may choose to factor out the generation of some <a href="https://en.wikipedia.org/wiki/Boilerplate_code">boilerplate code</a>, representing a complex set of expressions, from part of a pipeline into a separate JavaScript function. This new function is essentially a <a href="https://en.wikipedia.org/wiki/Macro_(computer_science)">macro</a>. They can then reuse this function from multiple places within the main pipeline's code. Whenever the pipeline invokes this function, the pipeline's body directly embeds the returned boilerplate code. The <a href="guides/../examples/array-manipulations/array-sort-percentiles.html#aggregation-pipeline">Array Sorting &amp; Percentiles</a> chapter, later in this book, provides an example of this approach.</li>
</ul>
<p>In summary, this book is not advocating a multi-variable approach over a single-variable approach when you define a pipeline. The book is just highlighting two highly composable options. Ultimately it is a personal choice concerning which you find most comfortable and productive. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="better-alternatives-to-a-project-stage"><a class="header" href="#better-alternatives-to-a-project-stage">Better Alternatives To A Project Stage</a></h1>
<p>The quintessential tool used in MongoDB's Query Language (MQL) to define or restrict fields to return is a <em>projection</em>. In the MongoDB Aggregation Framework, the analogous facility for specifying fields to include or exclude is the <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/project/"><code>$project</code></a> stage. For many earlier versions of MongoDB, this was the only tool to define which fields to keep or omit. However, <code>$project</code> comes with a few usability challenges: </p>
<ol>
<li>
<p><strong><code>$project</code> is confusing and non-intuitive</strong>. You can only choose to include fields or exclude fields in a single stage, but not both. There is one exception, though, where you can exclude the _id field yet still define other fields to include (this only applies to the _id field). It's as if <code>$project</code> has an identity crisis.</p>
</li>
<li>
<p><strong><code>$project</code> is verbose and inflexible</strong>. If you want to define one new field or revise one field, you will have to name all other fields in the projection to include. If each input record has 100 fields and the pipeline needs to employ a <code>$project</code> stage for the first time, things become tiresome. To include a new 101st field, you now also have to name all the original 100 fields in this new <code>$project</code> stage too. You will find this irritating if you have an evolving data model, where additional new fields appear in some records over time. Because you use a <code>$project</code> for inclusion, then each time a new field appears in the data set, you must go back to the old aggregation pipeline to modify it to name the new field explicitly for inclusion in the results. This is the antithesis of flexibility and agility.</p>
</li>
</ol>
<p>In MongoDB version 4.2, the <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/set/"><code>$set</code></a> and <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/unset/"><code>$unset</code></a> stages were introduced, which, in most cases, are preferable to using <code>$project</code> for declaring field inclusion and exclusion. They make the code's intent much clearer, lead to less verbose pipelines, and, critically, they reduce the need to refactor a pipeline whenever the data model evolves. How this works and guidance on when to use <code>$set</code> &amp; <code>$unset</code> stages is described in the section <em><a href="guides/project.html#when-to-use-set--unset">When To Use Set &amp; Unset</a></em>, further below.</p>
<p>Despite the challenges, though, there are some specific situations where using <code>$project</code> is advantageous over <code>$set</code>/<code>$unset</code>. These situations are described in the section <em><a href="guides/project.html#when-to-use-project">When To Use Project</a></em> further below. </p>
<blockquote>
<p><em>MongoDB version 3.4 addressed some of the disadvantages of <code>$project</code> by introducing a new <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/addFields/"><code>$addFields</code></a> stage, which has the same behaviour as <code>$set</code>. <code>$set</code> came later than <code>$addFields</code>and <code>$set</code> is actually just an alias for <code>$addFields</code>. Both <code>$set</code> and <code>$unset</code> stages are available in modern versions of MongoDB, and their counter purposes are obvious to deduce by their names (<code>$set</code> Vs <code>$unset</code>). The name <code>$addFields</code> doesn't fully reflect that you can modify existing fields rather than just adding new fields. This book prefers <code>$set</code> over <code>$addFields</code> to help promote consistency and avoid any confusion of intent. However, if you are wedded to <code>$addFields</code>, use that instead, as there is no behavioural difference.</em></p>
</blockquote>
<h2 id="when-to-use-set--unset"><a class="header" href="#when-to-use-set--unset">When To Use <code>$set</code> &amp; <code>$unset</code></a></h2>
<p>You should use <code>$set</code> &amp; <code>$unset</code> stages when you need to retain most of the fields in the input records, and you want to add, modify or remove a minority subset of fields. This is the case for most uses of aggregation pipelines.</p>
<p>For example, imagine there is a collection of credit card payment documents similar to the following:</p>
<pre><code class="language-javascript">// INPUT  (a record from the source collection to be operated on by an aggregation)
{
  _id: ObjectId(&quot;6044faa70b2c21f8705d8954&quot;),
  card_name: &quot;Mrs. Jane A. Doe&quot;,
  card_num: &quot;1234567890123456&quot;,
  card_expiry: &quot;2023-08-31T23:59:59.736Z&quot;,
  card_sec_code: &quot;123&quot;,
  card_provider_name: &quot;Credit MasterCard Gold&quot;,
  transaction_id: &quot;eb1bd77836e8713656d9bf2debba8900&quot;,
  transaction_date: ISODate(&quot;2021-01-13T09:32:07.000Z&quot;),
  transaction_curncy_code: &quot;GBP&quot;,
  transaction_amount: NumberDecimal(&quot;501.98&quot;),
  reported: true
}
</code></pre>
<p>Then imagine an aggregation pipeline is required to produce modified versions of the documents, as shown below:</p>
<pre><code class="language-javascript">// OUTPUT  (a record in the results of the executed aggregation)
{
  card_name: &quot;Mrs. Jane A. Doe&quot;,
  card_num: &quot;1234567890123456&quot;,
  card_expiry: ISODate(&quot;2023-08-31T23:59:59.736Z&quot;), // Field type converted from text
  card_sec_code: &quot;123&quot;,
  card_provider_name: &quot;Credit MasterCard Gold&quot;,
  transaction_id: &quot;eb1bd77836e8713656d9bf2debba8900&quot;,
  transaction_date: ISODate(&quot;2021-01-13T09:32:07.000Z&quot;),
  transaction_curncy_code: &quot;GBP&quot;,
  transaction_amount: NumberDecimal(&quot;501.98&quot;),
  reported: true,
  card_type: &quot;CREDIT&quot;                               // New added literal value field
}
</code></pre>
<p>Here, shown by the <code>//</code> comments, there was a requirement to modify each document's structure slightly, to convert the <code>card_expiry</code> text field into a proper date field, and add a new <code>card_type</code> field, set to the value &quot;CREDIT&quot;, for every record.</p>
<p>Naively you might decide to build an aggregation pipeline using a <code>$project</code> stage to achieve this transformation, which would probably look similar to the following:</p>
<pre><code class="language-javascript">// BAD
[
  {&quot;$project&quot;: {
    // Modify a field + add a new field
    &quot;card_expiry&quot;: {&quot;$dateFromString&quot;: {&quot;dateString&quot;: &quot;$card_expiry&quot;}},
    &quot;card_type&quot;: &quot;CREDIT&quot;,        

    // Must now name all the other fields for those fields to be retained
    &quot;card_name&quot;: 1,
    &quot;card_num&quot;: 1,
    &quot;card_sec_code&quot;: 1,
    &quot;card_provider_name&quot;: 1,
    &quot;transaction_id&quot;: 1,
    &quot;transaction_date&quot;: 1,
    &quot;transaction_curncy_code&quot;: 1,
    &quot;transaction_amount&quot;: 1,
    &quot;reported&quot;: 1,                
    
    // Remove _id field
    &quot;_id&quot;: 0,
  }},
]
</code></pre>
<p>As you can see, the pipeline's stage is quite lengthy, and because you use a <code>$project</code> stage to modify/add two fields, you must also explicitly name each other existing field from the source records for inclusion. Otherwise, you will lose those fields during the transformation. Imagine if each payment document has hundreds of possible fields, rather than just ten!</p>
<p>A better approach to building the aggregation pipeline, to achieve the same results, would be to use <code>$set</code> and <code>$unset</code> instead, as shown below:</p>
<pre><code class="language-javascript">// GOOD
[
  {&quot;$set&quot;: {
    // Modified + new field
    &quot;card_expiry&quot;: {&quot;$dateFromString&quot;: {&quot;dateString&quot;: &quot;$card_expiry&quot;}},
    &quot;card_type&quot;: &quot;CREDIT&quot;,        
  }},
  
  {&quot;$unset&quot;: [
    // Remove _id field
    &quot;_id&quot;,
  ]},
]
</code></pre>
<p>This time, when you need to add new documents to the collection of existing payments, which include additional new fields, e.g. <code>settlement_date</code> &amp; <code>settlement_curncy_code</code>, no changes are required. The existing aggregation pipeline allows these new fields to appear in the results automatically. However, when using <code>$project</code>, each time the possibility of a new field arises, a developer must first refactor the pipeline to incorporate an additional inclusion declaration (e.g. <code>&quot;settlement_date&quot;: 1</code>, or <code>&quot;settlement_curncy_code&quot;: 1</code>).</p>
<h2 id="when-to-use-project"><a class="header" href="#when-to-use-project">When To Use <code>$project</code></a></h2>
<p>It is best to use a <code>$project</code> stage when the required shape of output documents is very different from the input documents' shape. This situation often arises when you do not need to include most of the original fields.</p>
<p>This time for the same input payments collection, let us imagine you require a new aggregation pipeline to produce result documents. You need each output document's structure to be very different from the input structure, and you need to retain far fewer original fields, similar to the following:</p>
<pre><code class="language-javascript">// OUTPUT  (a record in the results of the executed aggregation)
{
  transaction_info: { 
    date: ISODate(&quot;2021-01-13T09:32:07.000Z&quot;),
    amount: NumberDecimal(&quot;501.98&quot;)
  },
  status: &quot;REPORTED&quot;
}
</code></pre>
<p>Using <code>$set</code>/<code>$unset</code> in the pipeline to achieve this output structure would be verbose and would require naming all the fields (for exclusion this time), as shown below:</p>
<pre><code class="language-javascript">// BAD
[
  {&quot;$set&quot;: {
    // Add some fields
    &quot;transaction_info.date&quot;: &quot;$transaction_date&quot;,
    &quot;transaction_info.amount&quot;: &quot;$transaction_amount&quot;,
    &quot;status&quot;: {&quot;$cond&quot;: {&quot;if&quot;: &quot;$reported&quot;, &quot;then&quot;: &quot;REPORTED&quot;, &quot;else&quot;: &quot;UNREPORTED&quot;}},
  }},
  
  {&quot;$unset&quot;: [
    // Remove _id field
    &quot;_id&quot;,

    // Must name all other existing fields to be omitted
    &quot;card_name&quot;,
    &quot;card_num&quot;,
    &quot;card_expiry&quot;,
    &quot;card_sec_code&quot;,
    &quot;card_provider_name&quot;,
    &quot;transaction_id&quot;,
    &quot;transaction_date&quot;,
    &quot;transaction_curncy_code&quot;,
    &quot;transaction_amount&quot;,
    &quot;reported&quot;,         
  ]}, 
]
</code></pre>
<p>However, by using <code>$project</code> for this specific aggregation, as shown below, to achieve the same results, the pipeline will be less verbose. The pipeline will have the flexibility of not requiring modification if you ever make subsequent additions to the data model, with new previously unknown fields:</p>
<pre><code class="language-javascript">// GOOD
[
  {&quot;$project&quot;: {
    // Add some fields
    &quot;transaction_info.date&quot;: &quot;$transaction_date&quot;,
    &quot;transaction_info.amount&quot;: &quot;$transaction_amount&quot;,
    &quot;status&quot;: {&quot;$cond&quot;: {&quot;if&quot;: &quot;$reported&quot;, &quot;then&quot;: &quot;REPORTED&quot;, &quot;else&quot;: &quot;UNREPORTED&quot;}},
    
    // Remove _id field
    &quot;_id&quot;: 0,
  }},
]
</code></pre>
<blockquote>
<p><em>Another potential downside can occur when using <code>$project</code> to define field inclusion, rather than using <code>$set</code> (or <code>$addFields</code>). When using <code>$project</code> to declare all required fields for inclusion, it can be easy for you to carelessly specify more fields from the source data than intended. Later on, if the pipeline contains something like a <code>$group</code> stage, this will cover up your mistake. The final aggregation's output will not include the erroneous field in the output. You might ask, &quot;Why is this a problem?&quot;. Well, what happens if you intended for the aggregation to take advantage of a <a href="https://docs.mongodb.com/manual/core/query-optimization/#covered-query">covered index query</a> for the few fields it requires, to avoid unnecessarily accessing the raw documents. In most cases, MongoDB's aggregation engine can track fields' dependencies throughout a pipeline and, left to its own devices, can understand which fields are not required. However, you would be overriding this capability by explicitly asking for the extra field. A common error is to forget to exclude the <code>_id</code> field in the projection inclusion stage, and so it will be included by default. This mistake will silently kill the potential optimisation. If you must use a <code>$project</code> stage, try to use it as late as possible in the pipeline because it is then clear to you precisely what you are asking for as the aggregation's final output. Also, unnecessary fields like <code>_id</code> may already have been identified by the aggregation engine as no longer required, due to the occurrence of an earlier <code>$group</code> stage, for example.</em></p>
</blockquote>
<h2 id="main-takeaway"><a class="header" href="#main-takeaway">Main Takeaway</a></h2>
<p>In summary, you should always look to use <code>$set</code> (or <code>$addFields</code>) and <code>$unset</code> for field inclusion and exclusion, rather than <code>$project</code>. The main exception is if you have an obvious requirement for a very different structure for result documents, where you only need to retain a small subset of the input fields.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="using-explain-plans"><a class="header" href="#using-explain-plans">Using Explain Plans</a></h1>
<p>When using the MongoDB Query Language (MQL) to develop queries, it is important to view the <a href="https://docs.mongodb.com/manual/reference/method/db.collection.explain/">explain plan</a> for a query to determine if you've used the appropriate index and if you need to optimise other aspects of the query or the data model. An explain plan allows you to fully understand the performance implications of the query you have created.</p>
<p>The same applies to aggregation pipelines and the ability to view an <em>explain plan</em> for the executed pipeline. However, with aggregations, an explain plan tends to be even more critical because considerably more complex logic can be assembled and run in the database. There are far more opportunities for performance bottlenecks to occur, requiring optimisation.</p>
<p>The MongoDB database engine will do its best to apply its own <a href="https://docs.mongodb.com/manual/core/aggregation-pipeline-optimization/">aggregation pipeline optimisations</a> at runtime. Nevertheless, there could be some optimisations that only you can make. A database engine should never optimise a pipeline in such a way as to risk changing the functional behaviour and outcome of the pipeline. The database engine doesn't always have the extra context that your brain has, relating to the actual business problem at hand. It may not be able to make some types of judgement calls about what pipeline changes to apply to make it run faster. The availability of an explain plan for aggregations enables you to bridge this gap. It allows you to understand the database engine's applied optimisations and detect further potential optimisations you can manually implement in the pipeline.</p>
<h2 id="viewing-an-explain-plan"><a class="header" href="#viewing-an-explain-plan">Viewing An Explain Plan</a></h2>
<p>To view the explain plan for an aggregation pipeline, you can execute commands such as the following:</p>
<pre><code class="language-javascript">db.coll.explain().aggregate([{&quot;$match&quot;: {&quot;name&quot;: &quot;Jo&quot;}}]);
</code></pre>
<p>In this book, you will already have seen the convention used to firstly define a separate variable for the pipeline, followed by the call to the <code>aggregate()</code> function, passing in the pipeline argument, as shown here:</p>
<pre><code class="language-javascript">db.coll.aggregate(pipeline);
</code></pre>
<p>By adopting this approach, it's easier for you to use the same pipeline definition interchangeably with different commands. Whilst prototyping and debugging a pipeline, it is handy for you to be able to quickly switch from executing the pipeline to instead generating the explain plan for the same defined pipeline, as follows:</p>
<pre><code class="language-javascript">db.coll.explain().aggregate(pipeline);
</code></pre>
<p>As with MQL, there are three different verbosity modes that you can generate an explain plan with, as shown below:</p>
<pre><code class="language-javascript">// QueryPlanner verbosity  (default if no verbosity parameter provided)
db.coll.explain(&quot;queryPlanner&quot;).aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">// ExecutionStats verbosity
db.coll.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">// AllPlansExecution verbosity 
db.coll.explain(&quot;allPlansExecution&quot;).aggregate(pipeline);
</code></pre>
<p>In most cases, you will find that running the <code>executionStats</code> variant is the most informative mode. Rather than showing just the query planner's thought process, it also provides actual statistics on the &quot;winning&quot; execution plan (e.g. the total keys examined, the total docs examined, etc.). However, this isn't the default because it actually executes the aggregation in addition to formulating the query plan. If the source collection is large or the pipeline is suboptimal, it will take a while to return the explain plan result.</p>
<p>Note, the <a href="https://docs.mongodb.com/manual/reference/method/db.collection.aggregate/">aggregate()</a> function also provides a vestigial <code>explain</code> option to ask for an explain plan to be generated and returned. Nonetheless, this is more limited and cumbersome to use, so you should avoid it.</p>
<h2 id="understanding-the-explain-plan"><a class="header" href="#understanding-the-explain-plan">Understanding The Explain Plan</a></h2>
<p>To provide an example, let us assume a shop's data set includes information on each customer and what retail orders the customer has made over the years. The <em>customer orders</em> collection contains documents similar to the following example:</p>
<pre><code class="language-javascript">{
  &quot;customer_id&quot;: &quot;elise_smith@myemail.com&quot;,
  &quot;orders&quot;: [
    {
      &quot;orderdate&quot;: ISODate(&quot;2020-01-13T09:32:07Z&quot;),
      &quot;product_type&quot;: &quot;GARDEN&quot;,
      &quot;value&quot;: NumberDecimal(&quot;99.99&quot;)
    },
    {
      &quot;orderdate&quot;: ISODate(&quot;2020-05-30T08:35:52Z&quot;),
      &quot;product_type&quot;: &quot;ELECTRONICS&quot;,
      &quot;value&quot;: NumberDecimal(&quot;231.43&quot;)
    }
  ]
}
</code></pre>
<p>You've defined an index on the <code>customer_id</code> field. You create the following aggregation pipeline to show the three most expensive orders made by a customer whose ID is <code>tonijones@myemail.com</code>, as shown below:</p>
<pre><code class="language-javascript">var pipeline = [
  // Unpack each order from customer orders array as a new separate record
  {&quot;$unwind&quot;: {
    &quot;path&quot;: &quot;$orders&quot;,
  }},
  
  // Match on only one customer
  {&quot;$match&quot;: {
    &quot;customer_id&quot;: &quot;tonijones@myemail.com&quot;,
  }},

  // Sort customer's purchases by most expensive first
  {&quot;$sort&quot; : {
    &quot;orders.value&quot; : -1,
  }},
  
  // Show only the top 3 most expensive purchases
  {&quot;$limit&quot; : 3},

  // Use the order's value as a top level field
  {&quot;$set&quot;: {
    &quot;order_value&quot;: &quot;$orders.value&quot;,
  }},
    
  // Drop the document's id and orders sub-document from the results
  {&quot;$unset&quot; : [
    &quot;_id&quot;,
    &quot;orders&quot;,
  ]},
];
</code></pre>
<p>Upon executing this aggregation against an extensive sample data set, you receive the following result:</p>
<pre><code class="language-javascript">[
  {
    customer_id: 'tonijones@myemail.com',
    order_value: NumberDecimal(&quot;1024.89&quot;)
  },
  {
    customer_id: 'tonijones@myemail.com',
    order_value: NumberDecimal(&quot;187.99&quot;)
  },
  {
    customer_id: 'tonijones@myemail.com',
    order_value: NumberDecimal(&quot;4.59&quot;)
  }
]
</code></pre>
<p>You then request the <em>query planner</em> part of the explain plan:</p>
<pre><code class="language-javascript">db.customer_orders.explain(&quot;queryPlanner&quot;).aggregate(pipeline);
</code></pre>
<p>The query plan output for this pipeline shows the following (excluding some information for brevity):</p>
<pre><code class="language-javascript">stages: [
  {
    '$cursor': {
      queryPlanner: {
        parsedQuery: { customer_id: { '$eq': 'tonijones@myemail.com' } },
        winningPlan: {
          stage: 'FETCH',
          inputStage: {
            stage: 'IXSCAN',
            keyPattern: { customer_id: 1 },
            indexName: 'customer_id_1',
            direction: 'forward',
            indexBounds: {
              customer_id: [
                '[&quot;tonijones@myemail.com&quot;, &quot;tonijones@myemail.com&quot;]'
              ]
            }
          }
        },
      }
    }
  },
  
  { '$unwind': { path: '$orders' } },
  
  { '$sort': { sortKey: { 'orders.value': -1 }, limit: 3 } },
  
  { '$set': { order_value: '$orders.value' } },
  
  { '$project': { _id: false, orders: false } }
]
</code></pre>
<p>You can deduce some illuminating insights from this query plan:</p>
<ul>
<li>
<p>To optimise the aggregation, the database engine has reordered the pipeline positioning the filter belonging to the <code>$match</code> to the top of the pipeline. The database engine moves the content of <code>$match</code> ahead of the <code>$unwind</code> stage without changing the aggregation's functional behaviour or outcome.</p>
</li>
<li>
<p>The first stage of the database optimised version of the pipeline is an <em>internal</em> <code>$cursor</code> stage, regardless of the order you placed the pipeline stages in. The <code>$cursor</code> <em>runtime</em> stage is always the first action executed for any aggregation. Under the covers, the aggregation engine reuses the MQL query engine to perform a &quot;regular&quot; query against the collection, with a filter based on the aggregation's <code>$match</code> contents. The aggregation runtime uses the resulting query cursor to pull batches of records. This is similar to how a client application with a MongoDB driver uses a query cursor when remotely invoking an MQL query to pull batches. As with a normal MQL query, the regular database query engine will try to use an index if it makes sense. In this case an index is indeed leveraged, as is visible in the embedded <code>$queryPlanner</code> metadata, showing the <code>&quot;stage&quot; : &quot;IXSCAN&quot;</code> element and the index used, <code>&quot;indexName&quot; : &quot;customer_id_1&quot;</code>.</p>
</li>
<li>
<p>To further optimise the aggregation, the database engine has collapsed the <code>$sort</code> and <code>$limit</code> into a single <em>special internal sort stage</em> which can perform both actions in one go. In this situation, during the sorting process, the aggregation engine only has to track the current three most expensive orders in memory. It does not have to hold the whole data set in memory when sorting, which may otherwise be resource prohibitive in many scenarios, requiring more RAM than is available.</p>
</li>
</ul>
<p>You might also want to see the <em>execution stats</em> part of the explain plan. The specific new information shown in <code>executionStats</code>, versus the default of <code>queryPlanner</code>, is identical to the <a href="https://docs.mongodb.com/manual/tutorial/analyze-query-plan/">normal MQL explain plan</a> returned for a regular <code>find()</code> operation. Consequently, for aggregations, similar principles to MQL apply for spotting things like &quot;have I used the optimal index?&quot; and &quot;does my data model lend itself to efficiently processing this query?&quot;.</p>
<p>You ask for the <em>execution stats</em> part of the explain plan:</p>
<pre><code class="language-javascript">db.customer_orders.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<p>Below is a redacted example of the output you will see, highlighting some of the most relevant metadata elements you should generally focus on.</p>
<pre><code class="language-javascript">executionStats: {
  nReturned: 1,
  totalKeysExamined: 1,
  totalDocsExamined: 1,
  executionStages: {
    stage: 'FETCH',
    nReturned: 1,
    works: 2,
    advanced: 1,
    docsExamined: 1,
    inputStage: {
      stage: 'IXSCAN',
      nReturned: 1,
      works: 2,
      advanced: 1,
      keyPattern: { customer_id: 1 },
      indexName: 'customer_id_1',
      direction: 'forward',
      indexBounds: {
        customer_id: [
          '[&quot;tonijones@myemail.com&quot;, &quot;tonijones@myemail.com&quot;]'
        ]
      },
      keysExamined: 1,
    }
  }
}
</code></pre>
<p>Here, this part of the plan also shows that the aggregation uses the existing index. Because <code>totalKeysExamined</code> and <code>totalDocsExamined</code> match, the aggregation fully leverages this index to identify the required records, which is good news. Nevertheless, the targeted index doesn't necessarily mean the aggregation's query part is fully optimised. For example, if there is the need to reduce latency further, you can do some analysis to determine if the index can completely <a href="https://docs.mongodb.com/manual/core/query-optimization/#covered-query">cover the query</a>. Suppose the <em>cursor query</em> part of the aggregation is satisfied entirely using the index and does not have to examine any raw documents. In that case, you will see <code>totalDocsExamined: 0</code> in the explain plan.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pipeline-performance-considerations"><a class="header" href="#pipeline-performance-considerations">Pipeline Performance Considerations</a></h1>
<p>Similar to any programming language, there is a downside if you prematurely optimise an aggregation pipeline. You risk producing an over-complicated solution that doesn't address the performance challenges that will manifest. As described in the previous chapter, <a href="guides/./explain.html">Using Explain Plans</a>, the tool you should use to identify opportunities for optimisation is the <em>explain plan</em>. You will typically use the explain plan during the final stages of your pipeline's development once it is functionally correct.</p>
<p>With all that said, it can still help you to be aware of some guiding principles regarding performance whilst you are prototyping a pipeline. Critically, such guiding principles will be invaluable to you once the aggregation's explain plan is analysed and if it shows that the current pipeline is sub-optimal.</p>
<p>This chapter outlines three crucial tips to assist you when creating and tuning an aggregation pipeline. For sizeable data sets, adopting these principles may mean the difference between aggregations completing in a few seconds versus minutes, hours or even longer.</p>
<h2 id="1-be-cognizant-of-streaming-vs-blocking-stages-ordering"><a class="header" href="#1-be-cognizant-of-streaming-vs-blocking-stages-ordering">1. Be Cognizant Of Streaming Vs Blocking Stages Ordering</a></h2>
<p>When executing an aggregation pipeline, the database engine pulls batches of records from the initial query cursor generated against the source collection. The database engine then attempts to stream each batch through the aggregation pipeline stages. For most types of stages, referred to as <em>streaming stages</em>, the database engine will take the processed batch from one stage and immediately stream it into the next part of the pipeline. It will do this without waiting for all the other batches to arrive at the prior stage. However, two types of stages must block and wait for all batches to arrive and accumulate together at that stage. These two stages are referred to as <em>blocking stages</em> and specifically, the two types of stages that block are:</p>
<ul>
<li><code>$sort</code></li>
<li><code>$group</code> *</li>
</ul>
<blockquote>
<p>* <em>actually when stating <code>$group</code>, this also includes other less frequently used &quot;grouping&quot; stages too, specifically:</em><code>$bucket</code>, <code>$bucketAuto</code>, <code>$count</code>, <code>$sortByCount</code> &amp; <code>$facet</code>  <em>(it's a stretch to call <code>$facet</code> a group stage, but in the context of this topic, it's best to think of it that way)</em></p>
</blockquote>
<p>The diagram below highlights the nature of streaming and blocking stages. Streaming stages allow batches to be processed and then passed through without waiting. Blocking stages wait for the whole of the input data set to arrive and accumulate before processing all this data together.</p>
<p><img src="guides/./pics/streaming-blocking.png" alt="MongoDB aggregation pipeline streaming Vs blocking stages" /></p>
<p>When considering <code>$sort</code> and <code>$group</code> stages, it becomes evident why they have to block. The following examples illustrate why this is the case:</p>
<ol>
<li>
<p><strong><code>$sort</code> blocking example</strong>: A pipeline must sort <em>people</em> in ascending order of <em>age</em>. If the stage only sorts each batch's content before passing the batch on to the pipeline's result, only individual batches of output records are sorted by age but not the whole result set. </p>
</li>
<li>
<p><strong><code>$group</code> blocking example</strong>: A pipeline must group <em>employees</em> by one of two <em>work departments</em> (either the <em>sales</em> or <em>manufacturing</em> departments). If the stage only groups employees for a batch, before passing it on, the final result contains the work departments repeated multiple times. Each duplicate department consists of some but not all of its employees. </p>
</li>
</ol>
<p>These often unavoidable blocking stages don't just increase aggregation execution time by reducing concurrency. If used without careful forethought, the throughput and latency of a pipeline will slow dramatically due to significantly increased memory consumption. The following sub-sections explore why this occurs and tactics to mitigate this.</p>
<h3 id="sort-memory-consumption-and-mitigation"><a class="header" href="#sort-memory-consumption-and-mitigation"><code>$sort</code> Memory Consumption And Mitigation</a></h3>
<p>Used naïvely, a <code>$sort</code> stage will need to see all the input records at once, and so the host server must have enough capacity to hold all the input data in memory. The amount of memory required depends heavily on the initial data size and the degree to which the prior stages can reduce the size. Also, multiple instances of the aggregation pipeline may be in-flight at any one time, in addition to other database workloads. These all compete for the same finite memory. Suppose the source data set is many gigabytes or even terabytes in size, and earlier pipeline stages have not reduced this size significantly. It will be unlikely that the host machine has sufficient memory to support the pipeline's blocking <code>$sort</code> stage.  Therefore, MongoDB enforces that every blocking stage is limited to 100 MB of consumed RAM. The database throws an error if it exceeds this limit.</p>
<p>To avoid the memory limit obstacle, you can set the <code>allowDiskUse:true</code> option for the overall aggregation for handling large result data sets. Consequently, the pipeline's <em>sort</em> operation spills to disk if required, and the 100 MB limit no longer constrains the pipeline. However, the sacrifice here is significantly higher latency, and the execution time is likely to increase by orders of magnitude.</p>
<p>To circumvent the aggregation needing to manifest the whole data set in memory or overspill to disk, attempt to refactor your pipeline to incorporate one of the following approaches (in order of most effective first):</p>
<ol>
<li>
<p><strong>Use Index Sort</strong>. If the <code>$sort</code> stage does not depend on a <code>$unwind</code>, <code>$group</code> or <code>$project</code> stage preceding it, move the <code>$sort</code> stage to near the start of your pipeline to target an index for the sort. The aggregation runtime does not need to perform an expensive in-memory sort operation as a result. The <code>$sort</code> stage won't necessarily be the first stage in your pipeline because there may also be a <code>$match</code> stage that takes advantage of the same index. Always inspect the explain plan to ensure you are inducing the intended behaviour.</p>
</li>
<li>
<p><strong>Use Limit With Sort</strong>. If you only need the first subset of records from the sorted set of data, add a <code>$limit</code> stage directly after the <code>$sort</code> stage, limiting the results to the fixed amount you require (e.g. 10). At runtime, the aggregation engine will collapse the <code>$sort</code> and <code>$limit</code> into a single special internal sort stage which performs both actions together. The in-flight sort process only has to track the ten records in memory, which currently satisfy the executing sort/limit rule. It does not have to hold the whole data set in memory to execute the sort successfully.</p>
</li>
<li>
<p><strong>Reduce Records To Sort</strong>. Move the <code>$sort</code> stage to as late as possible in your pipeline and ensure earlier stages significantly reduce the number of records streaming into this late blocking <code>$sort</code> stage. This blocking stage will have fewer records to process and less thirst for RAM.</p>
</li>
</ol>
<h3 id="group-memory-consumption-and-mitigation"><a class="header" href="#group-memory-consumption-and-mitigation"><code>$group</code> Memory Consumption And Mitigation</a></h3>
<p>Like the <code>$sort</code> stage, the <code>$group</code> stage has the potential to consume a large amount of memory. The aggregation pipeline's 100 MB RAM limit for blocking stages applies equally to the <code>$group</code> stage because it will potentially pressure the host's memory capacity. As with sorting, you can use the pipeline's <code>allowDiskUse:true</code> option to avoid this limit for heavyweight grouping operations, but with the same downsides.</p>
<p>In reality, most grouping scenarios focus on accumulating summary data such as totals, counts, averages, highs and lows, and not itemised data. In these situations, considerably reduced result data sets are produced, requiring far less processing memory than a <code>$sort</code> stage. Contrary to many sorting scenarios, grouping operations will typically demand a fraction of the host's RAM.</p>
<p>To ensure you avoid excessive memory consumption when you are looking to use a <code>$group</code> stage, adopt the following principles:</p>
<ol>
<li>
<p><strong>Avoid Unnecessary Grouping</strong>. This chapter covers this recommendation in far greater detail in the section <em><a href="guides/performance.html#2-avoid-unwinding--regrouping-documents-just-to-process-array-elements">2. Avoid Unwinding &amp; Regrouping Documents Just To Process Array Elements</a></em>.</p>
</li>
<li>
<p><strong>Group Summary Data Only</strong>. If the use case permits it, use the group stage to accumulate things like totals, counts and summary roll-ups only, rather than holding all the raw data of each record belonging to a group. The Aggregation Framework provides a robust set of <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/#accumulators---group-">accumulator operators</a> to help you achieve this inside a <code>$group</code> stage.</p>
</li>
</ol>
<h2 id="2-avoid-unwinding--regrouping-documents-just-to-process-array-elements"><a class="header" href="#2-avoid-unwinding--regrouping-documents-just-to-process-array-elements">2. Avoid Unwinding &amp; Regrouping Documents Just To Process Array Elements</a></h2>
<p>Sometimes, you need an aggregation pipeline to mutate or reduce an array field's content for each record. For example:</p>
<ul>
<li>You may need to add together all the values in the array into a total field</li>
<li>You may need to retain the first and last elements of the array only</li>
<li>You may need to retain only one recurring field for each sub-document in the array </li>
<li>...<em>or numerous other array &quot;reduction&quot; scenarios</em></li>
</ul>
<p>To bring this to life, imagine a retail <code>orders</code> collection where each document contains an array of products purchased as part of the order, as shown in the example below:</p>
<pre><code class="language-javascript">[
  {
    _id: 1197372932325,
    products: [
      {
        prod_id: 'abc12345',
        name: 'Asus Laptop',
        price: NumberDecimal('429.99')
      }
    ]
  },
  {
    _id: 4433997244387,
    products: [
      {
        prod_id: 'def45678',
        name: 'Karcher Hose Set',
        price: NumberDecimal('23.43')
      },
      {
        prod_id: 'jkl77336',
        name: 'Picky Pencil Sharpener',
        price: NumberDecimal('0.67')
      },
      {
        prod_id: 'xyz11228',
        name: 'Russell Hobbs Chrome Kettle',
        price: NumberDecimal('15.76')
      }
    ]
  }
]
</code></pre>
<p>The retailer wants to see a report of all the orders but only containing the expensive products purchased by customers (e.g. having just products priced greater than 15 dollars). Consequently, an aggregation is required to filter out the inexpensive product items of each order's array. The desired aggregation output might be:</p>
<pre><code class="language-javascript">[
  {
    _id: 1197372932325,
    products: [
      {
        prod_id: 'abc12345',
        name: 'Asus Laptop',
        price: NumberDecimal('429.99')
      }
    ]
  },
  {
    _id: 4433997244387,
    products: [
      {
        prod_id: 'def45678',
        name: 'Karcher Hose Set',
        price: NumberDecimal('23.43')
      },
      {
        prod_id: 'xyz11228',
        name: 'Russell Hobbs Chrome Kettle',
        price: NumberDecimal('15.76')
      }
    ]
  }
]
</code></pre>
<p>Notice order <code>4433997244387</code> now only shows two products and is missing the inexpensive product.</p>
<p>One naïve way of achieving this transformation is to <em>unwind</em> the <em>products</em> array of each order document to produce an intermediate set of individual product records. These records can then be <em>matched</em> to retain products priced greater than 15 dollars. Finally, the products can be <em>grouped</em> back together again by each order's <code>_id</code> field. The required pipeline to achieve this is below:</p>
<pre><code class="language-javascript">// SUBOPTIMAL

var pipeline = [
  // Unpack each product from the each order's product as a new separate record
  {&quot;$unwind&quot;: {
    &quot;path&quot;: &quot;$products&quot;,
  }},

  // Match only products valued over 15.00
  {&quot;$match&quot;: {
    &quot;products.price&quot;: {
      &quot;$gt&quot;: NumberDecimal(&quot;15.00&quot;),
    },
  }},

  // Group by product type
  {&quot;$group&quot;: {
    &quot;_id&quot;: &quot;$_id&quot;,
    &quot;products&quot;: {&quot;$push&quot;: &quot;$products&quot;},
  }},
];
</code></pre>
<p>This pipeline is suboptimal because a <code>$group</code> stage has been introduced, which is a blocking stage, as outlined earlier in this chapter. Both memory consumption and execution time will increase significantly, which could be fatal for a large input data set. There is a far better alternative by using one of the <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/#array-expression-operators">Array Operators</a> instead. Array Operators are sometimes less intuitive to code, but they avoid introducing a blocking stage into the pipeline. Consequently, they are significantly more efficient, especially for large data sets. Shown below is a far more economical pipeline, using the <code>$filter</code> array operator, rather than the <code>$unwind/$match/$group</code> combination, to produce the same outcome:</p>
<pre><code class="language-javascript">// OPTIMAL

var pipeline = [
  // Filter out products valued 15.00 or less
  {&quot;$set&quot;: {
    &quot;products&quot;: {
      &quot;$filter&quot;: {
        &quot;input&quot;: &quot;$products&quot;,
        &quot;as&quot;: &quot;product&quot;,
        &quot;cond&quot;: {&quot;$gt&quot;: [&quot;$$product.price&quot;, NumberDecimal(&quot;15.00&quot;)]},
      }
    },
  }},
];
</code></pre>
<p>Unlike the suboptimal pipeline, the optimal pipeline will include &quot;empty orders&quot; in the results for those orders that contained only inexpensive items. If this is a problem, you can include a simple <code>$match</code> stage at the start of the optimal pipeline with the same content as the <code>$match</code> stage shown in the suboptimal example.</p>
<p>To reiterate, there should never be the need to use an <code>$unwind/$group</code> combination in an aggregation pipeline to transform an array field's elements for each document in isolation. One way to recognise this anti-pattern is if your pipeline contains a <code>$group</code> on a <code>$_id</code> field. Instead, use <em>Array Operators</em> to avoid introducing a blocking stage. Otherwise, you will suffer a magnitude increase in execution time when the blocking group stage in your pipeline handles more than 100 MB of in-flight data. Adopting this best practice may mean the difference between achieving the required business outcome and abandoning the whole task as unachievable.</p>
<p>The primary use of an <code>$unwind/$group</code> combination is to correlate patterns across many records' arrays rather than transforming the content within each input record's array only. For an illustration of an appropriate use of <code>$unwind/$group</code> refer to this book's <a href="guides/../examples/foundational/unpack-array-group-differently.html">Unpack Array &amp; Group Differently</a> example.</p>
<h2 id="3-encourage-match-filters-to-appear-early-in-the-pipeline"><a class="header" href="#3-encourage-match-filters-to-appear-early-in-the-pipeline">3. Encourage Match Filters To Appear Early In The Pipeline</a></h2>
<h3 id="explore-if-bringing-forward-a-full-match-is-possible"><a class="header" href="#explore-if-bringing-forward-a-full-match-is-possible">Explore If Bringing Forward A Full Match Is Possible</a></h3>
<p>As discussed, the database engine will do its best to optimise the aggregation pipeline at runtime, with a particular focus on attempting to move the <code>$match</code> stages to the top of the pipeline. Top-level <code>$match</code> content will form part of the filter that the engine first executes as the initial query. The aggregation then has the best chance of leveraging an index. However, it may not always be possible to promote <code>$match</code> filters in such a way without changing the meaning and resulting output of an aggregation.</p>
<p>Sometimes, a <code>$match</code> stage is defined later in a pipeline to perform a filter on a field that the pipeline computed in an earlier stage. The computed field isn't present in the pipeline's original input collection. Some examples are:</p>
<ul>
<li>
<p>A pipeline where a <code>$group</code> stage creates a new <code>total</code> field based on an <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/group/#accumulators-group">accumulator operator</a>. Later in the pipeline, a <code>$match</code> stage filters groups where each group's <code>total</code> is greater than <code>1000</code>. </p>
</li>
<li>
<p>A pipeline where a <code>$set</code> stage computes a new <code>total</code> field value based on adding up all the elements of an array field in each document. Later in the pipeline, a <code>$match</code> stage filters documents where the <code>total</code> is less than <code>50</code>.</p>
</li>
</ul>
<p>At first glance, it may seem like the match on the computed field is irreversibly trapped behind an earlier stage that computed the field's value. Indeed the aggregation engine cannot automatically optimise this further. In some situations, though, there may be a missed opportunity where beneficial refactoring is possible by you, the developer.</p>
<p>Take the following trivial example of a collection of <em>customer order</em> documents:</p>
<pre><code class="language-javascript">[
  {
    customer_id: 'elise_smith@myemail.com',
    orderdate: ISODate('2020-05-30T08:35:52.000Z'),
    value: NumberDecimal('9999')
  }
  {
    customer_id: 'elise_smith@myemail.com',
    orderdate: ISODate('2020-01-13T09:32:07.000Z'),
    value: NumberDecimal('10101')
  }
]
</code></pre>
<p>Let's assume the orders are in a <em>Dollars</em> currency, and each <code>value</code> field shows the order's value in <em>cents</em>. You may have built a pipeline to display all orders where the value is greater than 100 dollars like below:</p>
<pre><code class="language-javascript">// SUBOPTIMAL

var pipeline = [
  {&quot;$set&quot;: {
    &quot;value_dollars&quot;: {&quot;$multiply&quot;: [0.01, &quot;$value&quot;]}, // Converts cents to dollars
  }},
  
  {&quot;$unset&quot;: [
    &quot;_id&quot;,
    &quot;value&quot;,
  ]},

  {&quot;$match&quot;: {
    &quot;value_dollars&quot;: {&quot;$gte&quot;: 100},  // Peforms a dollar check
  }},
];
</code></pre>
<p>The collection has an index defined for the <code>value</code> field (in <em>cents</em>). However, the <code>$match</code> filter uses a computed field, <code>value_dollars</code>. When you view the explain plan, you will see the pipeline does not leverage the index. The <code>$match</code> is trapped behind the <code>$set</code> stage (which computes the field) and cannot be moved to the pipeline's start. MongoDB's aggregation engine tracks a field's dependencies across multiple stages in a pipeline. It can establish how far up the pipeline it can promote fields without risking a change in the aggregation's behaviour. In this case, it knows that if it moves the <code>$match</code> stage ahead of the <code>$set</code> stage, it depends on, things will not work correctly.</p>
<p>In this example, as a developer, you can easily make a pipeline modification that will enable this pipeline to be more optimal without changing the pipeline's intended outcome. Change the <code>$match</code> filter to be based on the source field <code>value</code> instead (greater than <code>10000</code> cents), rather than the computed field (greater than <code>100</code> dollars). Also, ensure the <code>$match</code> stage appears before the <code>$unset</code> stage (which removes the <code>value</code> field). This change is enough to allow the pipeline to run efficiently. Below is how the pipeline looks after you have made  this change:</p>
<pre><code class="language-javascript">// OPTIMAL

var pipeline = [
  {&quot;$set&quot;: {
    &quot;value_dollars&quot;: {&quot;$multiply&quot;: [0.01, &quot;$value&quot;]},
  }},
  
  {&quot;$match&quot;: {                // Moved to before the $unset
    &quot;value&quot;: {&quot;$gte&quot;: 10000},   // Changed to perform a cents check
  }},

  {&quot;$unset&quot;: [
    &quot;_id&quot;,
    &quot;value&quot;,
  ]}, 
];
</code></pre>
<p>This pipeline produces the same data output. However, when you look at its explain plan, it shows the database engine has pushed the <code>$match</code> filter to the top of the pipeline and used an index on the <code>value</code> field. The aggregation is now optimal because the <code>$match</code> stage is no longer &quot;blocked&quot; by its dependency on the computed field.</p>
<h3 id="explore-if-bringing-forward-a-partial-match-is-possible"><a class="header" href="#explore-if-bringing-forward-a-partial-match-is-possible">Explore If Bringing Forward A Partial Match Is Possible</a></h3>
<p>There may be some cases where you can't unravel a computed value in such a manner. However, it may still be possible for you to include an additional <code>$match</code> stage, to perform a <em>partial match</em> targeting the aggregation's query cursor. Suppose you have a pipeline that masks the values of sensitive <code>date_of_birth</code> fields (replaced with computed <code>masked_date</code> fields). The computed field adds a random number of days (one to seven) to each current date. The pipeline already contains a <code>$match</code> stage with the filter <code>masked_date &gt; 01-Jan-2020</code>. The runtime cannot optimise this to the top of the pipeline due to the dependency on a computed value. Nevertheless, you can manually add an extra <code>$match</code> stage at the top of the pipeline, with the filter <code>date_of_birth &gt; 25-Dec-2019</code>. This new <code>$match</code> leverages an index and filters records seven days earlier than the existing <code>$match</code>, but the aggregation's final output is the same. The new <code>$match</code> may pass on a few more records than intended. However, later on, the pipeline applies the existing filter <code>masked_date &gt; 01-Jan-2020</code> that will naturally remove surviving surplus records before the pipeline completes.</p>
<h3 id="pipeline-match-summary"><a class="header" href="#pipeline-match-summary">Pipeline Match Summary</a></h3>
<p>In summary, if you have a pipeline leveraging a <code>$match</code> stage and the explain plan shows this is not moving to the start of the pipeline, explore whether manually refactoring will help. If the <code>$match</code> filter depends on a computed value, examine if you can alter this or add an extra <code>$match</code> to yield a more efficient pipeline.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="expressions-explained"><a class="header" href="#expressions-explained">Expressions Explained</a></h1>
<h2 id="summarising-aggregation-expressions"><a class="header" href="#summarising-aggregation-expressions">Summarising Aggregation Expressions</a></h2>
<p>Expressions give aggregation pipelines their data manipulation power. However, they tend to be something that developers start using by just copying examples from the MongoDB Manual and then refactoring these without thinking enough about what they are. Proficiency in aggregation pipelines demands a deeper understanding of expressions.</p>
<p>Aggregation expressions come in one of three primary flavours:</p>
<ul>
<li>
<p><strong>Operators.</strong> Accessed as an object with a <code>$</code> prefix followed by the operator function name. The &quot;<em>dollar-operator-name</em>&quot; is used as the main key for the object.  Examples:  <code>{$arrayElemAt: ...}</code>, <code>{$cond: ...}</code>, <code>{$dateToString: ...}</code></p>
</li>
<li>
<p><strong>Field Paths.</strong> Accessed as a string with a <code>$</code> prefix followed by the field's path in each record being processed.  Examples: <code>&quot;$account.sortcode&quot;</code>, <code>&quot;$addresses.address.city&quot;</code></p>
</li>
<li>
<p><strong>Variables.</strong> Accessed as a string with a <code>$$</code> prefix followed by the fixed name and falling into three sub-categories:</p>
<ul>
<li>
<p><strong>Context System Variables.</strong> With values coming from the system environment rather than each input record an aggregation stage is processing.  Examples:  <code>&quot;$$NOW&quot;</code>, <code>&quot;$$CLUSTER_TIME&quot;</code></p>
</li>
<li>
<p><strong>Marker Flag System Variables.</strong> To indicate desired behaviour to pass back to the aggregation runtime.  Examples: <code>&quot;$$ROOT&quot;</code>, <code>&quot;$$REMOVE&quot;</code>, <code>&quot;$$PRUNE&quot;</code></p>
</li>
<li>
<p><strong>Bind User Variables.</strong> For storing values you declare with a <code>$let</code> operator (or with the <code>let</code> option of a <code>$lookup</code> stage, or <code>as</code> option of a <code>$map</code> or <code>$filter</code> stage).  Examples: <code>&quot;$$product_name_var&quot;</code>, <code>&quot;$$orderIdVal&quot;</code></p>
</li>
</ul>
</li>
</ul>
<p>You can combine these three categories of aggregation expressions when operating on input records, enabling you to perform complex comparisons and transformations of data. To highlight this, the code snippet below is an excerpt from this book's <a href="guides/../examples/securing-data/mask-sensitive-fields.html">Mask Sensitive Fields</a> example, which combines all three expressions.</p>
<pre><code class="language-javascript">&quot;customer_info&quot;: {&quot;$cond&quot;: {
                    &quot;if&quot;:   {&quot;$eq&quot;: [&quot;$customer_info.category&quot;, &quot;SENSITIVE&quot;]}, 
                    &quot;then&quot;: &quot;$$REMOVE&quot;, 
                    &quot;else&quot;: &quot;$customer_info&quot;,
                 }}
</code></pre>
<p>The pipeline retains an embedded sub-document (<code>customer_info</code>) in each resulting record unless a field in the original sub-document has a specific value (<code>category=SENSITIVE</code>). <code>{$cond: ...}</code> is one of the operator expressions used in the excerpt (a &quot;conditional&quot; operator expression which takes three arguments: <code>if</code>, <code>then</code> &amp; <code>else</code>). <code>{$eq: ...}</code> is another operator expression (a &quot;comparison&quot; operator expression). <code>&quot;$$REMOVE&quot;</code> is a &quot;marker flag&quot; variable expression instructing the pipeline to exclude the field. Both <code>&quot;$customer_info.category&quot;</code> and <code>&quot;$customer_info&quot;</code> elements are field path expressions referencing each incoming record's fields.</p>
<h2 id="what-do-expressions-produce"><a class="header" href="#what-do-expressions-produce">What Do Expressions Produce?</a></h2>
<p>As described above, an expression can be an Operator (e.g. <code>{$concat: ...}</code>), a Variable (e.g. <code>&quot;$$ROOT&quot;</code>) or a Field Path (e.g. <code>&quot;$address&quot;</code>). In all these cases, an expression is just something that dynamically populates and returns a new <a href="https://en.wikipedia.org/wiki/JSON#Data_types">JSON</a>/<a href="https://en.wikipedia.org/wiki/BSON">BSON</a> data type element, which can be one of:</p>
<ul>
<li>a Number  <em>(including integer, long, float, double, decimal128)</em></li>
<li>a String  <em>(UTF-8)</em></li>
<li>a Boolean</li>
<li>a DateTime  <em>(UTC)</em></li>
<li>an Array</li>
<li>an Object</li>
</ul>
<p>However, a specific expression can restrict you to returning just one or a few of these types. For example, the <code>{$concat: ...}</code> Operator, which combines multiple strings, can only produce a <em>String</em> data type (or null). The Variable <code>&quot;$$ROOT&quot;</code> can only return an <em>Object</em> which refers to the root document currently being processed in the pipeline stage. </p>
<p>A Field Path (e.g. <code>&quot;$address&quot;</code>) is different and can return an element of any data type, depending on what the field refers to in the current input document. For example, suppose <code>&quot;$address&quot;</code> references a sub-document. In this case, it will return an <em>Object</em>. However, if it references a list of elements, it will return an <em>Array</em>. As a human, you can guess that the Field Path <code>&quot;$address&quot;</code> won't return a <em>DateTime</em>, but the aggregation runtime does not know this ahead of time. There could be even more dynamics at play. Due to MongoDB's flexible data model, <code>&quot;$address&quot;</code> could yield a different type for each record processed in a pipeline stage. The first record's <code>address</code> may be an <em>Object</em> sub-document with street name and city fields. The second record's <code>address</code> might represent the full address as a single <em>String</em>.</p>
<p>In summary, <em>Field Paths</em> and <em>Bind User Variables</em> are expressions that can return any JSON/BSON data type at runtime depending on their context. For the other kinds of expressions (<em>Operators</em>, <em>Context System Variables</em> and <em>Marker Flag System Variables</em>), the data type each can return is fixed to one or a set number of documented types. To establish the exact data type produced by these specific operators, you need to consult the <a href="https://docs.mongodb.com/manual/meta/aggregation-quick-reference/">Aggregation Pipeline Quick Reference documentation</a>. </p>
<p>For the Operator category of expressions, an expression can also take other expressions as parameters, making them composable. Suppose you need to determine the day of the week for a given date, for example:</p>
<pre><code>{&quot;$dayOfWeek&quot;: ISODate(&quot;2021-04-24T00:00:00Z&quot;)}
</code></pre>
<p>Here the <code>$dayOfWeek</code> Operator expression can only return an element of type <em>Number</em> and takes a single parameter, an element of type <em>DateTime</em>. However, rather than using a hardcoded date-time for the parameter, you could have provided an expression. This could be a <em>Field Path</em> expression, for example:</p>
<pre><code>{&quot;$dayOfWeek&quot;: &quot;$person_details.data_of_birth&quot;}
</code></pre>
<p>Alternatively, you could have defined the parameter using a <em>Context System Variable</em> expression, for example:</p>
<pre><code>{&quot;$dayOfWeek&quot;: &quot;$$NOW&quot;}
</code></pre>
<p>Or you could even have defined the parameter using yet another <em>Operator</em> expression, for example: </p>
<pre><code>{&quot;$dayOfWeek&quot;: {&quot;$dateFromParts&quot;: {&quot;year&quot; : 2021, &quot;month&quot; : 4, &quot;day&quot;: 24}}}
</code></pre>
<p>Furthermore, you could have defined <code>year</code>, <code>month</code> and <code>day</code> parameters for <code>$dateFromParts</code> to be dynamically generated using expressions rather than literal values. The ability to chain expressions together in this way gives your pipelines a lot of power and flexibility when you need it. </p>
<h2 id="can-all-stages-use-expressions"><a class="header" href="#can-all-stages-use-expressions">Can All Stages Use Expressions?</a></h2>
<p>The following question is something you may not have asked yourself before, but asking this question and considering why the answer is what it is can help reveal more about what aggregation expressions are and why you use them.</p>
<p><strong>Question:</strong> Can aggregation expressions be used within any type of pipeline stage?</p>
<p><strong>Answer:</strong> No</p>
<p>There are many types of stages in the Aggregation Framework that don't allow expressions to be embedded. Examples of some of the most commonly used of these stages are:</p>
<ul>
<li><code>$match</code></li>
<li><code>$limit</code></li>
<li><code>$skip</code></li>
<li><code>$sort</code></li>
<li><code>$count</code></li>
<li><code>$lookup</code></li>
<li><code>$out</code></li>
</ul>
<p>Some of these stages may be a surprise to you if you've never really thought about it before. You might well consider <code>$match</code> to be the most surprising item in this list. The content of a <code>$match</code> stage is just a set of query conditions with the same syntax as MQL rather than an aggregation expression. There is a good reason for this. The aggregation engine reuses the MQL query engine to perform a &quot;regular&quot; query against the collection, enabling the query engine to use all its usual optimisations. The query conditions are taken as-is from the <code>$match</code> stage at the top of the pipeline. Therefore, the <code>$match</code> filter must use the same syntax as MQL. </p>
<p>In most of the stages that are unable to leverage expressions, it doesn't usually make sense for their behaviour to be dynamic, based on the pipeline data entering the stage. For a client application that paginates results, you might define a value of <code>20</code> for the <code>$limit</code> stage. However, maybe you want to dynamically bind a value to the <code>$limit</code> stage, sourced by a <code>$lookup</code> stage earlier in the pipeline. The lookup operation might pull in the user's preferred &quot;page list size&quot; value from a &quot;user preferences&quot; collection. Nonetheless, the Aggregation Framework does not support this today for the listed stage types to avoid the overhead of the extra checks it would need to perform for what are essentially rare cases.</p>
<p>In most cases, only one of the listed stages needs to be more expressive: the <code>$match</code> stage, but this stage is already flexible by being based on MQL query conditions. However, sometimes, even MQL isn't expressive enough to sufficiently define a rule to identify records to retain in an aggregation. The remainder of this chapter explores these challenges and how they are solved.</p>
<h2 id="what-is-using-expr-inside-match-all-about"><a class="header" href="#what-is-using-expr-inside-match-all-about">What Is Using <code>$expr</code> Inside <code>$match</code> All About?</a></h2>
<p>The previously stated generalisation about <code>$match</code> not supporting expressions is actually inaccurate. Version 3.6 of MongoDB introduced the <a href="https://docs.mongodb.com/manual/reference/operator/query/expr/"><code>$expr</code></a> operator, which you can embed within a <code>$match</code> stage (or in MQL) to leverage aggregation expressions when filtering records. Essentially, this enables MongoDB's query runtime (which executes an aggregation's <code>$match</code>) to reuse expressions provided by MongoDB's aggregation runtime.</p>
<p>Inside a <code>$expr</code> operator, you can include any composite expression fashioned from <code>$</code> operator functions, <code>$</code> field paths and <code>$$</code> variables. A few situations demand having to use <code>$expr</code> from inside a <code>$match</code> stage. Examples include:</p>
<ul>
<li>
<p>A requirement to compare two fields from the same record to determine whether to keep the record based on the comparison's outcome</p>
</li>
<li>
<p>A requirement to perform a calculation based on values from multiple existing fields in each record and then comparing the calculation to a constant</p>
</li>
</ul>
<p>These are impossible in an aggregation (or MQL <code>find()</code>) if you use regular <code>$match</code> query conditions.</p>
<p>Take the example of a collection holding information on different instances of rectangles (capturing their width and height), similar to the following: </p>
<pre><code class="language-javascript">[
  { _id: 1, width: 2, height: 8 },
  { _id: 2, width: 3, height: 4 },
  { _id: 3, width: 20, height: 1 }
]
</code></pre>
<p>What if you wanted to run an aggregation pipeline to only return rectangles with an <code>area</code> greater than <code>12</code>? This comparison isn't possible in a conventional aggregation when using a single <code>$match</code> query condition. However, with <code>$expr</code>, you can analyse a combination of fields in-situ using expressions. You can implement the requirement with the following pipeline:</p>
<pre><code class="language-javascript">var pipeline = [
  {&quot;$match&quot;: {
    &quot;$expr&quot;: {&quot;$gt&quot;: [{&quot;$multiply&quot;: [&quot;$width&quot;, &quot;$height&quot;]}, 12]},
  }},
];
</code></pre>
<p>The result of executing an aggregation with this pipeline is:</p>
<pre><code class="language-javascript">[
  { _id: 1, width: 2, height: 8 },
  { _id: 3, width: 20, height: 1 }
]
</code></pre>
<p>As you can see, the second of the three shapes is not output because its area is only <code>12</code> (<code>3 x 4</code>).</p>
<h3 id="restrictions-when-using-expressions-with-match"><a class="header" href="#restrictions-when-using-expressions-with-match">Restrictions When Using Expressions with <code>$match</code></a></h3>
<p>You should be aware that there are restrictions on when the runtime can benefit from an index when using a <code>$expr</code> operator inside a <code>$match</code> stage. This partly depends on the version of MongoDB you are running. Using <code>$expr</code>, you can leverage a <code>$eq</code> comparison operator with some constraints, including an inability to use a <a href="https://docs.mongodb.com/manual/core/index-multikey/">multi-key index</a>. For MongoDB versions before 5.0, if you use a &quot;range&quot; comparison operator (<code>$gt</code>, <code>$gte</code>, <code>$lt</code> and <code>$lte</code>), an index cannot be employed to match the field, but this works fine in version 5.0 and greater.</p>
<p>There are also subtle differences when ordering values for a specific field across multiple documents when some values have different types. MongoDB's query runtime (which executes regular MQL and <code>$match</code> filters) and MongoDB's aggregation runtime (which implements <code>$expr</code>) can apply different ordering rules when filtering, referred to as &quot;type bracketing&quot;. Consequently, a range query may not yield the same result with <code>$expr</code> as it does with MQL if some values have different types.</p>
<p>Due to the potential challenges outlined, only use a <code>$expr</code> operator in a <code>$match</code> stage if there is no other way of assembling the filter criteria using regular MQL syntax.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sharding-considerations"><a class="header" href="#sharding-considerations">Sharding Considerations</a></h1>
<p><a href="https://docs.mongodb.com/manual/sharding/">MongoDB Sharding</a> isn't just an effective way to scale out your database to hold more data and support higher transactional throughput. Sharding also helps you scale out your analytical workloads, potentially enabling aggregations to complete far quicker. Depending on the nature of your aggregation and some adherence to best practices, the cluster may execute parts of the aggregation in parallel over multiple shards for faster completion.</p>
<p>There is no difference between a replica set and a sharded cluster regarding the functional capabilities of the aggregations you build, except for a minimal set of constraints. This chapter's <em><a href="guides/sharding.html#sharded-aggregation-constraints">Sharded Aggregation Constraints</a></em> section outlines these constraints. When it comes to optimising your aggregations, in most cases, there will be little to no difference in the structure of a pipeline when refactoring for performance on a sharded cluster compared to a simple replica set. You should always adhere to the advice outlined in the chapter <a href="guides/../guides/performance.html">Pipeline Performance Considerations</a>. The aggregation runtime takes care of distributing the appropriate parts of your pipeline to each shard that holds the required data. The runtime then transparently coalesces the results from these shards in the most optimal way possible. Nevertheless, it is worth understanding how the aggregation engine distributes work and applies its sharded optimisations in case you ever suffer a performance problem and need to dig deeper into why.</p>
<h2 id="brief-summary-of-sharded-clusters"><a class="header" href="#brief-summary-of-sharded-clusters">Brief Summary Of Sharded Clusters</a></h2>
<p>In a sharded cluster, you partition a collection of data across multiple shards, where each shard runs on a separate set of host machines. You control how the system distributes the data by defining a shard key rule. Based on the shard key of each document, the system groups subsets of documents together into &quot;chunks&quot;, where a range of shard key values identifies each chunk. The cluster balances these chunks across its shards.</p>
<p>In addition to holding sharded collections in a database, you may also be storing unsharded collections in the same database. All of a database's unsharded collections live on one specific shard in the cluster, designated as the &quot;primary shard&quot; for the database (not to be confused with a replica set's &quot;primary replica&quot;). The diagram below shows the relationship between a database's collections and the shards in the cluster.</p>
<p><img src="guides/./pics/sharded-db-cluster.png" alt="MongoDB sharded cluster containing sharded and unsharded collections" /></p>
<p>One or more deployed <a href="https://docs.mongodb.com/manual/reference/program/mongos/">mongos</a> processes act as a <a href="https://en.wikipedia.org/wiki/Reverse_proxy">reverse proxy</a>, routing read and write operations from the client application to the appropriate shards. For document write operations (i.e. create, update, delete), a mongos router knows which shard the document lives on and routes the operation to that specific shard. For read operations, if the query includes the shard key, the mongos knows which shards hold the required documents to route the query to (called &quot;targeting&quot;). If the query does not include the shard key, it sends the query to all shards using a &quot;scatter/gather&quot; pattern (called &quot;broadcasting&quot;). These are the rules for sharded reads and writes, but the approach for sharded aggregations requires a deeper explanation. Consequently, the rest of this chapter outlines how a sharded cluster handles the routing and execution of aggregations.</p>
<h2 id="sharded-aggregation-constraints"><a class="header" href="#sharded-aggregation-constraints">Sharded Aggregation Constraints</a></h2>
<p>Some of MongoDB's stages only partly support sharded aggregations depending on which version of MongoDB you are running. These stages all happen to reference a second collection in addition to the pipeline's source input collection. In each case, the pipeline can use a sharded collection as its source, but the second collection referenced must be unsharded (for earlier MongoDB versions, at least). The affected stages and versions are:</p>
<ul>
<li>
<p><strong><code>$lookup</code></strong>. In MongoDB versions prior to 5.1, the other referenced collection to join with must be unsharded.</p>
</li>
<li>
<p><strong><code>$graphLookup</code></strong>. In MongoDB versions prior to 5.1, the other referenced collection to recursively traverse must be unsharded.</p>
</li>
<li>
<p><strong><code>$out</code></strong>. In all MongoDB versions, the other referenced collection used as the destination of the aggregation's output must be unsharded. However, you can use a <code>$merge</code> stage instead to output the aggregation result to a sharded collection.</p>
</li>
</ul>
<h2 id="where-does-a-sharded-aggregation-run"><a class="header" href="#where-does-a-sharded-aggregation-run">Where Does A Sharded Aggregation Run?</a></h2>
<p>Sharded clusters provide the opportunity to reduce the response times of aggregations. For example, there may be an unsharded collection containing billions of documents where it takes 60 seconds for an aggregation pipeline to process all this data. Instead, suppose a cluster of four shards is hosting this same collection of evenly balanced data. Depending on the nature of the aggregation, it may be possible for the cluster to execute the aggregation's pipeline concurrently on each shard. Consequently, the same aggregation's total data processing time may be closer to 15 seconds. However, this won't always be the case because certain types of pipelines will demand combining substantial amounts of data from multiple shards for further processing. The aggregation's response time could go in the opposite direction in such circumstances, completing in far longer than 60 seconds due to the significant network transfer and marshalling overhead. </p>
<h3 id="pipeline-splitting-at-runtime"><a class="header" href="#pipeline-splitting-at-runtime">Pipeline Splitting At Runtime</a></h3>
<p>A sharded cluster will attempt to execute as many of a pipeline's stages as possible, in parallel, on each shard containing the required data. However, certain types of stages must operate on all the data in one place. Specifically, these are the sorting and grouping stages, collectively referred to as the &quot;blocking stages&quot; (described in the chapter <a href="guides/../guides/performance.html">Pipeline Performance Considerations</a>). Upon the first occurrence of a blocking stage in the pipeline, the aggregation engine will split the pipeline into two parts at the point where the blocking stage occurs. The Aggregation Framework refers to the first section of the divided pipeline as the &quot;Shards Part&quot;, which can run concurrently on multiple shards. The remaining portion of the split pipeline is called the &quot;Merger Part&quot;, which executes in one location. The following illustration shows how this pipeline division occurs.</p>
<p><img src="guides/./pics/split-pipeline.png" alt="Pipeline split into shards part and merger parts for an aggregation against a sharded cluster" /></p>
<p>One of the two stages which causes a split, shown as stage 3, is a <code>$group</code> stage. The same behaviour actually occurs with all grouping stages, specifically <code>$bucket</code>, <code>$bucketAuto</code>, <code>$count</code> and <code>$sortByCount</code>. Therefore any mention of the <code>$group</code> stage in this chapter is synonymous with all of these grouping stages. </p>
<p>You can see two examples of aggregation pipeline splitting in action in the MongoDB Shell screenshots displayed below, showing each pipeline and its explain plan. The cluster contains four shards (&quot;<em>s0</em>&quot;, &quot;<em>s1</em>&quot;, &quot;<em>s2</em>&quot; and &quot;<em>s3</em>&quot;) which hold the distributed collection. The two example aggregations perform the following actions respectively:</p>
<ol>
<li>
<p>Sharded sort, matching on shard key values and limiting the number of results</p>
</li>
<li>
<p>Sharded group, matching on non-shard key values with <code>allowDiskUse:true</code> and showing the total number of records per group</p>
</li>
</ol>
<p><img src="guides/./pics/sharded-aggs-plans.png" alt="Two example aggregation pipeline explain plans showing split pipelines and merger locations" /></p>
<p>You can observe some interesting behaviours from these two explain plans:</p>
<ul>
<li>
<p><strong>Shards Part Of Pipeline Running In Parallel</strong>. In both cases, the pipeline's <code>shardsPart</code> executes on multiple shards, as indicated in the shards array field at the base of the explain plan. In the first example, the aggregation runtime targets only three shards. However, in the second example, the runtime must broadcast the pipeline's <code>shardsPart</code> to run on all shards - the section <em><a href="guides/sharding.html#execution-of-the-shards-part-of-the-split-pipeline">Execution Of The Shards Part Of The Split Pipeline</a></em> in this chapter discusses why.</p>
</li>
<li>
<p><strong>Optimisations Applied In Shards Part</strong>. For the <code>$sort</code> or <code>$group</code> blocking stages where the pipeline splits, the blocking stage divides into two. The runtime executes the first phase of the blocking stage as the last stage of the <code>shardsPart</code> of the divided pipeline. It then completes the stage's remaining work as the first stage of the <code>mergerPart</code>. For a <code>$sort</code> stage, this means the cluster conducts a large portion of the sorting work in parallel on all shards, with a remaining &quot;merge sort&quot; occurring at the final location. For a <code>$group</code> stage, the cluster performs the grouping in parallel on every shard, accumulating partial sums and totals ahead of its final merge phase. Consequently, the runtime does not have to ship masses of raw ungrouped data from the source shards to where the runtime merges the partially formed groups.</p>
</li>
<li>
<p><strong>Merger Part Running From A Single Location</strong>. The specific location where the runtime executes the pipeline's <code>mergerPart</code> stages depends on several variables. The explain plan shows the location chosen by the runtime in the <code>mergeType</code> field of its output. In these two examples, the locations are <code>mongos</code> and <code>anyShard</code>, respectively. This chapter's <em><a href="guides/sharding.html#execution-of-the-merger-part-of-the-split-pipeline-if-any">Execution Of The Merger Part Of The Split Pipeline</a></em> section outlines the rules that the aggregation runtime uses to decide this location.</p>
</li>
<li>
<p><strong>Final Merge Sorting When The Sort Stage Is Split</strong>. The <code>$sort</code>'s final phase shown in the <code>mergerPart</code> of the first pipeline is not a blocking operation, whereas, with <code>$group</code> shown in the second pipeline, <code>$group</code>'s final phase is blocking. This chapter's <em><a href="guides/sharding.html#difference-in-merging-behaviour-for-grouping-vs-sorting">Difference In Merging Behaviour For Grouping Vs Sorting</a></em> section discusses why.</p>
</li>
</ul>
<blockquote>
<p><em>Unfortunately, if you are running your aggregations in MongoDB versions 4.2 to 5.2, the explain plan generated by the aggregation runtime erroneously neglects to log the final phase of the <code>$sort</code> stage in the pipeline's <code>mergerPart</code>. This is caused by a now fixed <a href="https://jira.mongodb.org/browse/SERVER-57383">explain plan bug</a> but rest assured that the final phase of the <code>$sort</code> stage (the &quot;merge sort&quot;) does indeed happen in the pipeline's <code>mergerPart</code> in all the MongoDB versions.</em></p>
</blockquote>
<h3 id="execution-of-the-shards-part-of-the-split-pipeline"><a class="header" href="#execution-of-the-shards-part-of-the-split-pipeline">Execution Of The Shards Part Of The Split Pipeline</a></h3>
<p>When a mongos receives a request to execute an aggregation pipeline, it needs to determine where to target the <em>shards part</em> of the pipeline. It will endeavour to run this on the relevant subset of shards rather than broadcasting the work to all. </p>
<p>Suppose there is a <code>$match</code> stage occurring at the start of the pipeline. If the filter for this <code>$match</code> includes the shard key or a prefix of the shard key, the mongos can perform a targeted operation. It routes the <em>shards part</em> of the split pipeline to execute on the applicable shards only.</p>
<p>Furthermore, suppose the runtime establishes that the <code>$match</code>'s filter contains an exact match on a shard key value for the source collection. In that case, the pipeline can target a single shard only, and doesn't even need to split the pipeline into two. The entire pipeline runs in one place, on the one shard where the data it needs lives. Even if the <code>$match</code>'s filter only has a partial match on the first part of the shard key (the &quot;prefix&quot;), if this spans a range of documents encapsulated within a single chunk, or multiple chunks on the same shard only, the runtime will just target the single shard.</p>
<h3 id="execution-of-the-merger-part-of-the-split-pipeline-if-any"><a class="header" href="#execution-of-the-merger-part-of-the-split-pipeline-if-any">Execution Of The Merger Part Of The Split Pipeline (If Any)</a></h3>
<p>The aggregation runtime applies a set of rules to determine where to execute the <em>merger part</em> of an aggregation pipeline for a sharded cluster and whether a split is even necessary. The following diagram captures the four different approaches the runtime will choose from.</p>
<p><img src="guides/./pics/four-agg-split-merge-patterns.png" alt="The four different ways a pipeline may be split with where the shards part runs and where the merger part runs" /></p>
<p>The aggregation runtime selects the <em>merger part</em> location (if any) by following a decision tree, with four possible outcomes. The list below outlines the ordered decisions the runtime takes. However, it is crucial to understand that this order does not reflect precedence or preference. Achieving either the <em>Targeted-Shard Execution</em> (2) or <em>Mongos Merge</em> (4) is usually the preferred outcome for optimum performance.</p>
<ol>
<li>
<p><strong>Primary-Shard Merge</strong>. When the pipeline contains a stage referencing a second unsharded collection, the aggregation runtime will place this stage in the <em>merger part</em> of the split pipeline. It executes this <em>merger part</em> on the designated primary shard, which holds the referenced unsharded collection. This is always the case for the stages that can only reference unsharded collections (i.e. for <code>$out</code> generally or for <code>$lookup</code> and <code>$graphLookup</code> in MongoDB versions before 5.1). This is also the situation if the collection happens to be unsharded and you reference it from a <code>$merge</code> stage or, in MongoDB 5.1 or greater, from a <code>$lookup</code> or <code>$graphLookup</code> stage.</p>
</li>
<li>
<p><strong>Targeted-Shard Execution</strong>. As discussed earlier, if the runtime can ensure the pipeline matches the required subset of the source collection data to just one shard, it does not split the pipeline, and there is no <em>merger part</em>. Instead, the runtime executes the entire pipeline on the one matched shard, just like it would for non-sharded deployments. This optimisation avoids unnecessarily breaking the pipeline into two parts, where intermediate data then has to move from the <em>shards part(s)</em> to the <em>merger part</em>. The behaviour of pinning to a single shard occurs even if the pipeline contains a <code>$merge</code>, <code>$lookup</code> or <code>$graphLookup</code> stage referencing a second sharded collection containing records dispersed across multiple shards.</p>
</li>
<li>
<p><strong>Any-Shard Merge</strong>. Suppose you've configured <code>allowDiskUse:true</code> for the aggregation to avoid the 100 MB memory consumption limit per stage. If one of the following two situations is also true, the aggregation runtime must run the <em>merger part</em> of the split pipeline on a randomly chosen shard (a.k.a. &quot;any shard&quot;):</p>
<ul>
<li>The pipeline contains a grouping stage (which is where the split occurs),  <em>or</em></li>
<li>The pipeline contains a <code>$sort</code> stage (which is where the split occurs), and a subsequent blocking stage (a grouping or <code>$sort</code> stage) occurs later.</li>
</ul>
<p>For these cases, the runtime picks a shard to execute the merger, rather than merging on the mongos, to maximise the likelihood that the host machine has enough storage space to spill to disk. Invariably, each shard's host machine will have greater storage capacity than the host machine of a mongos. Consequently, the runtime must take this caution because, with <code>allowDiskUse:true</code>, you are indicating the likelihood that your pipeline will cause memory capacity pressure. Notably, the aggregation runtime does not need to mitigate the same risk by merging on a shard for the other type of blocking stage (a <code>$sort</code>) when <code>$sort</code> is the only blocking stage in the pipeline. You can read why a single <code>$sort</code> stage can be treated differently and does not need the same host storage capacity for merging in this chapter's <em><a href="guides/sharding.html#difference-in-merging-behaviour-for-grouping-vs-sorting">Difference In Merging Behaviour For Grouping Vs Sorting</a></em> section.</p>
</li>
<li>
<p><strong>Mongos Merge</strong>. This is the default approach and location. The aggregation runtime will perform the <em>merger part</em> of the split pipeline on the mongos that instigated the aggregation in all the remaining situations. If the pipeline's <em>merger part</em> only contains streaming stages (described in the chapter <a href="guides/../guides/performance.html">Pipeline Performance Considerations</a>), the runtime assumes it is safe for the mongos to run the remaining pipeline. A mongos has no concept of local storage to hold data. However, it doesn't matter in this situation because the runtime won't need to write to disk as RAM pressure will be minimal. The category of streaming tasks that supports a Mongos Merge also includes the final phase of a split <code>$sort</code> stage, which processes data in a streaming fashion without needing to block to see all the data together. Additionally, suppose you have defined <code>allowDiskUse:false</code> (the default). In that case, you are signalling that even if the pipeline has a <code>$group</code> stage (or a <code>$sort</code> stage followed by another blocking stage), these blocking activities will not need to overspill to disk. Performing the final merge on the mongos is the default because fewer network data transfer hops are required to fulfil the aggregation, thus reducing latency compared with merging on &quot;any shard&quot;.</p>
</li>
</ol>
<p>Regardless of where the <em>merger part</em> runs, the mongos is always responsible for streaming the aggregation's final batches of results back to the client application. </p>
<p>It is worth considering when no blocking stages exist in a pipeline. In this case, the runtime executes the entire pipeline in parallel on the relevant shards and the runtime streams each shard's output directly to the mongos. You can regard this as just another variation of the default behaviour (<em>4 - Mongos Merge</em>). All the stages in the aggregation constitute just the <em>shards part</em> of the pipeline, and the mongos &quot;stream merges&quot; the final data through to the client. </p>
<h3 id="difference-in-merging-behaviour-for-grouping-vs-sorting"><a class="header" href="#difference-in-merging-behaviour-for-grouping-vs-sorting">Difference In Merging Behaviour For Grouping Vs Sorting</a></h3>
<p>You will have read in the <a href="guides/../guides/performance.html">Pipeline Performance Considerations</a> chapter about <code>$sort</code> and <code>$group</code> stages being blocking stages and potentially consuming copious RAM. Consequently, you may be confused by the statement that, unlike a <code>$group</code> stage, when the pipeline splits, the aggregation runtime will finalise a <code>$sort</code> stage on a mongos even if you specify <code>allowDiskUse:true</code>. This is because the final phase of a split <code>$sort</code> stage is not a blocking activity, whereas the final phase of a split <code>$group</code> stage is. For <code>$group</code>, the pipeline's <em>merger part</em> must wait for all the data coming out of all the targeted shards. For <code>$sort</code>, the runtime executes a streaming merge sort operation, only waiting for the next batch of records coming out of each shard. As long as it can see the first of the sorted documents in the next batch from every shard, it knows which documents it can immediately process and stream on to the rest of the pipeline. It doesn't have to block waiting to see all of the records to guarantee correct ordering. </p>
<p>This optimisation doesn't mean that MongoDB has magically found a way to avoid a <code>$sort</code> stage being a blocking stage in a sharded cluster. It hasn't. The first phase of the <code>$sort</code> stage, run on each shard in parallel, is still blocking, waiting to see all the matched input data for that shard. However, the final phase of the same <code>$sort</code> stage, executed at the merge location, does not need to block. </p>
<h3 id="summarising-sharded-pipeline-execution-approaches"><a class="header" href="#summarising-sharded-pipeline-execution-approaches">Summarising Sharded Pipeline Execution Approaches</a></h3>
<p>In summary, the aggregation runtime seeks to execute a pipeline on the subset of shards containing the required data only. If the runtime must split the pipeline to perform grouping or sorting, it completes the final merge work on a mongos, when possible. Merging on a mongos helps to reduce the number of required network hops and the execution time.</p>
<h2 id="performance-tips-for-sharded-aggregations"><a class="header" href="#performance-tips-for-sharded-aggregations">Performance Tips For Sharded Aggregations</a></h2>
<p>All the recommended aggregation optimisations outlined in the <a href="guides/../guides/performance.html">Pipeline Performance Considerations</a> chapter equally apply to a sharded cluster. In fact, in most cases, these same recommendations, repeated below, become even more critical when executing aggregations on sharded clusters: </p>
<ol>
<li>
<p><strong>Sorting - Use Index Sort</strong>. When the runtime has to split on a <code>$sort</code> stage, the <em>shards part</em> of the split pipeline running on each shard in parallel will avoid an expensive in-memory sort operation.</p>
</li>
<li>
<p><strong>Sorting - Use Limit With Sort</strong>. The runtime has to transfer fewer intermediate records over the network, from each shard performing the <em>shards part</em> of a split pipeline to the location that executes the pipeline's <em>merger part</em>.</p>
</li>
<li>
<p><strong>Sorting - Reduce Records To Sort</strong>. If you cannot adopt point 1 or 2, moving a <code>$sort</code> stage to as late as possible in a pipeline will typically benefit performance in a sharded cluster. Wherever the <code>$sort</code> stage appears in a pipeline, the aggregation runtime will split the pipeline at this point (unless preceded by a <code>$group</code> stage which would cause the split earlier). By promoting other activities to occur in the pipeline first, the hope is these reduce the number of records entering the blocking <code>$sort</code> stage. This sorting operation, executing in parallel at the end of <em>shards part</em> of the split pipeline, will exhibit less memory pressure. The runtime will also stream fewer records over the network to the split pipeline's <em>merger part</em> location.</p>
</li>
<li>
<p><strong>Grouping - Avoid Unnecessary Grouping</strong>. Using array operators where possible instead of <code>$unwind</code> and <code>$group</code> stages will mean that the runtime does not need to split the pipeline due to an unnecessarily introduced <code>$group</code> stage. Consequently, the aggregation can efficiently process and stream data directly to the mongos rather than flowing through an intermediary shard first.</p>
</li>
<li>
<p><strong>Grouping - Group Summary Data Only</strong>. The runtime has to move fewer computed records over the network from each shard performing the <em>shards part</em> of a split pipeline to the <em>merger part</em>'s location.</p>
</li>
<li>
<p><strong>Encourage Match Filters To Appear Early In The Pipeline</strong>. By filtering out a large subset of records on each shard when performing the <em>shards part</em> of the split pipeline, the runtime needs to stream fewer records to the <em>merger part</em> location.</p>
</li>
</ol>
<p>Specifically for sharded clusters, there are two further performance optimisations you should aspire to achieve:</p>
<ol>
<li>
<p><strong>Look For Opportunities To Target Aggregations To One Shard Only</strong>. If possible, include a <code>$match</code> stage with a filter on a shard key value (or shard key prefix value). </p>
</li>
<li>
<p><strong>Look For Opportunities For A Split Pipeline To Merge On A Mongos</strong>. If the pipeline has a <code>$group</code> stage (or a <code>$sort</code> stage followed by a <code>$group</code>/<code>$sort</code> stage) which causes the pipeline to divide, avoid specifying <code>allowDiskUse:true</code> if possible. This reduces the amount of intermediate data transferred over the network, thus reducing latency.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advanced-use-of-expressions-for-array-processing"><a class="header" href="#advanced-use-of-expressions-for-array-processing">Advanced Use Of Expressions For Array Processing</a></h1>
<p>One of the most compelling aspects of MongoDB is the ability to embed arrays within documents. Unlike relational databases, this characteristic typically allows each entity's entire data structure to exist in one place as a document. Documents better represent &quot;real-world&quot; objects and how developers think about such entities. When writing code to interact with the stored data, this intuitive data representation reduces the cognitive load on developers, enabling them to deliver new application capabilities quicker.</p>
<p>The Aggregation Framework provides a <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/#array-expression-operators">rich set of aggregation operator expressions</a> for analysing and manipulating arrays. When <a href="guides/performance.html#2-avoid-unwinding--regrouping-documents-just-to-process-array-elements">optimising for performance</a>, these array expressions are critical to avoid unwinding and regrouping documents where you only need to process each document's array in isolation. For most situations when you need to manipulate an array, there is usually a single <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/#array-expression-operators">array operator expression</a> that you can turn to solve your requirement. </p>
<p>Occasionally, you may still need to assemble a composite of multiple lower-level expressions to handle a challenging array manipulation task. These situations are the most difficult aspect for anyone using the Aggregation Framework. As a result, this chapter endeavours to bootstrap the knowledge you will require to fulfil such undertakings. Like aggregation pipelines in general, a large part of the challenge relates to adapting your mindset to a <a href="https://en.wikipedia.org/wiki/Functional_programming">Functional programming</a> paradigm rather than a <a href="https://en.wikipedia.org/wiki/Procedural_programming">Procedural</a> one. As this book discusses in its <a href="guides/../intro/introducing-aggregations.html#what-is-mongodbs-aggregations-language">introductory chapter</a>, the functional aspect of aggregations is essential for the database's aggregation engine to process data at scale efficiently. </p>
<p>Comparing with procedural approaches can help to bring clarity when describing array manipulation pipeline logic. Therefore, the first few explanations in this chapter include examples of equivalent JavaScript code snippets you would use to achieve comparable outcomes in regular client-side applications.</p>
<p>Lastly, if you haven't read this book's <a href="guides/expressions.html">Expressions Explained chapter</a> yet, you should do so before continuing with this chapter.</p>
<h2 id="if-else-conditional-comparison"><a class="header" href="#if-else-conditional-comparison">&quot;If-Else&quot; Conditional Comparison</a></h2>
<p>Even though performing conditional comparisons is more of a general principle than specific to array manipulation, it is first worth touching upon it to introduce the topic of advanced expressions. Consider the trivialised scenario of a retailer wanting to calculate the total cost of a customer’s shopping order. The customer might order multiple of the same product, and the vendor applies a discount if more than 5 of the product items are in the order.</p>
<p>In a procedural style of JavaScript, you might write the following code to calculate the total order cost:</p>
<pre><code class="language-javascript">let order = {&quot;product&quot; : &quot;WizzyWidget&quot;, &quot;price&quot;: 25.99, &quot;qty&quot;: 8};

// Procedural style JavaScript
if (order.qty &gt; 5) {
  order.cost = order.price * order.qty * 0.9;
} else {
  order.cost = order.price * order.qty;
}
</code></pre>
<p>This code modifies the customer’s order to the following, to include the total cost:</p>
<pre><code class="language-javascript">{product: 'WizzyWidget', qty: 8, price: 25.99, cost: 187.128}
</code></pre>
<p>To achieve a similar outcome in an aggregation pipeline, you might use the following:</p>
<pre><code class="language-javascript">db.customer_orders.insertOne(order);

var pipeline = [
  {&quot;$set&quot;: {
    &quot;cost&quot;: {
      &quot;$cond&quot;: { 
        &quot;if&quot;:   {&quot;$gte&quot;: [&quot;$qty&quot;, 5]}, 
        &quot;then&quot;: {&quot;$multiply&quot;: [&quot;$price&quot;, &quot;$qty&quot;, 0.9]},
        &quot;else&quot;: {&quot;$multiply&quot;: [&quot;$price&quot;, &quot;$qty&quot;]},
      }    
    },
  }},
];

db.customer_orders.aggregate(pipeline);
</code></pre>
<p>This pipeline produces the following output with the customer order document transformed to:</p>
<pre><code class="language-javascript">{product: 'WizzyWidget', price: 25.99, qty: 8, cost: 187.128}
</code></pre>
<p>If you were going to use a functional programming approach in JavaScript, the code would be more like the following to achieve the same outcome:</p>
<pre><code class="language-javascript">// Functional style JavaScript
order.cost = (
              (order.qty &gt; 5) ?
              (order.price * order.qty * 0.9) :
              (order.price * order.qty)
             );
</code></pre>
<p>Here, you can see that the JavaScript code's construction in a functional style more closely resembles the aggregation pipeline's structure. This comparison highlights why some people may find composing aggregation expressions foreboding. The challenge is predominantly due to the less familiar paradigm of functional programming rather than the intricacies of MongoDB's aggregation language per se.</p>
<p>The other difference in this comparison and the rest of the comparisons in this chapter is the pipeline will work unchanged when run against a collection of many records, which could feasibly be many billions. The sample JavaScript code only works against one document at a time and would need to be modified to loop through a list of records. This JavaScript code would need to fetch each document from the database back to a client, apply the modifications and then write the result back to the database. Instead, the aggregation pipeline’s logic operates against each document in-situ within the database for far superior performance and efficiency.</p>
<h2 id="the-power-array-operators"><a class="header" href="#the-power-array-operators">The &quot;Power&quot; Array Operators</a></h2>
<p>When you want to transform or extract data from an array field, and a single high-level array operator (e.g. <code>$avg</code>, <code>$max</code>, <code>$filter</code>) does not give you what you need, the tools to turn to are the <code>$map</code> and <code>$reduce</code> array operators. These two &quot;power&quot; operators enable you to iterate through an array, perform whatever complexity of logic you need against each array element and collect together the result for inclusion in a stage's output. </p>
<p>The <code>$map</code> and <code>$reduce</code> operators are the &quot;swiss army knives&quot; of the Aggregation Framework. Do not confuse these two array operators with MongoDB's old <a href="https://docs.mongodb.com/manual/core/map-reduce/">Map-Reduce API</a>, which was essentially made redundant and obsolete by the <a href="guides/../intro/history.html">emergence of the superior Aggregation Framework in MongoDB</a>. In the old Map-Reduce API, you combine a <code>map()</code> function and a <code>reduce()</code> function to generate a result. In the Aggregation Framework, the <code>$map</code> and <code>$reduce</code> operators are independent of each other. Depending on your specific requirements, you would use one or the other to process an array's field, but not necessarily both together. Here's an explanation of these two &quot;power&quot; operators:</p>
<ul>
<li><a href="https://docs.mongodb.com/manual/reference/operator/aggregation/map/"><code>$map</code></a>. Allows you to specify some logic to perform against each element in the array that the operator iterates, returning an array as the final result. Typically you use <code>$map</code> to mutate each array member and then return this transformed array. The <code>$map</code> operator exposes the current array element's content to your logic via a special variable, with the default name of <code>$$this</code>.</li>
<li><a href="https://docs.mongodb.com/manual/reference/operator/aggregation/reduce/"><code>$reduce</code></a>. Similarly, you can specify some logic to execute for each element in an array that the operator iterates but instead returning a single value (rather than an array) as the final result. You typically use <code>$reduce</code> to compute a summary having analysed each array element. For example, you might want to return a number by multiplying together a specific field value from each element in the array. Like the <code>$map</code> operator, the <code>$reduce</code> operator provides your logic with access to the current array element via the variable <code>$$this</code>. The operator also provides a second variable, called <code>$$value</code>, for your logic to update when accumulating the single result (e.g. the multiplication result).</li>
</ul>
<p>The rest of this chapter explores how these two &quot;power&quot; operators are used to manipulate arrays.</p>
<h2 id="for-each-looping-to-transform-an-array"><a class="header" href="#for-each-looping-to-transform-an-array">&quot;For-Each&quot; Looping To Transform An Array</a></h2>
<p>Imagine you wanted to process a list of the products ordered by a customer and convert the array of product names to uppercase. In a procedural style of JavaScript, you might write the following code to loop through each product in the array and convert its name to uppercase:</p>
<pre><code class="language-javascript">let order = {
  &quot;orderId&quot;: &quot;AB12345&quot;,
  &quot;products&quot;: [&quot;Laptop&quot;, &quot;Kettle&quot;, &quot;Phone&quot;, &quot;Microwave&quot;]
};
 
// Procedural style JavaScript
for (let pos in order.products) {
  order.products[pos] = order.products[pos].toUpperCase();
}
</code></pre>
<p>This code modifies the order’s product names to the following, with the product names now in uppercase:</p>
<pre><code class="language-javascript">{orderId: 'AB12345', products: ['LAPTOP', 'KETTLE', 'PHONE', 'MICROWAVE']}
</code></pre>
<p>To achieve a similar outcome in an aggregation pipeline, you might use the following:</p>
<pre><code class="language-javascript">db.orders.insertOne(order);

var pipeline = [
  {&quot;$set&quot;: {
    &quot;products&quot;: {
      &quot;$map&quot;: {
        &quot;input&quot;: &quot;$products&quot;,
        &quot;as&quot;: &quot;product&quot;,
        &quot;in&quot;: {&quot;$toUpper&quot;: &quot;$$product&quot;}
      }
    }
  }}
];

db.orders.aggregate(pipeline);
</code></pre>
<p>Here, a <code>$map</code> operator expression is applied to loop through each product name in the input products array and add the uppercase version of the product name to the replacement output array.</p>
<p>This pipeline produces the following output with the order document transformed to:</p>
<pre><code class="language-javascript">{orderId: 'AB12345', products: ['LAPTOP', 'KETTLE', 'PHONE', 'MICROWAVE']}
</code></pre>
<p>Using functional style in JavaScript, your looping code would more closely resemble the following to achieve the same outcome:</p>
<pre><code class="language-javascript">// Functional style JavaScript
order.products = order.products.map(
  product =&gt; {
    return product.toUpperCase(); 
  }
);
</code></pre>
<p>Comparing an aggregation <code>$map</code> operator expression to a JavaScript <code>map()</code> array function is far more illuminating to help explain how the operator works.</p>
<h2 id="for-each-looping-to-compute-a-summary-value-from-an-array"><a class="header" href="#for-each-looping-to-compute-a-summary-value-from-an-array">&quot;For-Each&quot; Looping To Compute A Summary Value From An Array</a></h2>
<p>Suppose you wanted to process a list of the products ordered by a customer but produce a single summary string field from this array by concatenating all the product names from the array. In a procedural JavaScript style, you could code the following to produce the product names summary field:</p>
<pre><code class="language-javascript">let order = {
  &quot;orderId&quot;: &quot;AB12345&quot;,
  &quot;products&quot;: [&quot;Laptop&quot;, &quot;Kettle&quot;, &quot;Phone&quot;, &quot;Microwave&quot;]
};
 
order.productList = &quot;&quot;;

// Procedural style JavaScript
for (const pos in order.products) {
  order.productList += order.products[pos] + &quot;; &quot;;
}
</code></pre>
<p>This code yields the following output with a new <code>productList</code> string field produced, which contains the names of all the products in the order, delimited by semicolons:</p>
<pre><code class="language-javascript">{
  orderId: 'AB12345',
  products: [ 'Laptop', 'Kettle', 'Phone', 'Microwave' ],
  productList: 'Laptop; Kettle; Phone; Microwave; '
}
</code></pre>
<p>You can use the following pipeline to achieve a similar outcome:</p>
<pre><code class="language-javascript">db.orders.insertOne(order);

var pipeline = [
  {&quot;$set&quot;: {
    &quot;productList&quot;: {
      &quot;$reduce&quot;: {
        &quot;input&quot;: &quot;$products&quot;,
        &quot;initialValue&quot;: &quot;&quot;,
        &quot;in&quot;: {
          &quot;$concat&quot;: [&quot;$$value&quot;, &quot;$$this&quot;, &quot;; &quot;]
        }            
      }
    }
  }}
];

db.orders.aggregate(pipeline);
</code></pre>
<p>Here, a <code>$reduce</code> operator expression loops through each product in the input array and concatenates each product’s name into an accumulating string. You use the <code>$$this</code> expression to access the current array element's value during each iteration. For each iteration, you employ the <code>$$value</code> expression to reference the final output value, to which you append the current product string (+ delimiter).</p>
<p>This pipeline produces the following output where it transforms the order document to:</p>
<pre><code class="language-javascript">{
  orderId: 'AB12345',
  products: [ 'Laptop', 'Kettle', 'Phone', 'Microwave' ],
  productList: 'Laptop; Kettle; Phone; Microwave; '
}
</code></pre>
<p>Using a functional approach in JavaScript, you could have used the following code to achieve the same result:</p>
<pre><code class="language-javascript">// Functional style JavaScript
order.productList = order.products.reduce(
  (previousValue, currentValue) =&gt; {
    return previousValue + currentValue + &quot;; &quot;;
  },
  &quot;&quot;
);
</code></pre>
<p>Once more, by comparing the use of the aggregation operator expression (<code>$reduce</code>) to the equivalent JavaScript array function (<code>reduce()</code>), the similarity is more pronounced.</p>
<h2 id="for-each-looping-to-locate-an-array-element"><a class="header" href="#for-each-looping-to-locate-an-array-element">&quot;For-Each&quot; Looping To Locate An Array Element</a></h2>
<p>Imagine storing data about buildings on a campus where each building document contains an array of rooms with their sizes (width and length). A room reservation system may require finding the first room in the building with sufficient floor space for a particular number of meeting attendees. Below is an example of one building's data you might load into the database, with its array of rooms and their dimensions in metres:</p>
<pre><code class="language-javascript">db.buildings.insertOne({
  &quot;building&quot;: &quot;WestAnnex-1&quot;,
  &quot;room_sizes&quot;: [
    {&quot;width&quot;: 9, &quot;length&quot;: 5},
    {&quot;width&quot;: 8, &quot;length&quot;: 7},
    {&quot;width&quot;: 7, &quot;length&quot;: 9},
    {&quot;width&quot;: 9, &quot;length&quot;: 8},
  ]
});
</code></pre>
<p>You want to create a pipeline to locate an appropriate meeting room that produces an output similar to the following. The result should contain a newly added field, <code>firstLargeEnoughRoomArrayIndex</code>, to indicate the array position of the first room found to have enough capacity.</p>
<pre><code class="language-javascript">{
  building: 'WestAnnex-1',
  room_sizes: [
    { width: 9, length: 5 },
    { width: 8, length: 7 },
    { width: 7, length: 9 },
    { width: 9, length: 8 }
  ],
  firstLargeEnoughRoomArrayIndex: 2
}
</code></pre>
<p>Below is a suitable pipeline that iterates through the room array elements capturing the position of the first one with a calculated area greater than 60m²:</p>
<pre><code class="language-javascript">var pipeline = [
  {&quot;$set&quot;: {
    &quot;firstLargeEnoughRoomArrayIndex&quot;: {
      &quot;$reduce&quot;: {
        &quot;input&quot;: {&quot;$range&quot;: [0, {&quot;$size&quot;: &quot;$room_sizes&quot;}]},
        &quot;initialValue&quot;: -1,
        &quot;in&quot;: {
          &quot;$cond&quot;: { 
            &quot;if&quot;: {
              &quot;$and&quot;: [
                // IF ALREADY FOUND DON'T CONSIDER SUBSEQUENT ELEMENTS
                {&quot;$lt&quot;: [&quot;$$value&quot;, 0]}, 
                // IF WIDTH x LENGTH &gt; 60
                {&quot;$gt&quot;: [
                  {&quot;$multiply&quot;: [
                    {&quot;$getField&quot;: {&quot;input&quot;: {&quot;$arrayElemAt&quot;: [&quot;$room_sizes&quot;, &quot;$$this&quot;]}, &quot;field&quot;: &quot;width&quot;}},
                    {&quot;$getField&quot;: {&quot;input&quot;: {&quot;$arrayElemAt&quot;: [&quot;$room_sizes&quot;, &quot;$$this&quot;]}, &quot;field&quot;: &quot;length&quot;}},
                  ]},
                  60
                ]}
              ]
            }, 
            // IF ROOM SIZE IS BIG ENOUGH CAPTURE ITS ARRAY POSITION
            &quot;then&quot;: &quot;$$this&quot;,  
            // IF ROOM SIZE NOT BIG ENOUGH RETAIN EXISTING VALUE (-1)
            &quot;else&quot;: &quot;$$value&quot;  
          }            
        }            
      }
    }
  }}
];

db.buildings.aggregate(pipeline);
</code></pre>
<p>Here the <code>$reduce</code> operator is again used to loop and eventually return a single value. However, the pipeline uses a generated sequence of incrementing numbers for its input rather than the existing array field in each source document. The <code>$range</code> operator is used to create this sequence which has the same size as the rooms array field of each document. The pipeline uses this approach to track the array position of the matching room using the <code>$$this</code> variable. For each iteration, the pipeline calculates the array room element's area. If the size is greater than 60, the pipeline assigns the current array position (represented by <code>$$this</code>) to the final result (represented by<code> $$value</code>).</p>
<p>The &quot;iterator&quot; array expressions have no concept of a <em>break</em> command that procedural programming languages typically provide. Therefore, even though the executing logic may have already located a room of sufficient size, the looping process will continue through the remaining array elements. Consequently, the pipeline logic must include a check during each iteration to avoid overriding the final value (the <code>$$value</code> variable) if it already has a value. Naturally, for massive arrays containing a few hundred or more elements, an aggregation pipeline will incur a noticeable latency impact when iterating the remaining array members even though the logic has already identified the required element. </p>
<p>Suppose you just wanted to return the first matching array element for a room with sufficient floor space, not its index. In that case, the pipeline can be more straightforward, using <code>$filter</code> to trim the array elements to only those with sufficient space and then the <code>$first</code> operator to grab just the first element from the filter. You would use a pipeline similar to the following:</p>
<pre><code class="language-javascript">var pipeline = [
  {&quot;$set&quot;: {
    &quot;firstLargeEnoughRoom&quot;: {
      &quot;$first&quot;: {
        &quot;$filter&quot;: { 
          &quot;input&quot;: &quot;$room_sizes&quot;, 
          &quot;as&quot;: &quot;room&quot;,
          &quot;cond&quot;: {
            &quot;$gt&quot;: [
              {&quot;$multiply&quot;: [&quot;$$room.width&quot;, &quot;$$room.length&quot;]},
              60
            ]
          } 
        }    
      }
    }
  }}
];

db.buildings.aggregate(pipeline);
</code></pre>
<p>This pipeline produces the following output:</p>
<pre><code class="language-javascript">[
  {
    _id: ObjectId(&quot;637b4b8a86fac07908ef98b3&quot;),
    building: 'WestAnnex-1',
    room_sizes: [
      { width: 9, length: 5 },
      { width: 8, length: 7 },
      { width: 7, length: 9 },
      { width: 9, length: 8 }
    ],
    firstLargeEnoughRoom: { width: 7, length: 9 }
  }
]
</code></pre>
<p>In reality, the array of rooms would be likely to also include an ID for each building's room, for example:</p>
<pre><code class="language-javascript">&quot;room_sizes&quot;: [
  {&quot;roomId&quot;: &quot;Mercury&quot;, &quot;width&quot;: 9, &quot;length&quot;: 5},
  {&quot;roomId&quot;: &quot;Venus&quot;, &quot;width&quot;: 8, &quot;length&quot;: 7},
  {&quot;roomId&quot;: &quot;Jupiter&quot;, &quot;width&quot;: 7, &quot;length&quot;: 9},
  {&quot;roomId&quot;: &quot;Saturn&quot;, &quot;width&quot;: 9, &quot;length&quot;: 8},
]
</code></pre>
<p>Consequently, <code>firstLargeEnoughRoom: { roomId: &quot;Jupiter&quot;, width: 7, length: 9 }</code> would be the first element returned from the filtering pipeline stage, giving you the room's ID, so there would be no need to obtain the array's index for this particular use case.  However, the previous example, using the <code>$reduce</code> based pipeline, is helpful for more complicated situations where you do need the index of the matching array element.</p>
<h2 id="reproducing-map-behaviour-using-reduce"><a class="header" href="#reproducing-map-behaviour-using-reduce">Reproducing <em>$map</em> Behaviour Using <em>$reduce</em></a></h2>
<p>It is possible to implement the <code>$map</code> behaviour using <code>$reduce</code> to transform an array. This method is more complex, but you may need to use it in some rare circumstances. Before looking at an example of why let's first compare a more basic example of using <code>$map</code> and then <code>$reduce</code> to achieve the same thing. </p>
<p>Suppose you have captured some sensor readings for a device:</p>
<pre><code class="language-javascript">db.deviceReadings.insertOne({
  &quot;device&quot;: &quot;A1&quot;,
  &quot;readings&quot;: [27, 282, 38, -1, 187]
});
</code></pre>
<p>Imagine you want to produce a transformed version of the <code>readings</code> array, with the device’s ID concatenated with each reading in the array. You want the pipeline to produce an output similar to the following, with the newly included array field:</p>
<pre><code class="language-javascript">{
  device: 'A1',
  readings: [ 27, 282, 38, -1, 187 ],
  deviceReadings: [ 'A1:27', 'A1:282', 'A1:38', 'A1:-1', 'A1:187' ]
}
</code></pre>
<p>You can achieve this using the <code>$map</code> operator expression in the following pipeline:</p>
<pre><code class="language-javascript">var pipeline = [
  {&quot;$set&quot;: {
    &quot;deviceReadings&quot;: {
      &quot;$map&quot;: {
        &quot;input&quot;: &quot;$readings&quot;,
        &quot;as&quot;: &quot;reading&quot;,
        &quot;in&quot;: {
          &quot;$concat&quot;: [&quot;$device&quot;, &quot;:&quot;, {&quot;$toString&quot;: &quot;$$reading&quot;}]
        }
      }
    }
  }}
];

db.deviceReadings.aggregate(pipeline);
</code></pre>
<p>You can also accomplish the same with the <code>$reduce</code> operator expression in the following pipeline:</p>
<pre><code class="language-javascript">var pipeline = [
  {&quot;$set&quot;: {
    &quot;deviceReadings&quot;: {
      &quot;$reduce&quot;: {
        &quot;input&quot;: &quot;$readings&quot;,
        &quot;initialValue&quot;: [],
        &quot;in&quot;: {
          &quot;$concatArrays&quot;: [
            &quot;$$value&quot;,
            [{&quot;$concat&quot;: [&quot;$device&quot;, &quot;:&quot;, {&quot;$toString&quot;: &quot;$$this&quot;}]}]
          ]
        }
      }
    }
  }}
];

db.deviceReadings.aggregate(pipeline);
</code></pre>
<p>You will see the pipeline has to do more work here, holding the transformed element in a new array and then concatenating this with the &quot;final value&quot; array the logic is accumulating in the <code>$$value</code> variable. </p>
<p>So why would you ever want to use <code>$reduce</code> for this requirement and take on this extra complexity? Suppose the mapping code in the stage needs to include a condition to omit outlier readings that signify a device sensor faulty reading (i.e., a <code>-1</code> reading value). The challenge here when using <code>$map</code> is that for 5 input array elements, 5 array elements will need to be output. However, using <code>$reduce</code>, for an input of 5 array elements, 4 array elements can be output using a pipeline similar to the following:</p>
<pre><code class="language-javascript">var pipeline = [
  {&quot;$set&quot;: {
    &quot;deviceReadings&quot;: {
      &quot;$reduce&quot;: {
        &quot;input&quot;: &quot;$readings&quot;,
        &quot;initialValue&quot;: [],
        &quot;in&quot;: {
          &quot;$concatArrays&quot;: [
            &quot;$$value&quot;,
            {&quot;$cond&quot;: { 
              &quot;if&quot;: {&quot;$gte&quot;: [&quot;$$this&quot;, 0]},
              &quot;then&quot;: [{&quot;$concat&quot;: [&quot;$device&quot;, &quot;:&quot;, {&quot;$toString&quot;: &quot;$$this&quot;}]}],  
              &quot;else&quot;: []
            }}                                    
          ]
        }
      }
    }
  }}
];
</code></pre>
<p>This time, the output does not include the faulty device reading (`-1'):</p>
<pre><code class="language-javascript">[
  {
    device: 'A1',
    readings: [ 27, 282, 38, -1, 187 ],
    deviceReadings: [ 'A1:27', 'A1:282', 'A1:38', 'A1:187' ]
  }
]
</code></pre>
<p>Of course, this being the aggregation framework, multiple ways exist to solve the same problem. Another approach could be to continue with the <code>$map</code> based pipeline and, using the <code>$cond</code> operator, return an empty string (<code>''</code>) for each faulty reading. You would then need to wrap the <code>$map</code> stage in a <code>$filter</code> stage with logic to filter out elements where the element's string length is zero.</p>
<p>In summary, you typically use a <code>$map</code> stage when the ratio of input elements to output elements is the same (i.e. many-to-many or <em>M:M</em>). You employ a <code>$reduce</code> stage when the ratio of input elements to output elements is many-to-one (i.e. <em>M:1</em>). For situations where the ratio of input elements is many-to-few (i.e. <em>M:N</em>),  instead of <code>$map</code>, you will invariably reach for <code>$reduce</code> with its &quot;null array concatenation&quot; trick when <code>$filter</code> does not suffice.</p>
<h2 id="adding-new-fields-to-existing-objects-in-an-array"><a class="header" href="#adding-new-fields-to-existing-objects-in-an-array">Adding New Fields To Existing Objects In An Array</a></h2>
<p>One of the primary uses of the <code>$map</code> operator expression is to add more data to each existing object in an array. Suppose you've persisted a set of retail orders, where each order document contains an array of order items. Each order item in the array captures the product’s name, unit price, and quantity purchased, as shown in the example below:</p>
<pre><code class="language-javascript">db.orders.insertOne({
    &quot;custid&quot;: &quot;jdoe@acme.com&quot;,
    &quot;items&quot;: [
      {
        &quot;product&quot; : &quot;WizzyWidget&quot;, 
        &quot;unitPrice&quot;: 25.99,
        &quot;qty&quot;: 8,
      },
      {
        &quot;product&quot; : &quot;HighEndGizmo&quot;, 
        &quot;unitPrice&quot;: 33.24,
        &quot;qty&quot;: 3,
      }
    ]
});
</code></pre>
<p>You now need to calculate the total cost for each product item (<code>quantity</code> x <code>unitPrice</code>) and add that cost to the corresponding order item in the array. You can use a pipeline similar to the following to achieve this:</p>
<pre><code class="language-javascript">var pipeline = [
  {&quot;$set&quot;: {
    &quot;items&quot;: {
      &quot;$map&quot;: {
        &quot;input&quot;: &quot;$items&quot;,
        &quot;as&quot;: &quot;item&quot;,
        &quot;in&quot;: {
          &quot;product&quot;: &quot;$$item.product&quot;,
          &quot;unitPrice&quot;: &quot;$$item.unitPrice&quot;,
          &quot;qty&quot;: &quot;$$item.qty&quot;,
          &quot;cost&quot;: {&quot;$multiply&quot;: [&quot;$$item.unitPrice&quot;, &quot;$$item.qty&quot;]}},
        }
      }
    }
  }
];

db.orders.aggregate(pipeline);
</code></pre>
<p>Here, for each element in the source array, the pipeline creates an element in the new array by explicitly pulling in the three fields from the old element (<code>product</code>, <code>unitPrice</code> and <code>quantity</code>) and adding one new computed field (<code>cost</code>). The pipeline produces the following output:</p>
<pre><code class="language-javascript">{
  custid: 'jdoe@acme.com',
  items: [
    {
      product: 'WizzyWidget',
      unitPrice: 25.99,
      qty: 8,
      cost: 187.128
    },
    {
      product: 'HighEndGizmo',
      unitPrice: 33.24,
      qty: 3,
      cost: 99.72
    }
  ]
}
</code></pre>
<p>Similar to the disadvantages of using a <code>$project</code> stage in a pipeline, <a href="guides/project.html">outlined in an earlier chapter</a>, the <code>$map</code> code is burdened by explicitly naming every field in the array element to retain. You will find this tiresome if each array element has lots of fields. In addition, if your data model evolves and new types of fields appear in the array's items over time, you will be forced to return to your pipeline and refactor it each time to include these newly introduced fields. </p>
<p>Just like using <code>$set</code> instead of <code>$project</code> for a pipeline stage, there is a better solution to allow you to retain all existing array item fields and add new ones when you process arrays. A good solution is to employ the <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/merge/"><code>$mergeObjects</code></a> operator expression to combine all existing fields plus the newly computed fields into each new array element. <code>$mergeObjects</code> takes an array of objects and combines the fields from all the array's objects into one single object. To use <code>$mergeObjects</code> in this situation, you provide the current array element as the first parameter to <code>$mergeObjects</code>. The second parameter you provide is a new object containing each computed field. In the example below, the code adds only one generated field, but if you require it, you can include multiple generated fields in this new object:</p>
<pre><code class="language-javascript">var pipeline = [
  {&quot;$set&quot;: {
    &quot;items&quot;: {
      &quot;$map&quot;: {
        &quot;input&quot;: &quot;$items&quot;,
        &quot;as&quot;: &quot;item&quot;,
        &quot;in&quot;: {
          &quot;$mergeObjects&quot;: [
            &quot;$$item&quot;,            
            {&quot;cost&quot;: {&quot;$multiply&quot;: [&quot;$$item.unitPrice&quot;, &quot;$$item.qty&quot;]}},
          ]
        }
      }
    }
  }}
];

db.orders.aggregate(pipeline);
</code></pre>
<p>This pipeline produces the same output as the previous &quot;hardcoded field names&quot; pipeline, but with the advantage of being sympathetic to new types of fields appearing in the source array in the future.</p>
<p>Instead of using <code>$mergeObjects</code>, there is an alternative and slightly more verbose combination of three different array operator expressions that you can similarly employ to retain all existing array item fields and add new ones. These three operators are:</p>
<ul>
<li><a href="https://docs.mongodb.com/manual/reference/operator/aggregation/objectToArray/"><code>$objectToArray</code></a>. This converts an object containing different field key/value pairs into an array of objects where each object has two fields: <code>k</code>, holding the field's name, and <code>v</code>, holding the field's value. For example: <code>{height: 170, weight: 60}</code> becomes <code>[{k: 'height', v: 170}, {k: 'weight', v: 60}]</code></li>
<li><a href="https://docs.mongodb.com/manual/reference/operator/aggregation/concatArrays/"><code>$concatArrays</code></a>. This combines the contents of multiple arrays into one single array result.</li>
<li><a href="https://docs.mongodb.com/manual/reference/operator/aggregation/arrayToObject/"><code>$arrayToObject</code></a>. This converts an array into an object by performing the reverse of the <code>$objectToArray</code> operator. For example: <code>{k: 'height', v: 170}, {k: 'weight', v: 60}, {k: 'shoeSize', v: 10}]</code> becomes <code>{height: 170, weight: 60, shoeSize: 10}</code></li>
</ul>
<p>The pipeline below shows the combination in action for the same retail orders data set as before, adding the newly computed total cost for each product:</p>
<pre><code class="language-javascript">var pipeline = [
  {&quot;$set&quot;: {
    &quot;items&quot;: {
      &quot;$map&quot;: {
        &quot;input&quot;: &quot;$items&quot;,
        &quot;as&quot;: &quot;item&quot;,
        &quot;in&quot;: {
          &quot;$arrayToObject&quot;: {
            &quot;$concatArrays&quot;: [
              {&quot;$objectToArray&quot;: &quot;$$item&quot;},            
              [{
                &quot;k&quot;: &quot;cost&quot;,
                &quot;v&quot;: {&quot;$multiply&quot;: [&quot;$$item.unitPrice&quot;, &quot;$$item.qty&quot;]},
              }]              
            ]
          }
        }
      }
    }}
  }
];

db.orders.aggregate(pipeline);
</code></pre>
<p>If this achieves the same as using <code>$mergeObjects</code> but is more verbose, why bother using this pattern? Well, in most cases, you wouldn't. One situation where you would use the more verbose combination is if you need to dynamically set the name of an array item's field, in addition to its value. Rather than naming the computed total field as <code>cost</code>, suppose you want the field's name also to reflect the product's name (e.g. <code>costForWizzyWidget</code>, <code>costForHighEndGizmo</code>). You can achieve this by using the <code>$arrayToObject</code>/<code>$concatArrays</code>/<code>$objectToArray</code> approach rather than the <code>$mergeObjects</code> method, as follows:</p>
<pre><code class="language-javascript">var pipeline = [
  {&quot;$set&quot;: {
    &quot;items&quot;: {
      &quot;$map&quot;: {
        &quot;input&quot;: &quot;$items&quot;,
        &quot;as&quot;: &quot;item&quot;,
        &quot;in&quot;: {
          &quot;$arrayToObject&quot;: {
            &quot;$concatArrays&quot;: [
              {&quot;$objectToArray&quot;: &quot;$$item&quot;},            
              [{
                &quot;k&quot;: {&quot;$concat&quot;: [&quot;costFor&quot;, &quot;$$item.product&quot;]},
                &quot;v&quot;: {&quot;$multiply&quot;: [&quot;$$item.unitPrice&quot;, &quot;$$item.qty&quot;]},
              }]              
            ]
          }
        }
      }
    }}
  }
];

db.orders.aggregate(pipeline);
</code></pre>
<p>Below you can see the new pipeline's output. The pipeline has retained all existing array item's fields and added a new field to each item with a dynamically generated name.</p>
<pre><code class="language-javascript">{
  custid: 'jdoe@acme.com',
  items: [
    {
      product: 'WizzyWidget',
      unitPrice: 25.99,
      qty: 8,
      costForWizzyWidget: 207.92
    },
    {
      product: 'HighEndGizmo',
      unitPrice: 33.24,
      qty: 3,
      costForHighEndGizmo: 99.72
    }
  ]
}
</code></pre>
<p>When retaining existing items from an array, plus adding new fields, you can use either approach to override an existing item's field with a new value. For example, you may want to modify the current <code>unitPrice</code> field to incorporate a discount. For both <code>$mergeObjects</code> and <code>$arrayToObject</code> expressions, to achieve this, you provide a re-definition of the field as a subsequent parameter after first providing the reference to the source array item. This tactic works because the last definition wins if the same field is defined more than once with different values.</p>
<h2 id="rudimentary-schema-reflection-using-arrays"><a class="header" href="#rudimentary-schema-reflection-using-arrays">Rudimentary Schema Reflection Using Arrays</a></h2>
<p>As a final &quot;fun&quot; example, let's see how to employ an <code>$objectToArray</code> operator expression to use <a href="https://en.wikipedia.org/wiki/Reflective_programming">reflection</a> to analyse the shape of a collection of documents as part of a custom schema analysis tool. Such reflection capabilities are vital in databases that provide a flexible data model, such as MongoDB, where the included fields may vary from document to document. </p>
<p>Imagine you have a collection of customer documents, similar to the following:</p>
<pre><code class="language-javascript">db.customers.insertMany([
  {
    &quot;_id&quot;: ObjectId('6064381b7aa89666258201fd'),
    &quot;email&quot;: 'elsie_smith@myemail.com',
    &quot;dateOfBirth&quot;: ISODate('1991-05-30T08:35:52.000Z'),
    &quot;accNnumber&quot;: 123456,
    &quot;balance&quot;: NumberDecimal(&quot;9.99&quot;),
    &quot;address&quot;: {
      &quot;firstLine&quot;: &quot;1 High Street&quot;,
      &quot;city&quot;: &quot;Newtown&quot;,
      &quot;postcode&quot;: &quot;NW1 1AB&quot;,
    },
    &quot;telNums&quot;: [&quot;07664883721&quot;, &quot;01027483028&quot;],
    &quot;optedOutOfMarketing&quot;: true,
  },
  {
    &quot;_id&quot;: ObjectId('734947394bb73732923293ed'),
    &quot;email&quot;: 'jon.jones@coolemail.com',
    &quot;dateOfBirth&quot;: ISODate('1993-07-11T22:01:47.000Z'),
    &quot;accNnumber&quot;: 567890,
    &quot;balance&quot;: NumberDecimal(&quot;299.22&quot;),
    &quot;telNums&quot;: &quot;07836226281&quot;,
    &quot;contactPrefernece&quot;: &quot;email&quot;,
  },
]);
</code></pre>
<p>In your schema analysis pipeline, you use <code>$objectToArray</code> to capture the name and type of each top-level field in the document as follows:</p>
<pre><code class="language-javascript">var pipeline = [
  {&quot;$project&quot;: {
    &quot;_id&quot;: 0,
    &quot;schema&quot;: {
      &quot;$map&quot;: {
        &quot;input&quot;: {&quot;$objectToArray&quot;: &quot;$$ROOT&quot;},
        &quot;as&quot;: &quot;field&quot;,
        &quot;in&quot;: {
          &quot;fieldname&quot;: &quot;$$field.k&quot;,
          &quot;type&quot;: {&quot;$type&quot;: &quot;$$field.v&quot;},          
        }
      }
    }
  }}
];

db.customers.aggregate(pipeline);
</code></pre>
<p>For the two example documents in the collection, the pipeline outputs the following:</p>
<pre><code class="language-javascript">{
  schema: [
    {fieldname: '_id', type: 'objectId'},
    {fieldname: 'email', type: 'string'},
    {fieldname: 'dateOfBirth', type: 'date'},
    {fieldname: 'accNnumber', type: 'int'},
    {fieldname: 'balance', type: 'decimal'},
    {fieldname: 'address', type: 'object'},
    {fieldname: 'telNums', type: 'array'},
    {fieldname: 'optedOutOfMarketing', type: 'bool'}
  ]
},
{
  schema: [
    {fieldname: '_id', type: 'objectId'},
    {fieldname: 'email', type: 'string'},
    {fieldname: 'dateOfBirth', type: 'date'},
    {fieldname: 'accNnumber', type: 'int'},
    {fieldname: 'balance', type: 'decimal'},
    {fieldname: 'telNums', type: 'string'},
    {fieldname: 'contactPrefernece', type: 'string'}
}
</code></pre>
<p>The difficulty with this basic pipeline approach is once there are many documents in the collection, the output will be too lengthy and complex for you to detect common schema patterns. Instead, you will want to add an <code>$unwind</code> and <code>$group</code> stage combination to accumulate recurring fields that match. The generated result should also highlight if the same field name appears in multiple documents but with different data types. Here is the improved pipeline:</p>
<pre><code class="language-javascript">var pipeline = [
  {&quot;$project&quot;: {
    &quot;_id&quot;: 0,
    &quot;schema&quot;: {
      &quot;$map&quot;: {
        &quot;input&quot;: {&quot;$objectToArray&quot;: &quot;$$ROOT&quot;},
        &quot;as&quot;: &quot;field&quot;,
        &quot;in&quot;: {
          &quot;fieldname&quot;: &quot;$$field.k&quot;,
          &quot;type&quot;: {&quot;$type&quot;: &quot;$$field.v&quot;},          
        }
      }
    }
  }},
  
  {&quot;$unwind&quot;: &quot;$schema&quot;},

  {&quot;$group&quot;: {
    &quot;_id&quot;: &quot;$schema.fieldname&quot;,
    &quot;types&quot;: {&quot;$addToSet&quot;: &quot;$schema.type&quot;},
  }},
  
  {&quot;$set&quot;: {
    &quot;fieldname&quot;: &quot;$_id&quot;,
    &quot;_id&quot;: &quot;$$REMOVE&quot;,
  }},
];

db.customers.aggregate(pipeline);
</code></pre>
<p>This pipeline’s output now provides a far more comprehensible summary, as shown below:</p>
<pre><code class="language-javascript">{fieldname: '_id', types: ['objectId']},
{fieldname: 'address', types: ['object']},
{fieldname: 'email', types: ['string']},
{fieldname: 'telNums', types: ['string', 'array']},
{fieldname: 'contactPrefernece', types: ['string']},
{fieldname: 'accNnumber', types: ['int']},
{fieldname: 'balance', types: ['decimal']},
{fieldname: 'dateOfBirth', types: ['date']},
{fieldname: 'optedOutOfMarketing', types: ['bool']}
</code></pre>
<p>This result highlights that the <code>telNums</code> field can have one of two different data types within documents.</p>
<p>The main drawback of this rudimentary schema analysis pipeline is its inability to descend through layers of arrays and sub-documents hanging off each top-level document. This challenge is indeed solvable using a pure aggregation pipeline, but the code involved is far more complex and beyond the scope of this chapter. If you are interested in exploring this further, the <a href="https://github.com/pkdone/mongo-agg-schema-analyzer">&quot;mongo-agg-schema-analyzer&quot; GitHub project</a> solves this problem. That project shows you how to traverse through hierarchically structured documents using a single aggregation pipeline to infer the schema.</p>
<h2 id="further-array-manipulation-examples"><a class="header" href="#further-array-manipulation-examples">Further Array Manipulation Examples</a></h2>
<p>This book's <a href="guides/../examples/array-manipulations/array-manipulations.html">Array Manipulation Examples</a> section contains more examples of using expressions to process arrays.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aggregations-by-example"><a class="header" href="#aggregations-by-example">Aggregations By Example</a></h1>
<p>The following set of chapters provide examples to solve common data manipulation challenges grouped by different data processing requirements. The best way to use these examples is to try them out yourself as you read each example (see <a href="examples/../intro/getting-started.html">Getting Started</a> for advice on how to execute these examples).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="foundational-examples"><a class="header" href="#foundational-examples">Foundational Examples</a></h1>
<p>This section provides examples for common data manipulation patterns used in many aggregation pipelines, which are relatively straightforward to understand and adapt.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="filtered-top-subset"><a class="header" href="#filtered-top-subset">Filtered Top Subset</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.2</p>
<h2 id="scenario"><a class="header" href="#scenario">Scenario</a></h2>
<p>You want to query a collection of people to find the three youngest people who have a job in engineering, sorted by the youngest person first.</p>
<blockquote>
<p><em>This example is the only one in the book that you can also achieve entirely using MQL and serves as a helpful comparison between MQL and Aggregation Pipelines.</em></p>
</blockquote>
<h2 id="sample-data-population"><a class="header" href="#sample-data-population">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate a new <code>persons</code> collection with 6 person documents:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-filtered-top-subset&quot;);
db.dropDatabase();

// Create an index for a persons collection
db.persons.createIndex({&quot;vocation&quot;: 1, &quot;dateofbirth&quot;: 1});

// Insert records into the persons collection
db.persons.insertMany([
  {
    &quot;person_id&quot;: &quot;6392529400&quot;,
    &quot;firstname&quot;: &quot;Elise&quot;,
    &quot;lastname&quot;: &quot;Smith&quot;,
    &quot;dateofbirth&quot;: ISODate(&quot;1972-01-13T09:32:07Z&quot;),
    &quot;vocation&quot;: &quot;ENGINEER&quot;,
    &quot;address&quot;: { 
        &quot;number&quot;: 5625,
        &quot;street&quot;: &quot;Tipa Circle&quot;,
        &quot;city&quot;: &quot;Wojzinmoj&quot;,
    },
  },
  {
    &quot;person_id&quot;: &quot;1723338115&quot;,
    &quot;firstname&quot;: &quot;Olive&quot;,
    &quot;lastname&quot;: &quot;Ranieri&quot;,
    &quot;dateofbirth&quot;: ISODate(&quot;1985-05-12T23:14:30Z&quot;),    
    &quot;gender&quot;: &quot;FEMALE&quot;,
    &quot;vocation&quot;: &quot;ENGINEER&quot;,
    &quot;address&quot;: {
        &quot;number&quot;: 9303,
        &quot;street&quot;: &quot;Mele Circle&quot;,
        &quot;city&quot;: &quot;Tobihbo&quot;,
    },
  },
  {
    &quot;person_id&quot;: &quot;8732762874&quot;,
    &quot;firstname&quot;: &quot;Toni&quot;,
    &quot;lastname&quot;: &quot;Jones&quot;,
    &quot;dateofbirth&quot;: ISODate(&quot;1991-11-23T16:53:56Z&quot;),    
    &quot;vocation&quot;: &quot;POLITICIAN&quot;,
    &quot;address&quot;: {
        &quot;number&quot;: 1,
        &quot;street&quot;: &quot;High Street&quot;,
        &quot;city&quot;: &quot;Upper Abbeywoodington&quot;,
    },
  },
  {
    &quot;person_id&quot;: &quot;7363629563&quot;,
    &quot;firstname&quot;: &quot;Bert&quot;,
    &quot;lastname&quot;: &quot;Gooding&quot;,
    &quot;dateofbirth&quot;: ISODate(&quot;1941-04-07T22:11:52Z&quot;),    
    &quot;vocation&quot;: &quot;FLORIST&quot;,
    &quot;address&quot;: {
        &quot;number&quot;: 13,
        &quot;street&quot;: &quot;Upper Bold Road&quot;,
        &quot;city&quot;: &quot;Redringtonville&quot;,
    },
  },
  {
    &quot;person_id&quot;: &quot;1029648329&quot;,
    &quot;firstname&quot;: &quot;Sophie&quot;,
    &quot;lastname&quot;: &quot;Celements&quot;,
    &quot;dateofbirth&quot;: ISODate(&quot;1959-07-06T17:35:45Z&quot;),    
    &quot;vocation&quot;: &quot;ENGINEER&quot;,
    &quot;address&quot;: {
        &quot;number&quot;: 5,
        &quot;street&quot;: &quot;Innings Close&quot;,
        &quot;city&quot;: &quot;Basilbridge&quot;,
    },
  },
  {
    &quot;person_id&quot;: &quot;7363626383&quot;,
    &quot;firstname&quot;: &quot;Carl&quot;,
    &quot;lastname&quot;: &quot;Simmons&quot;,
    &quot;dateofbirth&quot;: ISODate(&quot;1998-12-26T13:13:55Z&quot;),    
    &quot;vocation&quot;: &quot;ENGINEER&quot;,
    &quot;address&quot;: {
        &quot;number&quot;: 187,
        &quot;street&quot;: &quot;Hillside Road&quot;,
        &quot;city&quot;: &quot;Kenningford&quot;,
    },
  },
]);
</code></pre>
<h2 id="aggregation-pipeline"><a class="header" href="#aggregation-pipeline">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Match engineers only
  {&quot;$match&quot;: {
    &quot;vocation&quot;: &quot;ENGINEER&quot;,
  }},
    
  // Sort by youngest person first
  {&quot;$sort&quot;: {
    &quot;dateofbirth&quot;: -1,
  }},      
    
  // Only include the first 3 youngest people
  {&quot;$limit&quot;: 3},  

  // Exclude unrequired fields from each person record
  {&quot;$unset&quot;: [
    &quot;_id&quot;,
    &quot;vocation&quot;,
    &quot;address&quot;,
  ]},    
];
</code></pre>
<h2 id="execution"><a class="header" href="#execution">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.persons.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.persons.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results"><a class="header" href="#expected-results">Expected Results</a></h2>
<p>Three documents should be returned, representing the three youngest people who are engineers (ordered by youngest first), omitting the <code>_id</code> or <code>address</code> attributes of each person, as shown below:</p>
<pre><code class="language-javascript">[
  {
    person_id: '7363626383',
    firstname: 'Carl',
    lastname: 'Simmons',
    dateofbirth: ISODate('1998-12-26T13:13:55.000Z')
  },
  {
    person_id: '1723338115',
    firstname: 'Olive',
    lastname: 'Ranieri',
    dateofbirth: ISODate('1985-05-12T23:14:30.000Z'),
    gender: 'FEMALE'
  },
  {
    person_id: '6392529400',
    firstname: 'Elise',
    lastname: 'Smith',
    dateofbirth: ISODate('1972-01-13T09:32:07.000Z')
  }
]
</code></pre>
<h2 id="observations"><a class="header" href="#observations">Observations</a></h2>
<ul>
<li>
<p><strong>Index Use.</strong> A basic aggregation pipeline, where if many records belong to the collection, a compound index for <code>vocation + dateofbirth</code> should exist to enable the database to fully optimise the execution of the pipeline combining the filter of the <code>$match</code> stage with the sort from the <code>sort</code> stage and the limit of the <code>limit</code> stage.</p>
</li>
<li>
<p><strong>Unset Use.</strong> An <code>$unset</code> stage is used rather than a <code>$project</code> stage. This enables the pipeline to avoid being verbose. More importantly, it means the pipeline does not have to be modified if a new field appears in documents added in the future (for example, see the <code>gender</code> field that appears in only <em>Olive's</em> record).</p>
</li>
<li>
<p><strong>MQL Similarity.</strong> For reference, the MQL equivalent for you to achieve the same result is shown below (you can try this in the <em>Shell</em>):</p>
<pre><code class="language-javascript">db.persons.find(
    {&quot;vocation&quot;: &quot;ENGINEER&quot;},
    {&quot;_id&quot;: 0, &quot;vocation&quot;: 0, &quot;address&quot;: 0},
  ).sort(
    {&quot;dateofbirth&quot;: -1}
  ).limit(3);
</code></pre>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="group--total"><a class="header" href="#group--total">Group &amp; Total</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.2</p>
<h2 id="scenario-1"><a class="header" href="#scenario-1">Scenario</a></h2>
<p>You want to generate a report to show what each shop customer purchased in 2020. You will group the individual order records by customer, capturing each customer's first purchase date, the number of orders they made, the total value of all their orders and a list of their order items sorted by date. </p>
<h2 id="sample-data-population-1"><a class="header" href="#sample-data-population-1">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate a new <code>orders</code> collection with 9 order documents spanning 2019-2021, for 3 different unique customers:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-group-and-total&quot;);
db.dropDatabase();

// Create index for an orders collection
db.orders.createIndex({&quot;orderdate&quot;: -1});

// Insert records into the orders collection
db.orders.insertMany([
  {
    &quot;customer_id&quot;: &quot;elise_smith@myemail.com&quot;,
    &quot;orderdate&quot;: ISODate(&quot;2020-05-30T08:35:52Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;231.43&quot;),
  },
  {
    &quot;customer_id&quot;: &quot;elise_smith@myemail.com&quot;,
    &quot;orderdate&quot;: ISODate(&quot;2020-01-13T09:32:07Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;99.99&quot;),
  },
  {
    &quot;customer_id&quot;: &quot;oranieri@warmmail.com&quot;,
    &quot;orderdate&quot;: ISODate(&quot;2020-01-01T08:25:37Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;63.13&quot;),
  },
  {
    &quot;customer_id&quot;: &quot;tj@wheresmyemail.com&quot;,
    &quot;orderdate&quot;: ISODate(&quot;2019-05-28T19:13:32Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;2.01&quot;),
  },  
  {
    &quot;customer_id&quot;: &quot;tj@wheresmyemail.com&quot;,
    &quot;orderdate&quot;: ISODate(&quot;2020-11-23T22:56:53Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;187.99&quot;),
  },
  {
    &quot;customer_id&quot;: &quot;tj@wheresmyemail.com&quot;,
    &quot;orderdate&quot;: ISODate(&quot;2020-08-18T23:04:48Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;4.59&quot;),
  },
  {
    &quot;customer_id&quot;: &quot;elise_smith@myemail.com&quot;,
    &quot;orderdate&quot;: ISODate(&quot;2020-12-26T08:55:46Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;48.50&quot;),
  },
  {
    &quot;customer_id&quot;: &quot;tj@wheresmyemail.com&quot;,
    &quot;orderdate&quot;: ISODate(&quot;2021-02-29T07:49:32Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;1024.89&quot;),
  },
  {
    &quot;customer_id&quot;: &quot;elise_smith@myemail.com&quot;,
    &quot;orderdate&quot;: ISODate(&quot;2020-10-03T13:49:44Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;102.24&quot;),
  },
]);
</code></pre>
<h2 id="aggregation-pipeline-1"><a class="header" href="#aggregation-pipeline-1">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Match only orders made in 2020
  {&quot;$match&quot;: {
    &quot;orderdate&quot;: {
      &quot;$gte&quot;: ISODate(&quot;2020-01-01T00:00:00Z&quot;),
      &quot;$lt&quot;: ISODate(&quot;2021-01-01T00:00:00Z&quot;),
    },
  }},
  
  // Sort by order date ascending (required to pick out 'first_purchase_date' below)
  {&quot;$sort&quot;: {
    &quot;orderdate&quot;: 1,
  }},      

  // Group by customer
  {&quot;$group&quot;: {
    &quot;_id&quot;: &quot;$customer_id&quot;,
    &quot;first_purchase_date&quot;: {&quot;$first&quot;: &quot;$orderdate&quot;},
    &quot;total_value&quot;: {&quot;$sum&quot;: &quot;$value&quot;},
    &quot;total_orders&quot;: {&quot;$sum&quot;: 1},
    &quot;orders&quot;: {&quot;$push&quot;: {&quot;orderdate&quot;: &quot;$orderdate&quot;, &quot;value&quot;: &quot;$value&quot;}},
  }},
  
  // Sort by each customer's first purchase date
  {&quot;$sort&quot;: {
    &quot;first_purchase_date&quot;: 1,
  }},    
  
  // Set customer's ID to be value of the field that was grouped on
  {&quot;$set&quot;: {
    &quot;customer_id&quot;: &quot;$_id&quot;,
  }},
  
  // Omit unwanted fields
  {&quot;$unset&quot;: [
    &quot;_id&quot;,
  ]},   
];
</code></pre>
<h2 id="execution-1"><a class="header" href="#execution-1">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.orders.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.orders.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-1"><a class="header" href="#expected-results-1">Expected Results</a></h2>
<p>Three documents should be returned, representing the three customers, each showing the customer's first purchase date, the total value of all their orders, the number of orders they made and a list of each order's detail, for 2020 only, as shown below:</p>
<pre><code class="language-javascript">[
  {
    customer_id: 'oranieri@warmmail.com',
    first_purchase_date: ISODate('2020-01-01T08:25:37.000Z'),
    total_value: NumberDecimal('63.13'),
    total_orders: 1,
    orders: [
      {orderdate: ISODate('2020-01-01T08:25:37.000Z'), value: NumberDecimal('63.13')}
    ]
  },
  {
    customer_id: 'elise_smith@myemail.com',
    first_purchase_date: ISODate('2020-01-13T09:32:07.000Z'),
    total_value: NumberDecimal('482.16'),
    total_orders: 4,
    orders: [
      {orderdate: ISODate('2020-01-13T09:32:07.000Z'), value: NumberDecimal('99.99')},
      {orderdate: ISODate('2020-05-30T08:35:52.000Z'), value: NumberDecimal('231.43')},
      {orderdate: ISODate('2020-10-03T13:49:44.000Z'), value: NumberDecimal('102.24')},
      {orderdate: ISODate('2020-12-26T08:55:46.000Z'), value: NumberDecimal('48.50')}
    ]
  },
  {
    customer_id: 'tj@wheresmyemail.com',
    first_purchase_date: ISODate('2020-08-18T23:04:48.000Z'),
    total_value: NumberDecimal('192.58'),
    total_orders: 2,
    orders: [
      {orderdate: ISODate('2020-08-18T23:04:48.000Z'), value: NumberDecimal('4.59')},
      {orderdate: ISODate('2020-11-23T22:56:53.000Z'), value: NumberDecimal('187.99')}
    ]
  }
]
</code></pre>
<p><em>Note, the order of fields shown for each document may vary.</em></p>
<h2 id="observations-1"><a class="header" href="#observations-1">Observations</a></h2>
<ul>
<li>
<p><strong>Double Sort Use.</strong> It is necessary to perform a <code>$sort</code> on the order date both before and after the <code>$group</code> stage. The <code>$sort</code> before the <code>$group</code> is required because the <code>$group</code> stage uses a <code>$first</code> group accumulator to capture just the first order's <code>orderdate</code> value for each grouped customer. The <code>$sort</code> after the <code>$group</code> is required because the act of having just grouped on customer ID will mean that the records are no longer sorted by purchase date for the records coming out of the <code>$group</code> stage.</p>
</li>
<li>
<p><strong>Renaming Group.</strong> Towards the end of the pipeline, you will see what is a typical pattern for pipelines that use <code>$group</code>, consisting of a combination of <code>$set</code>+<code>$unset</code> stages, to essentially take the group's key (which is always called <code>_id</code>) and substitute it with a more meaningful name (<code>customer_id</code>).</p>
</li>
<li>
<p><strong>High-Precision Decimals</strong>. You may notice the pipeline uses a <code>NumberDecimal()</code> function to ensure the order amounts in the inserted records are using a high-precision decimal type, <a href="https://docs.mongodb.com/manual/tutorial/model-monetary-data/">IEEE 754 decimal128</a>. In this example, if you use a JSON <em>float</em> or <em>double</em> type instead, the order totals will significantly lose precision. For instance, for the customer <code>elise_smith@myemail.com</code>, if you use a <em>double</em> type, the <code>total_value</code> result will have the value shown in the second line below, rather than the first line:</p>
<pre><code class="language-javascript">// Desired result achieved by using decimal128 types
total_value: NumberDecimal('482.16')

// Result that occurs if using float or double types instead
total_value: 482.15999999999997
</code></pre>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="unpack-arrays--group-differently"><a class="header" href="#unpack-arrays--group-differently">Unpack Arrays &amp; Group Differently</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.2</p>
<h2 id="scenario-2"><a class="header" href="#scenario-2">Scenario</a></h2>
<p>You want to generate a retail report to list the total value and quantity of expensive products sold (valued over 15 dollars). The source data is a list of shop orders, where each order contains the set of products purchased as part of the order.</p>
<h2 id="sample-data-population-2"><a class="header" href="#sample-data-population-2">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate a new <code>orders</code> collection where each document contains an array of products purchased:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-unpack-array-group-differently&quot;);
db.dropDatabase();

// Insert 4 records into the orders collection each with 1+ product items
db.orders.insertMany([
  {
    &quot;order_id&quot;: 6363763262239,
    &quot;products&quot;: [
      {
        &quot;prod_id&quot;: &quot;abc12345&quot;,
        &quot;name&quot;: &quot;Asus Laptop&quot;,
        &quot;price&quot;: NumberDecimal(&quot;431.43&quot;),
      },
      {
        &quot;prod_id&quot;: &quot;def45678&quot;,
        &quot;name&quot;: &quot;Karcher Hose Set&quot;,
        &quot;price&quot;: NumberDecimal(&quot;22.13&quot;),
      },
    ],
  },
  {
    &quot;order_id&quot;: 1197372932325,
    &quot;products&quot;: [
      {
        &quot;prod_id&quot;: &quot;abc12345&quot;,
        &quot;name&quot;: &quot;Asus Laptop&quot;,
        &quot;price&quot;: NumberDecimal(&quot;429.99&quot;),
      },
    ],
  },
  {
    &quot;order_id&quot;: 9812343774839,
    &quot;products&quot;: [
      {
        &quot;prod_id&quot;: &quot;pqr88223&quot;,
        &quot;name&quot;: &quot;Morphy Richardds Food Mixer&quot;,
        &quot;price&quot;: NumberDecimal(&quot;431.43&quot;),
      },
      {
        &quot;prod_id&quot;: &quot;def45678&quot;,
        &quot;name&quot;: &quot;Karcher Hose Set&quot;,
        &quot;price&quot;: NumberDecimal(&quot;21.78&quot;),
      },
    ],
  },
  {
    &quot;order_id&quot;: 4433997244387,
    &quot;products&quot;: [
      {
        &quot;prod_id&quot;: &quot;def45678&quot;,
        &quot;name&quot;: &quot;Karcher Hose Set&quot;,
        &quot;price&quot;: NumberDecimal(&quot;23.43&quot;),
      },
      {
        &quot;prod_id&quot;: &quot;jkl77336&quot;,
        &quot;name&quot;: &quot;Picky Pencil Sharpener&quot;,
        &quot;price&quot;: NumberDecimal(&quot;0.67&quot;),
      },
      {
        &quot;prod_id&quot;: &quot;xyz11228&quot;,
        &quot;name&quot;: &quot;Russell Hobbs Chrome Kettle&quot;,
        &quot;price&quot;: NumberDecimal(&quot;15.76&quot;),
      },
    ],
  },
]);
</code></pre>
<h2 id="aggregation-pipeline-2"><a class="header" href="#aggregation-pipeline-2">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Unpack each product from the each order's product as a new separate record
  {&quot;$unwind&quot;: {
    &quot;path&quot;: &quot;$products&quot;,
  }},

  // Match only products valued greater than 15.00
  {&quot;$match&quot;: {
    &quot;products.price&quot;: {
      &quot;$gt&quot;: NumberDecimal(&quot;15.00&quot;),
    },
  }},
  
  // Group by product type, capturing each product's total value + quantity
  {&quot;$group&quot;: {
    &quot;_id&quot;: &quot;$products.prod_id&quot;,
    &quot;product&quot;: {&quot;$first&quot;: &quot;$products.name&quot;},
    &quot;total_value&quot;: {&quot;$sum&quot;: &quot;$products.price&quot;},
    &quot;quantity&quot;: {&quot;$sum&quot;: 1},
  }},

  // Set product id to be the value of the field that was grouped on
  {&quot;$set&quot;: {
    &quot;product_id&quot;: &quot;$_id&quot;,
  }},
  
  // Omit unwanted fields
  {&quot;$unset&quot;: [
    &quot;_id&quot;,
  ]},   
];
</code></pre>
<h2 id="execution-2"><a class="header" href="#execution-2">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.orders.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.orders.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-2"><a class="header" href="#expected-results-2">Expected Results</a></h2>
<p>Four documents should be returned, representing only the four expensive products that were referenced multiple times in the customer orders, each showing the product's total order value and amount sold as shown below:</p>
<pre><code class="language-javascript">[
  {
    product_id: 'pqr88223',
    product: 'Morphy Richardds Food Mixer',
    total_value: NumberDecimal('431.43'),
    quantity: 1
  },
  {
    product_id: 'abc12345',
    product: 'Asus Laptop',
    total_value: NumberDecimal('861.42'),
    quantity: 2
  },
  {
    product_id: 'def45678',
    product: 'Karcher Hose Set',
    total_value: NumberDecimal('67.34'),
    quantity: 3
  },
  {
    product_id: 'xyz11228',
    product: 'Russell Hobbs Chrome Kettle',
    total_value: NumberDecimal('15.76'),
    quantity: 1
  }
]
</code></pre>
<p><em>Note, the order of fields shown for each document may vary.</em></p>
<h2 id="observations-2"><a class="header" href="#observations-2">Observations</a></h2>
<ul>
<li>
<p><strong>Unwinding Arrays.</strong> The <code>$unwind</code> stage is a powerful concept, although often unfamiliar to many developers initially. Distilled down, it does one simple thing: it generates a new record for each element in an array field of every input document. If a source collection has 3 documents and each document contains an array of 4 elements, then performing an <code>$unwind</code> on each record's array field produces 12 records (3 x 4).</p>
</li>
<li>
<p><strong>Introducing A Partial Match</strong>. The current example pipeline scans all documents in the collection and then filters out unpacked products where <code>price &gt; 15.00</code>. If the pipeline executed this filter as the first stage, it would incorrectly produce some result product records with a value of 15 dollars or less. This would be the case for an order composed of both inexpensive and expensive products. However, you can still improve the pipeline by including an additional &quot;partial match&quot; filter at the start of the pipeline for products valued at over 15 dollars. The aggregation could leverage an index (on <code>products.price</code>), resulting in a partial rather than full collection scan. This extra filter stage is beneficial if the input data set is large and many customer orders are for inexpensive items only. This approach is described in the chapter <a href="examples/foundational/../../guides/performance.html#explore-if-bringing-forward-a-partial-match-is-possible">Pipeline Performance Considerations</a>.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="distinct-list-of-values"><a class="header" href="#distinct-list-of-values">Distinct List Of Values</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.2</p>
<h2 id="scenario-3"><a class="header" href="#scenario-3">Scenario</a></h2>
<p>You want to query a collection of persons where each document contains data on one or more languages spoken by the person. The query result should be an alphabetically sorted list of unique languages that a developer can subsequently use to populate a list of values in a user interface's &quot;drop-down&quot; widget.</p>
<blockquote>
<p><em>This example is the equivalent of a <em>SELECT DISTINCT</em> statement in <a href="https://en.wikipedia.org/wiki/SQL">SQL</a>.</em></p>
</blockquote>
<h2 id="sample-data-population-3"><a class="header" href="#sample-data-population-3">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate a new <code>persons</code> collection with 9 documents:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-distinct-values&quot;);
db.dropDatabase();

// Insert records into the persons collection
db.persons.insertMany([
  {
    &quot;firstname&quot;: &quot;Elise&quot;,
    &quot;lastname&quot;: &quot;Smith&quot;,
    &quot;vocation&quot;: &quot;ENGINEER&quot;,
    &quot;language&quot;: &quot;English&quot;,
  },
  {
    &quot;firstname&quot;: &quot;Olive&quot;,
    &quot;lastname&quot;: &quot;Ranieri&quot;,
    &quot;vocation&quot;: &quot;ENGINEER&quot;,
    &quot;language&quot;: [&quot;Italian&quot;, &quot;English&quot;],
  },
  {
    &quot;firstname&quot;: &quot;Toni&quot;,
    &quot;lastname&quot;: &quot;Jones&quot;,
    &quot;vocation&quot;: &quot;POLITICIAN&quot;,
    &quot;language&quot;: [&quot;English&quot;, &quot;Welsh&quot;],
  },
  {
    &quot;firstname&quot;: &quot;Bert&quot;,
    &quot;lastname&quot;: &quot;Gooding&quot;,
    &quot;vocation&quot;: &quot;FLORIST&quot;,
    &quot;language&quot;: &quot;English&quot;,
  },
  {
    &quot;firstname&quot;: &quot;Sophie&quot;,
    &quot;lastname&quot;: &quot;Celements&quot;,
    &quot;vocation&quot;: &quot;ENGINEER&quot;,
    &quot;language&quot;: [&quot;Gaelic&quot;, &quot;English&quot;],
  },
  {
    &quot;firstname&quot;: &quot;Carl&quot;,
    &quot;lastname&quot;: &quot;Simmons&quot;,
    &quot;vocation&quot;: &quot;ENGINEER&quot;,
    &quot;language&quot;: &quot;English&quot;,
  },
  {
    &quot;firstname&quot;: &quot;Diego&quot;,
    &quot;lastname&quot;: &quot;Lopez&quot;,
    &quot;vocation&quot;: &quot;CHEF&quot;,
    &quot;language&quot;: &quot;Spanish&quot;,
  },
  {
    &quot;firstname&quot;: &quot;Helmut&quot;,
    &quot;lastname&quot;: &quot;Schneider&quot;,
    &quot;vocation&quot;: &quot;NURSE&quot;,
    &quot;language&quot;: &quot;German&quot;,
  },
  {
    &quot;firstname&quot;: &quot;Valerie&quot;,
    &quot;lastname&quot;: &quot;Dubois&quot;,
    &quot;vocation&quot;: &quot;SCIENTIST&quot;,
    &quot;language&quot;: &quot;French&quot;,
  },
]);  
</code></pre>
<h2 id="aggregation-pipeline-3"><a class="header" href="#aggregation-pipeline-3">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Unpack each language field which may be an array or a single value
  {&quot;$unwind&quot;: {
    &quot;path&quot;: &quot;$language&quot;,
  }},
  
  // Group by language
  {&quot;$group&quot;: {
    &quot;_id&quot;: &quot;$language&quot;,
  }},
  
  // Sort languages alphabetically
  {&quot;$sort&quot;: {
    &quot;_id&quot;: 1,
  }}, 
  
  // Change _id field's name to 'language'
  {&quot;$set&quot;: {
    &quot;language&quot;: &quot;$_id&quot;,
    &quot;_id&quot;: &quot;$$REMOVE&quot;,     
  }},
];
</code></pre>
<h2 id="execution-3"><a class="header" href="#execution-3">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.persons.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.persons.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-3"><a class="header" href="#expected-results-3">Expected Results</a></h2>
<p>Seven unique language names should be returned sorted in alphabetical order, as shown below:</p>
<pre><code class="language-javascript">[
  {language: 'English'},
  {language: 'French'},
  {language: 'Gaelic'},
  {language: 'German'},
  {language: 'Italian'},
  {language: 'Spanish'},
  {language: 'Welsh'}
]
</code></pre>
<h2 id="observations-3"><a class="header" href="#observations-3">Observations</a></h2>
<ul>
<li>
<p><strong>Unwinding Non-Arrays.</strong> In some of the example's documents, the <code>language</code> field is an array, whilst in others, the field is a simple string value. The <code>$unwind</code> stage can seamlessly deal with both field types and does not throw an error if it encounters a non-array value. Instead, if the field is not an array, the stage outputs a single record using the field's string value in the same way it would if the field was an array containing just one element. If you are sure the field in every document will only ever be a simple field rather than an array, you can omit this first stage (<code>$unwind</code>) from the pipeline.</p>
</li>
<li>
<p><strong>Group ID Provides Unique Values.</strong> By grouping on a single field and not accumulating other fields such as total or count, the output of a <code>$group</code> stage is just every unique group's ID, which in this case is every unique language.</p>
</li>
<li>
<p><strong>Unset Alternative.</strong> For the pipeline to be consistent with earlier examples in this book, it could have included an additional <code>$unset</code> stage to exclude the <code>_id</code> field. However, partly to show another way, the example pipeline used here marks the <code>_id</code> field for exclusion in the <code>$set</code> stage by being assigned the <code>$$REMOVE</code> variable.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="joining-data-examples"><a class="header" href="#joining-data-examples">Joining Data Examples</a></h1>
<p>This section provides examples for joining a source collection used by an aggregation pipeline with another collection using different approaches.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="one-to-one-join"><a class="header" href="#one-to-one-join">One-to-One Join</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.4    <em>(due to use of <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/first-array-element/"><code>$first</code></a> array operator)</em></p>
<h2 id="scenario-4"><a class="header" href="#scenario-4">Scenario</a></h2>
<p>You want to generate a report to list all shop purchases for 2020, showing the product's name and category for each order, rather than the product's id. To achieve this, you need to take the customer <em>orders</em> collection and join each order record to the corresponding product record in the <em>products</em> collection. There is a many:1 relationship between both collections, resulting in a 1:1 join when matching an order to a product. The join will use a single field comparison between both sides, based on the product's id.</p>
<h2 id="sample-data-population-4"><a class="header" href="#sample-data-population-4">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate new <code>products</code> and <code>orders</code> collections with documents spanning 2019-2021 (the database commands have been split in two to enable your clipboard to hold all the text - ensure you copy and execute each of the two sections):</p>
<p> <strong>-Part 1-</strong></p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-one-to-one-join&quot;);
db.dropDatabase();

// Create index for a products collection
db.products.createIndex({&quot;id&quot;: 1});

// Insert 4 records into the products collection
db.products.insertMany([
  {
    &quot;id&quot;: &quot;a1b2c3d4&quot;,
    &quot;name&quot;: &quot;Asus Laptop&quot;,
    &quot;category&quot;: &quot;ELECTRONICS&quot;,
    &quot;description&quot;: &quot;Good value laptop for students&quot;,
  },
  {
    &quot;id&quot;: &quot;z9y8x7w6&quot;,
    &quot;name&quot;: &quot;The Day Of The Triffids&quot;,
    &quot;category&quot;: &quot;BOOKS&quot;,
    &quot;description&quot;: &quot;Classic post-apocalyptic novel&quot;,
  },
  {
    &quot;id&quot;: &quot;ff11gg22hh33&quot;,
    &quot;name&quot;: &quot;Morphy Richardds Food Mixer&quot;,
    &quot;category&quot;: &quot;KITCHENWARE&quot;,
    &quot;description&quot;: &quot;Luxury mixer turning good cakes into great&quot;,
  },
  {
    &quot;id&quot;: &quot;pqr678st&quot;,
    &quot;name&quot;: &quot;Karcher Hose Set&quot;,
    &quot;category&quot;: &quot;GARDEN&quot;,
    &quot;description&quot;: &quot;Hose + nosels + winder for tidy storage&quot;,
  },
]); 
</code></pre>
<p> <strong>-Part 2-</strong></p>
<pre><code class="language-javascript">// Create index for a orders collection
db.orders.createIndex({&quot;orderdate&quot;: -1});

// Insert 4 records into the orders collection
db.orders.insertMany([
  {
    &quot;customer_id&quot;: &quot;elise_smith@myemail.com&quot;,
    &quot;orderdate&quot;: ISODate(&quot;2020-05-30T08:35:52Z&quot;),
    &quot;product_id&quot;: &quot;a1b2c3d4&quot;,
    &quot;value&quot;: NumberDecimal(&quot;431.43&quot;),
  },
  {
    &quot;customer_id&quot;: &quot;tj@wheresmyemail.com&quot;,
    &quot;orderdate&quot;: ISODate(&quot;2019-05-28T19:13:32Z&quot;),
    &quot;product_id&quot;: &quot;z9y8x7w6&quot;,
    &quot;value&quot;: NumberDecimal(&quot;5.01&quot;),
  },  
  {
    &quot;customer_id&quot;: &quot;oranieri@warmmail.com&quot;,
    &quot;orderdate&quot;: ISODate(&quot;2020-01-01T08:25:37Z&quot;),
    &quot;product_id&quot;: &quot;ff11gg22hh33&quot;,
    &quot;value&quot;: NumberDecimal(&quot;63.13&quot;),
  },
  {
    &quot;customer_id&quot;: &quot;jjones@tepidmail.com&quot;,
    &quot;orderdate&quot;: ISODate(&quot;2020-12-26T08:55:46Z&quot;),
    &quot;product_id&quot;: &quot;a1b2c3d4&quot;,
    &quot;value&quot;: NumberDecimal(&quot;429.65&quot;),
  },
]);
</code></pre>
<h2 id="aggregation-pipeline-4"><a class="header" href="#aggregation-pipeline-4">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Match only orders made in 2020
  {&quot;$match&quot;: {
    &quot;orderdate&quot;: {
      &quot;$gte&quot;: ISODate(&quot;2020-01-01T00:00:00Z&quot;),
      &quot;$lt&quot;: ISODate(&quot;2021-01-01T00:00:00Z&quot;),
    }
  }},

  // Join &quot;product_id&quot; in orders collection to &quot;id&quot; in products&quot; collection
  {&quot;$lookup&quot;: {
    &quot;from&quot;: &quot;products&quot;,
    &quot;localField&quot;: &quot;product_id&quot;,
    &quot;foreignField&quot;: &quot;id&quot;,
    &quot;as&quot;: &quot;product_mapping&quot;,
  }},

  // For this data model, will always be 1 record in right-side
  // of join, so take 1st joined array element
  {&quot;$set&quot;: {
    &quot;product_mapping&quot;: {&quot;$first&quot;: &quot;$product_mapping&quot;},
  }},
  
  // Extract the joined embeded fields into top level fields
  {&quot;$set&quot;: {
    &quot;product_name&quot;: &quot;$product_mapping.name&quot;,
    &quot;product_category&quot;: &quot;$product_mapping.category&quot;,
  }},
  
  // Omit unwanted fields
  {&quot;$unset&quot;: [
    &quot;_id&quot;,
    &quot;product_id&quot;,
    &quot;product_mapping&quot;,
  ]},     
];
</code></pre>
<h2 id="execution-4"><a class="header" href="#execution-4">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.orders.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.orders.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-4"><a class="header" href="#expected-results-4">Expected Results</a></h2>
<p>Three documents should be returned, representing the three customers orders that occurred in 2020, but with each order's <code>product_id</code> field replaced by two new looked up fields, <code>product_name</code> and <code>product_category</code>, as shown below:</p>
<pre><code class="language-javascript">[
  {
    customer_id: 'elise_smith@myemail.com',
    orderdate: ISODate('2020-05-30T08:35:52.000Z'),
    value: NumberDecimal('431.43'),
    product_name: 'Asus Laptop',
    product_category: 'ELECTRONICS'
  },
  {
    customer_id: 'oranieri@warmmail.com',
    orderdate: ISODate('2020-01-01T08:25:37.000Z'),
    value: NumberDecimal('63.13'),
    product_name: 'Morphy Richardds Food Mixer',
    product_category: 'KITCHENWARE'
  },
  {
    customer_id: 'jjones@tepidmail.com',
    orderdate: ISODate('2020-12-26T08:55:46.000Z'),
    value: NumberDecimal('429.65'),
    product_name: 'Asus Laptop',
    product_category: 'ELECTRONICS'
  }
]
</code></pre>
<h2 id="observations-4"><a class="header" href="#observations-4">Observations</a></h2>
<ul>
<li>
<p><strong>Single Field Match.</strong> The pipeline includes a <code>$lookup</code> join between a single field from each collection. For an illustration of performing a join based on two or more matching fields, see the <a href="examples/joining/./multi-one-to-many.html">Multi-Field Join &amp; One-to-Many</a> example.</p>
</li>
<li>
<p><strong>First Element Assumption.</strong> In this particular data model example, the join between the two collections is 1:1. Therefore the returned array of joined elements coming out of the <code>$lookup</code> stage always contains precisely one array element. As a result, the pipeline extracts the data from this first array element only, using the <code>$first</code> operator. For an illustration of performing a 1:many join instead, see the <a href="examples/joining/./multi-one-to-many.html">Multi-Field Join &amp; One-to-Many</a> example.</p>
</li>
<li>
<p><strong>First Element For Earlier MongoDB Versions.</strong> MongoDB only introduced the <code>$first</code> array operator expression in version 4.4. However, it is straightforward for you to replace its use in the pipeline with an equivalent solution, using the <code>$arrayElemAt</code> operator, to then allow the pipeline to work in MongoDB versions before 4.4:</p>
<pre><code class="language-javascript">// $first equivalent
&quot;product_mapping&quot;: {&quot;$arrayElemAt&quot;: [&quot;$product_mapping&quot;, 0]},
</code></pre>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="multi-field-join--one-to-many"><a class="header" href="#multi-field-join--one-to-many">Multi-Field Join &amp; One-to-Many</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.2</p>
<h2 id="scenario-5"><a class="header" href="#scenario-5">Scenario</a></h2>
<p>You want to generate a report to list all the orders made for each product in 2020. To achieve this, you need to take a shop's <em>products</em> collection and join each product record to all its orders stored in an <em>orders</em> collection. There is a 1:many relationship between both collections, based on a match of two fields on each side. Rather than joining on a single field like <code>product_id</code> (which doesn't exist in this data set), you need to use two common fields to join (<code>product_name</code> and <code>product_variation</code>). </p>
<blockquote>
<p><em>The requirement to perform a 1:many join does not mandate the need to join by multiple fields on each side of the join. However, in this example, it has been deemed beneficial to show both of these aspects in one place.</em></p>
</blockquote>
<h2 id="sample-data-population-5"><a class="header" href="#sample-data-population-5">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate new <code>products</code> and <code>orders</code> collections with documents spanning 2019-2021 (the database commands have been split in two to enable your clipboard to hold all the text - ensure you copy and execute each of the two sections):</p>
<p> <strong>-Part 1-</strong></p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-multi-one-to-many&quot;);
db.dropDatabase();

// Insert 6 records into the products collection
db.products.insertMany([
  {
    &quot;name&quot;: &quot;Asus Laptop&quot;,
    &quot;variation&quot;: &quot;Ultra HD&quot;,
    &quot;category&quot;: &quot;ELECTRONICS&quot;,
    &quot;description&quot;: &quot;Great for watching movies&quot;,
  },
  {
    &quot;name&quot;: &quot;Asus Laptop&quot;,
    &quot;variation&quot;: &quot;Normal Display&quot;,
    &quot;category&quot;: &quot;ELECTRONICS&quot;,
    &quot;description&quot;: &quot;Good value laptop for students&quot;,
  },
  {
    &quot;name&quot;: &quot;The Day Of The Triffids&quot;,
    &quot;variation&quot;: &quot;1st Edition&quot;,
    &quot;category&quot;: &quot;BOOKS&quot;,
    &quot;description&quot;: &quot;Classic post-apocalyptic novel&quot;,
  },
  {
    &quot;name&quot;: &quot;The Day Of The Triffids&quot;,
    &quot;variation&quot;: &quot;2nd Edition&quot;,
    &quot;category&quot;: &quot;BOOKS&quot;,
    &quot;description&quot;: &quot;Classic post-apocalyptic novel&quot;,
  },
  {
    &quot;name&quot;: &quot;Morphy Richards Food Mixer&quot;,
    &quot;variation&quot;: &quot;Deluxe&quot;,
    &quot;category&quot;: &quot;KITCHENWARE&quot;,
    &quot;description&quot;: &quot;Luxury mixer turning good cakes into great&quot;,
  },
  {
    &quot;name&quot;: &quot;Karcher Hose Set&quot;,
    &quot;variation&quot;: &quot;Full Monty&quot;,
    &quot;category&quot;: &quot;GARDEN&quot;,
    &quot;description&quot;: &quot;Hose + nosels + winder for tidy storage&quot;,
  },
]); 
</code></pre>
<p> <strong>-Part 2-</strong></p>
<pre><code class="language-javascript">// Create index for the orders collection
db.orders.createIndex({&quot;product_name&quot;: 1, &quot;product_variation&quot;: 1});

// Insert 4 records into the orders collection
db.orders.insertMany([
  {
    &quot;customer_id&quot;: &quot;elise_smith@myemail.com&quot;,
    &quot;orderdate&quot;: ISODate(&quot;2020-05-30T08:35:52Z&quot;),
    &quot;product_name&quot;: &quot;Asus Laptop&quot;,
    &quot;product_variation&quot;: &quot;Normal Display&quot;,
    &quot;value&quot;: NumberDecimal(&quot;431.43&quot;),
  },
  {
    &quot;customer_id&quot;: &quot;tj@wheresmyemail.com&quot;,
    &quot;orderdate&quot;: ISODate(&quot;2019-05-28T19:13:32Z&quot;),
    &quot;product_name&quot;: &quot;The Day Of The Triffids&quot;,
    &quot;product_variation&quot;: &quot;2nd Edition&quot;,
    &quot;value&quot;: NumberDecimal(&quot;5.01&quot;),
  },  
  {
    &quot;customer_id&quot;: &quot;oranieri@warmmail.com&quot;,
    &quot;orderdate&quot;: ISODate(&quot;2020-01-01T08:25:37Z&quot;),
    &quot;product_name&quot;: &quot;Morphy Richards Food Mixer&quot;,
    &quot;product_variation&quot;: &quot;Deluxe&quot;,
    &quot;value&quot;: NumberDecimal(&quot;63.13&quot;),
  },
  {
    &quot;customer_id&quot;: &quot;jjones@tepidmail.com&quot;,
    &quot;orderdate&quot;: ISODate(&quot;2020-12-26T08:55:46Z&quot;),
    &quot;product_name&quot;: &quot;Asus Laptop&quot;,
    &quot;product_variation&quot;: &quot;Normal Display&quot;,
    &quot;value&quot;: NumberDecimal(&quot;429.65&quot;),
  },
]);
</code></pre>
<h2 id="aggregation-pipeline-5"><a class="header" href="#aggregation-pipeline-5">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Join by 2 fields in products collection to 2 fields in orders collection
  {&quot;$lookup&quot;: {
    &quot;from&quot;: &quot;orders&quot;,
    &quot;let&quot;: {
      &quot;prdname&quot;: &quot;$name&quot;,
      &quot;prdvartn&quot;: &quot;$variation&quot;,
    },
    // Embedded pipeline to control how the join is matched
    &quot;pipeline&quot;: [
      // Join by two fields in each side
      {&quot;$match&quot;:
        {&quot;$expr&quot;:
          {&quot;$and&quot;: [
            {&quot;$eq&quot;: [&quot;$product_name&quot;,  &quot;$$prdname&quot;]},
            {&quot;$eq&quot;: [&quot;$product_variation&quot;,  &quot;$$prdvartn&quot;]},            
          ]},
        },
      },

      // Match only orders made in 2020
      {&quot;$match&quot;: {
        &quot;orderdate&quot;: {
          &quot;$gte&quot;: ISODate(&quot;2020-01-01T00:00:00Z&quot;),
          &quot;$lt&quot;: ISODate(&quot;2021-01-01T00:00:00Z&quot;),
        }
      }},
      
      // Exclude some unwanted fields from the right side of the join
      {&quot;$unset&quot;: [
        &quot;_id&quot;,
        &quot;product_name&quot;,
        &quot;product_variation&quot;,
      ]},
    ],
    as: &quot;orders&quot;,
  }},

  // Only show products that have at least one order
  {&quot;$match&quot;: {
    &quot;orders&quot;: {&quot;$ne&quot;: []},
  }},

  // Omit unwanted fields
  {&quot;$unset&quot;: [
    &quot;_id&quot;,
  ]}, 
];
</code></pre>
<h2 id="execution-5"><a class="header" href="#execution-5">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.products.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.products.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-5"><a class="header" href="#expected-results-5">Expected Results</a></h2>
<p>Two documents should be returned, representing the two products that had one or more orders in 2020, with the orders embedded in an array against each product, as shown below:</p>
<pre><code class="language-javascript">[
  {
    name: 'Asus Laptop',
    variation: 'Normal Display',
    category: 'ELECTRONICS',
    description: 'Good value laptop for students',
    orders: [
      {
        customer_id: 'elise_smith@myemail.com',
        orderdate: ISODate('2020-05-30T08:35:52.000Z'),
        value: NumberDecimal('431.43')
      },
      {
        customer_id: 'jjones@tepidmail.com',
        orderdate: ISODate('2020-12-26T08:55:46.000Z'),
        value: NumberDecimal('429.65')
      }
    ]
  },
  {
    name: 'Morphy Richards Food Mixer',
    variation: 'Deluxe',
    category: 'KITCHENWARE',
    description: 'Luxury mixer turning good cakes into great',
    orders: [
      {
        customer_id: 'oranieri@warmmail.com',
        orderdate: ISODate('2020-01-01T08:25:37.000Z'),
        value: NumberDecimal('63.13')
      }
    ]
  }
]
</code></pre>
<h2 id="observations-5"><a class="header" href="#observations-5">Observations</a></h2>
<ul>
<li>
<p><strong>Multiple Join Fields.</strong> To perform a join of two or more fields between the two collections, you need to use a <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/lookup/#join-conditions-and-uncorrelated-sub-queries"><code>let</code></a> parameter rather than specifying the <code>localField</code> and <code>foreignField</code> parameters used in a single field join. With a <code>let</code> parameter, you bind multiple fields from the first collection into variables ready to be used in the joining process. You use an embedded <code>pipeline</code> inside the <code>$lookup</code> stage to match the <em>bind</em> variables with fields in the second collection's records. In this instance, because the <code>$expr</code> operator performs an equality comparison specifically (as opposed to a range comparison), the aggregation runtime can employ an appropriate index for this match.</p>
</li>
<li>
<p><strong>Reducing Array Content.</strong> The presence of an embedded pipeline in the <code>$lookup</code> stage provides an opportunity to filter out three unwanted fields brought in from the second collection. Instead, you could use an <code>$unset</code> stage later in the top-level pipeline to project out these unwanted array elements. If you need to perform more complex array content filtering rules, you can use the approach described in section <em><a href="examples/joining/../../guides/performance.html#2-avoid-unwinding--regrouping-documents-just-to-process-array-elements">2. Avoid Unwinding &amp; Regrouping Documents Just To Process Array Elements</a></em> of the <em>Pipeline Performance Considerations</em> chapter.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-types-conversion-examples"><a class="header" href="#data-types-conversion-examples">Data Types Conversion Examples</a></h1>
<p>This section provides examples for converting weakly typed fields, represented as strings in documents, to strongly typed fields that are easier for applications to query and use.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="strongly-typed-conversion"><a class="header" href="#strongly-typed-conversion">Strongly-Typed Conversion</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.2</p>
<h2 id="scenario-6"><a class="header" href="#scenario-6">Scenario</a></h2>
<p>A 3rd party has imported a set of <em>retail orders</em> into a MongoDB collection but with all data typing lost (it stored all field values as strings). You want to re-establish correct typing for all the documents and copy them into a new &quot;cleaned&quot; collection. You can incorporate such type transformation logic in the aggregation pipeline because you know the type each field had in the original record structure.</p>
<blockquote>
<p><em>Unlike most examples in this book, the aggregation pipeline writes its output to a collection rather than streaming the results back to the calling application.</em></p>
</blockquote>
<h2 id="sample-data-population-6"><a class="header" href="#sample-data-population-6">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate a new <code>orders</code> collection with three orders documents, where each order has text fields only (note, the second document is intentionality missing the field <code>reported</code> in the sub-document <code>further_info</code>):</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-convert-to-strongly-typed&quot;);
db.dropDatabase();

// Insert orders documents
db.orders.insertMany([
  {
    &quot;customer_id&quot;: &quot;elise_smith@myemail.com&quot;,
    &quot;order_date&quot;: &quot;2020-05-30T08:35:52&quot;,
    &quot;value&quot;: &quot;231.43&quot;,
    &quot;further_info&quot;: {
      &quot;item_qty&quot;: &quot;3&quot;,
      &quot;reported&quot;: &quot;false&quot;,
    },
  },
  {
    &quot;customer_id&quot;: &quot;oranieri@warmmail.com&quot;,
    &quot;order_date&quot;: &quot;2020-01-01T08:25:37&quot;,
    &quot;value&quot;: &quot;63.13&quot;,
    &quot;further_info&quot;: {
      &quot;item_qty&quot;: &quot;2&quot;,
    },
  },
  {
    &quot;customer_id&quot;: &quot;tj@wheresmyemail.com&quot;,
    &quot;order_date&quot;: &quot;2019-05-28T19:13:32&quot;,
    &quot;value&quot;: &quot;2.01&quot;,
    &quot;further_info&quot;: {
      &quot;item_qty&quot;: &quot;1&quot;,
      &quot;reported&quot;: &quot;true&quot;,
    },
  },  
]);
</code></pre>
<h2 id="aggregation-pipeline-6"><a class="header" href="#aggregation-pipeline-6">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Convert strings to required types
  {&quot;$set&quot;: {
    &quot;order_date&quot;: {&quot;$toDate&quot;: &quot;$order_date&quot;},    
    &quot;value&quot;: {&quot;$toDecimal&quot;: &quot;$value&quot;},
    &quot;further_info.item_qty&quot;: {&quot;$toInt&quot;: &quot;$further_info.item_qty&quot;},
    &quot;further_info.reported&quot;: {&quot;$switch&quot;: {
      &quot;branches&quot;: [
        {&quot;case&quot;: {&quot;$eq&quot;: [{&quot;$toLower&quot;: &quot;$further_info.reported&quot;}, &quot;true&quot;]}, &quot;then&quot;: true},
        {&quot;case&quot;: {&quot;$eq&quot;: [{&quot;$toLower&quot;: &quot;$further_info.reported&quot;}, &quot;false&quot;]}, &quot;then&quot;: false},
      ],
      &quot;default&quot;: {&quot;$ifNull&quot;: [&quot;$further_info.reported&quot;, &quot;$$REMOVE&quot;]},
    }},     
  }},     
  
  // Output to an unsharded or sharded collection
  {&quot;$merge&quot;: {
    &quot;into&quot;: &quot;orders_typed&quot;,
  }},    
];
</code></pre>
<h2 id="execution-6"><a class="header" href="#execution-6">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline to generate and populate a new collection called <code>orders_typed</code>:</p>
<pre><code class="language-javascript">db.orders.aggregate(pipeline);
</code></pre>
<p>Check the contents of the new <code>orders_typed</code> collection to ensure the relevant fields are now appropriately typed:</p>
<pre><code class="language-javascript">db.orders_typed.find();
</code></pre>
<p>View the explain plan for the aggregation pipeline:</p>
<pre><code class="language-javascript">db.orders.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-6"><a class="header" href="#expected-results-6">Expected Results</a></h2>
<p>The same number of documents should appear in the new <code>orders_typed</code> collection as the source collection had, with the same field structure and fields names, but now using strongly-typed boolean/date/integer/decimal values where appropriate, as shown below:</p>
<pre><code class="language-javascript">[
  {
    _id: ObjectId('6064381b7aa89666258201fd'),
    customer_id: 'elise_smith@myemail.com',
    further_info: { 
      item_qty: 3, 
      reported: false 
    },
    order_date: ISODate('2020-05-30T08:35:52.000Z'),
    value: NumberDecimal('231.43')
  },
  {
    _id: ObjectId('6064381b7aa89666258201fe'),
    customer_id: 'oranieri@warmmail.com',
    further_info: {
      item_qty: 2 
    },
    order_date: ISODate('2020-01-01T08:25:37.000Z'),
    value: NumberDecimal('63.13')
  },
  {
    _id: ObjectId('6064381b7aa89666258201ff'),
    customer_id: 'tj@wheresmyemail.com',\
    further_info: {
      item_qty: 1,
      reported: true
    },
    order_date: ISODate('2019-05-28T19:13:32.000Z'),
    value: NumberDecimal('2.01')
  }
]
</code></pre>
<h2 id="observations-6"><a class="header" href="#observations-6">Observations</a></h2>
<ul>
<li>
<p><strong>Boolean Conversion.</strong> The pipeline's conversions for integers, decimals, and dates are straightforward using the corresponding operator expressions, <code>$toInt</code>, <code>$toDecimal</code> and <code>$toDate</code>. However, the operator expression <code>$toBool</code> is not used for the boolean conversion. This is because <code>$toBool</code> will convert any non-empty string to <em>true</em> regardless of its value. As a result, the pipeline uses a <code>$switch</code> operator to compare the lowercase version of strings with the text <code>'true'</code> and <code>'false'</code>, returning the matching boolean.</p>
</li>
<li>
<p><strong>Preserving Non-Existence.</strong> The field <code>further_info.reported</code> is an optional field in this scenario. The field may not always appear in a document, as illustrated by one of the three documents in the example. If a field is not present in a document, this potentially significant fact should never be lost. The pipeline includes additional logic for the <code>further_info.reported</code> field to preserve this information. The pipeline ensures the field is not included in the output document if it didn't exist in the source document. A <code>$ifNull</code> conditional operator is used, which returns the <code>$$REMOVE</code> marker flag if the field is missing, instructing the aggregation engine to omit it.</p>
</li>
<li>
<p><strong>Output To A Collection.</strong> The pipeline uses a <code>$merge</code> stage to instruct the aggregation engine to write the output to a collection rather than returning a stream of results. For this example, the default settings for <code>$merge</code> are sufficient. Each transformed record coming out of the aggregation pipeline becomes a new record in the target collection. The pipeline could have used a <code>$out</code> rather than a <code>$merge</code> stage. However, because <code>$merge</code> supports both unsharded and sharded collections, whereas <code>$out</code> only supports the former, <code>$merge</code> provides a more universally applicable example. If your aggregation needs to create a brand new unsharded collection, <code>$out</code> may be a little faster because the aggregation will completely replace the existing collection if it exists. Using <code>$merge</code>, the system has to perform more checks for every record the aggregation inserts (even though, in this case, it will be to a new collection).</p>
</li>
<li>
<p><strong>Trickier Date Conversions.</strong> In this example, the date strings contain all the date parts required by the <code>$toDate</code> operator to perform a conversion correctly. In some situations, this may not be the case, and a date string may be missing some valuable information (e.g. which century a 2-character year string is for, such as the century <code>19</code> or <code>21</code>). To understand how to deal with these cases, see the <a href="examples/type-convert/./convert-incomplete-dates.html">Convert Incomplete Date Strings</a> example chapter.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="convert-incomplete-date-strings"><a class="header" href="#convert-incomplete-date-strings">Convert Incomplete Date Strings</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.2</p>
<h2 id="scenario-7"><a class="header" href="#scenario-7">Scenario</a></h2>
<p>An application is ingesting <em>payment</em> documents into a MongoDB collection where each document's <em>payment date</em> field contains a string looking vaguely like a date-time, such as <code>&quot;01-JAN-20 01.01.01.123000000&quot;</code>. You want to convert each <em>payment date</em> into a valid BSON date type when aggregating the payments. However, the payment date fields do not contain all the information required for you to determine the exact date-time accurately. Therefore you cannot use just the MongoDB's <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/#date-expression-operators">Date Operator Expressions</a> directly to perform the text-to-date conversion. Each of these text fields is missing the following information:</p>
<ul>
<li>The specific <strong>century</strong> (1900s?, 2000s, other?)</li>
<li>The specific <strong>time-zone</strong> (GMT?, IST?, PST?, other?) </li>
<li>The specific <strong>language</strong> that the three-letter month abbreviation represents (is &quot;JAN&quot; in French? in English? other?)</li>
</ul>
<p>You subsequently learn that all the payment records are for the <strong>21st century</strong> only, the time-zone used when ingesting the data is <strong>UTC</strong>, and the language used is <strong>English</strong>. Armed with this information, you build an aggregation pipeline to transform these text fields into date fields.</p>
<h2 id="sample-data-population-7"><a class="header" href="#sample-data-population-7">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate a new <em>payments</em> collection with 12 sample payments documents, providing coverage across all 12 months for the year 2020, with random time elements.</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-convert-incomplete-dates&quot;);
db.dropDatabase();

// Insert records into the payments collection
db.payments.insertMany([
  {&quot;account&quot;: &quot;010101&quot;, &quot;paymentDate&quot;: &quot;01-JAN-20 01.01.01.123000000&quot;, &quot;amount&quot;: 1.01},
  {&quot;account&quot;: &quot;020202&quot;, &quot;paymentDate&quot;: &quot;02-FEB-20 02.02.02.456000000&quot;, &quot;amount&quot;: 2.02},
  {&quot;account&quot;: &quot;030303&quot;, &quot;paymentDate&quot;: &quot;03-MAR-20 03.03.03.789000000&quot;, &quot;amount&quot;: 3.03},
  {&quot;account&quot;: &quot;040404&quot;, &quot;paymentDate&quot;: &quot;04-APR-20 04.04.04.012000000&quot;, &quot;amount&quot;: 4.04},
  {&quot;account&quot;: &quot;050505&quot;, &quot;paymentDate&quot;: &quot;05-MAY-20 05.05.05.345000000&quot;, &quot;amount&quot;: 5.05},
  {&quot;account&quot;: &quot;060606&quot;, &quot;paymentDate&quot;: &quot;06-JUN-20 06.06.06.678000000&quot;, &quot;amount&quot;: 6.06},
  {&quot;account&quot;: &quot;070707&quot;, &quot;paymentDate&quot;: &quot;07-JUL-20 07.07.07.901000000&quot;, &quot;amount&quot;: 7.07},
  {&quot;account&quot;: &quot;080808&quot;, &quot;paymentDate&quot;: &quot;08-AUG-20 08.08.08.234000000&quot;, &quot;amount&quot;: 8.08},
  {&quot;account&quot;: &quot;090909&quot;, &quot;paymentDate&quot;: &quot;09-SEP-20 09.09.09.567000000&quot;, &quot;amount&quot;: 9.09},
  {&quot;account&quot;: &quot;101010&quot;, &quot;paymentDate&quot;: &quot;10-OCT-20 10.10.10.890000000&quot;, &quot;amount&quot;: 10.10},
  {&quot;account&quot;: &quot;111111&quot;, &quot;paymentDate&quot;: &quot;11-NOV-20 11.11.11.111000000&quot;, &quot;amount&quot;: 11.11},
  {&quot;account&quot;: &quot;121212&quot;, &quot;paymentDate&quot;: &quot;12-DEC-20 12.12.12.999000000&quot;, &quot;amount&quot;: 12.12}
]);
</code></pre>
<h2 id="aggregation-pipeline-7"><a class="header" href="#aggregation-pipeline-7">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Change field from a string to a date, filling in the gaps
  {&quot;$set&quot;: {
    &quot;paymentDate&quot;: {    
      &quot;$let&quot;: {
        &quot;vars&quot;: {
          &quot;txt&quot;: &quot;$paymentDate&quot;,  // Assign &quot;paymentDate&quot; field to variable &quot;txt&quot;,
          &quot;month&quot;: {&quot;$substrCP&quot;: [&quot;$paymentDate&quot;, 3, 3]},  // Extract month text
        },
        &quot;in&quot;: { 
          &quot;$dateFromString&quot;: {&quot;format&quot;: &quot;%d-%m-%Y %H.%M.%S.%L&quot;, &quot;dateString&quot;:
            {&quot;$concat&quot;: [
              {&quot;$substrCP&quot;: [&quot;$$txt&quot;, 0, 3]},  // Use 1st 3 chars in string
              {&quot;$switch&quot;: {&quot;branches&quot;: [  // Replace month 3 chars with month number
                {&quot;case&quot;: {&quot;$eq&quot;: [&quot;$$month&quot;, &quot;JAN&quot;]}, &quot;then&quot;: &quot;01&quot;},
                {&quot;case&quot;: {&quot;$eq&quot;: [&quot;$$month&quot;, &quot;FEB&quot;]}, &quot;then&quot;: &quot;02&quot;},
                {&quot;case&quot;: {&quot;$eq&quot;: [&quot;$$month&quot;, &quot;MAR&quot;]}, &quot;then&quot;: &quot;03&quot;},
                {&quot;case&quot;: {&quot;$eq&quot;: [&quot;$$month&quot;, &quot;APR&quot;]}, &quot;then&quot;: &quot;04&quot;},
                {&quot;case&quot;: {&quot;$eq&quot;: [&quot;$$month&quot;, &quot;MAY&quot;]}, &quot;then&quot;: &quot;05&quot;},
                {&quot;case&quot;: {&quot;$eq&quot;: [&quot;$$month&quot;, &quot;JUN&quot;]}, &quot;then&quot;: &quot;06&quot;},
                {&quot;case&quot;: {&quot;$eq&quot;: [&quot;$$month&quot;, &quot;JUL&quot;]}, &quot;then&quot;: &quot;07&quot;},
                {&quot;case&quot;: {&quot;$eq&quot;: [&quot;$$month&quot;, &quot;AUG&quot;]}, &quot;then&quot;: &quot;08&quot;},
                {&quot;case&quot;: {&quot;$eq&quot;: [&quot;$$month&quot;, &quot;SEP&quot;]}, &quot;then&quot;: &quot;09&quot;},
                {&quot;case&quot;: {&quot;$eq&quot;: [&quot;$$month&quot;, &quot;OCT&quot;]}, &quot;then&quot;: &quot;10&quot;},
                {&quot;case&quot;: {&quot;$eq&quot;: [&quot;$$month&quot;, &quot;NOV&quot;]}, &quot;then&quot;: &quot;11&quot;},
                {&quot;case&quot;: {&quot;$eq&quot;: [&quot;$$month&quot;, &quot;DEC&quot;]}, &quot;then&quot;: &quot;12&quot;},
               ], &quot;default&quot;: &quot;ERROR&quot;}},
              &quot;-20&quot;,  // Add hyphen + hardcoded century 2 digits
              {&quot;$substrCP&quot;: [&quot;$$txt&quot;, 7, 15]}  // Use time up to 3 millis (ignore last 6 nanosecs)
            ]
          }}                  
        }
      }        
    },             
  }},

  // Omit unwanted fields
  {&quot;$unset&quot;: [
    &quot;_id&quot;,
  ]},         
];
</code></pre>
<h2 id="execution-7"><a class="header" href="#execution-7">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.payments.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.payments.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-7"><a class="header" href="#expected-results-7">Expected Results</a></h2>
<p>Twelve documents should be returned, corresponding to the original twelve source documents, but this time with the <code>paymentDate</code> field converted from text values to proper date typed values, as shown below:</p>
<pre><code class="language-javascript">[
  {
    account: '010101',
    paymentDate: ISODate('2020-01-01T01:01:01.123Z'),
    amount: 1.01
  },
  {
    account: '020202',
    paymentDate: ISODate('2020-02-02T02:02:02.456Z'),
    amount: 2.02
  },
  {
    account: '030303',
    paymentDate: ISODate('2020-03-03T03:03:03.789Z'),
    amount: 3.03
  },
  {
    account: '040404',
    paymentDate: ISODate('2020-04-04T04:04:04.012Z'),
    amount: 4.04
  },
  {
    account: '050505',
    paymentDate: ISODate('2020-05-05T05:05:05.345Z'),
    amount: 5.05
  },
  {
    account: '060606',
    paymentDate: ISODate('2020-06-06T06:06:06.678Z'),
    amount: 6.06
  },
  {
    account: '070707',
    paymentDate: ISODate('2020-07-07T07:07:07.901Z'),
    amount: 7.07
  },
  {
    account: '080808',
    paymentDate: ISODate('2020-08-08T08:08:08.234Z'),
    amount: 8.08
  },
  {
    account: '090909',
    paymentDate: ISODate('2020-09-09T09:09:09.567Z'),
    amount: 9.09
  },
  {
    account: '101010',
    paymentDate: ISODate('2020-10-10T10:10:10.890Z'),
    amount: 10.1
  },
  {
    account: '111111',
    paymentDate: ISODate('2020-11-11T11:11:11.111Z'),
    amount: 11.11
  },
  {
    account: '121212',
    paymentDate: ISODate('2020-12-12T12:12:12.999Z'),
    amount: 12.12
  }
]
</code></pre>
<h2 id="observations-7"><a class="header" href="#observations-7">Observations</a></h2>
<ul>
<li>
<p><strong>Concatenation Explanation.</strong> In this pipeline, the text fields (e.g. <code>'12-DEC-20 12.12.12.999000000'</code>) are each converted to date fields (e.g. <code>2020-12-12T12:12:12.999Z</code>). This is achieved by concatenating together the following four example elements before passing them to the <code>$dateFromString</code> operator to convert to a date type:</p>
<ul>
<li><code>'12-'</code> <em>(day of the month from the input string + the hyphen suffix already present in the text)</em></li>
<li><code>'12'</code> <em>(replacing 'DEC')</em></li>
<li><code>'-20'</code> <em>(hard-coded hyphen + hardcoded century)</em></li>
<li><code>'20 12.12.12.999'</code> <em>(the rest of input string apart from the last 6 nanosecond digits)</em></li>
</ul>
</li>
<li>
<p><strong>Temporary Reusable Variables.</strong> The pipeline includes a <code>$let</code> operator to define two variables ready to be reused in multiple places in the central part of the data conversion logic belonging to the <code>$dateFromString</code> operator. The <code>txt</code> variable provides a minor convenience to ensure the main part of the expression logic works regardless of whether the referenced field path is currently named <code>$paymentDate</code> or changes in a future version of the source collection (e.g. to <code>$transactionDate</code>). The <code>month</code> variable is more valuable, ensuring that the pipeline does not have to repeat the same 'substring' logic in multiple places. </p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="trend-analysis-examples"><a class="header" href="#trend-analysis-examples">Trend Analysis Examples</a></h1>
<p>This section provides examples of analysing data sets to identify different trends, categorisations and relationships.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="faceted-classification"><a class="header" href="#faceted-classification">Faceted Classification</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.2</p>
<h2 id="scenario-8"><a class="header" href="#scenario-8">Scenario</a></h2>
<p>You want to provide a <a href="https://en.wikipedia.org/wiki/Faceted_search">faceted search</a> capability on your retail website to enable customers to refine their product search by selecting specific characteristics against the product results listed in the web page. It is beneficial to classify the products by different dimensions, where each dimension, or facet, corresponds to a particular field in a product record (e.g. <em>product rating</em>, <em>product price</em>). Each facet should be broken down into sub-ranges so that a customer can select a specific sub-range (<em>4 - 5 stars</em>) for a particular facet (e.g. <em>rating</em>). The aggregation pipeline will analyse the <em>products</em> collection by each facet's field (<em>rating</em> and <em>price</em>) to determine each facet's spread of values.</p>
<h2 id="sample-data-population-8"><a class="header" href="#sample-data-population-8">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate a new <code>products</code> collection with 16 documents (the database commands have been split in two to enable your clipboard to hold all the text - ensure you copy and execute each of the two sections):</p>
<p> <strong>-Part 1-</strong></p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-faceted-classfctn&quot;);
db.dropDatabase();

// Insert first 8 records into the collection
db.products.insertMany([
  {
    &quot;name&quot;: &quot;Asus Laptop&quot;,
    &quot;category&quot;: &quot;ELECTRONICS&quot;,
    &quot;description&quot;: &quot;Good value laptop for students&quot;,
    &quot;price&quot;: NumberDecimal(&quot;431.43&quot;),
    &quot;rating&quot;: NumberDecimal(&quot;4.2&quot;),
  },
  {
    &quot;name&quot;: &quot;The Day Of The Triffids&quot;,
    &quot;category&quot;: &quot;BOOKS&quot;,
    &quot;description&quot;: &quot;Classic post-apocalyptic novel&quot;,
    &quot;price&quot;: NumberDecimal(&quot;5.01&quot;),
    &quot;rating&quot;: NumberDecimal(&quot;4.8&quot;),
  },
  {
    &quot;name&quot;: &quot;Morphy Richardds Food Mixer&quot;,
    &quot;category&quot;: &quot;KITCHENWARE&quot;,
    &quot;description&quot;: &quot;Luxury mixer turning good cakes into great&quot;,
    &quot;price&quot;: NumberDecimal(&quot;63.13&quot;),
    &quot;rating&quot;: NumberDecimal(&quot;3.8&quot;),
  },
  {
    &quot;name&quot;: &quot;Karcher Hose Set&quot;,
    &quot;category&quot;: &quot;GARDEN&quot;,
    &quot;description&quot;: &quot;Hose + nosels + winder for tidy storage&quot;,
    &quot;price&quot;: NumberDecimal(&quot;22.13&quot;),
    &quot;rating&quot;: NumberDecimal(&quot;4.3&quot;),
  },
  {
    &quot;name&quot;: &quot;Oak Coffee Table&quot;,
    &quot;category&quot;: &quot;HOME&quot;,
    &quot;description&quot;: &quot;size is 2m x 0.5m x 0.4m&quot;,
    &quot;price&quot;: NumberDecimal(&quot;22.13&quot;),
    &quot;rating&quot;: NumberDecimal(&quot;3.8&quot;),
  },
  {
    &quot;name&quot;: &quot;Lenovo Laptop&quot;,
    &quot;category&quot;: &quot;ELECTRONICS&quot;,
    &quot;description&quot;: &quot;High spec good for gaming&quot;,
    &quot;price&quot;: NumberDecimal(&quot;1299.99&quot;),
    &quot;rating&quot;: NumberDecimal(&quot;4.1&quot;),
  },
  {
    &quot;name&quot;: &quot;One Day in the Life of Ivan Denisovich&quot;,
    &quot;category&quot;: &quot;BOOKS&quot;,
    &quot;description&quot;: &quot;Brutal life in a labour camp&quot;,
    &quot;price&quot;: NumberDecimal(&quot;4.29&quot;),
    &quot;rating&quot;: NumberDecimal(&quot;4.9&quot;),
  },
  {
    &quot;name&quot;: &quot;Russell Hobbs Chrome Kettle&quot;,
    &quot;category&quot;: &quot;KITCHENWARE&quot;,
    &quot;description&quot;: &quot;Nice looking budget kettle&quot;,
    &quot;price&quot;: NumberDecimal(&quot;15.76&quot;),
    &quot;rating&quot;: NumberDecimal(&quot;3.9&quot;),
  },
]);   
</code></pre>
<p> <strong>-Part 2-</strong></p>
<pre><code class="language-javascript">// Insert second 8 records into the collection
db.products.insertMany([  
  {
    &quot;name&quot;: &quot;Tiffany Gold Chain&quot;,
    &quot;category&quot;: &quot;JEWELERY&quot;,
    &quot;description&quot;: &quot;Looks great for any age and gender&quot;,
    &quot;price&quot;: NumberDecimal(&quot;582.22&quot;),
    &quot;rating&quot;: NumberDecimal(&quot;4.0&quot;),
  },
  {
    &quot;name&quot;: &quot;Raleigh Racer 21st Century Classic&quot;,
    &quot;category&quot;: &quot;BICYCLES&quot;,
    &quot;description&quot;: &quot;Modern update to a classic 70s bike design&quot;,
    &quot;price&quot;: NumberDecimal(&quot;523.00&quot;),
    &quot;rating&quot;: NumberDecimal(&quot;4.5&quot;),
  },
  {
    &quot;name&quot;: &quot;Diesel Flare Jeans&quot;,
    &quot;category&quot;: &quot;CLOTHES&quot;,
    &quot;description&quot;: &quot;Top end casual look&quot;,
    &quot;price&quot;: NumberDecimal(&quot;129.89&quot;),
    &quot;rating&quot;: NumberDecimal(&quot;4.3&quot;),
  },
  {
    &quot;name&quot;: &quot;Jazz Silk Scarf&quot;,
    &quot;category&quot;: &quot;CLOTHES&quot;,
    &quot;description&quot;: &quot;Style for the winder months&quot;,
    &quot;price&quot;: NumberDecimal(&quot;28.39&quot;),
    &quot;rating&quot;: NumberDecimal(&quot;3.7&quot;),
  },
  {
    &quot;name&quot;: &quot;Dell XPS 13 Laptop&quot;,
    &quot;category&quot;: &quot;ELECTRONICS&quot;,
    &quot;description&quot;: &quot;Developer edition&quot;,
    &quot;price&quot;: NumberDecimal(&quot;1399.89&quot;),
    &quot;rating&quot;: NumberDecimal(&quot;4.4&quot;),
  },
  {
    &quot;name&quot;: &quot;NY Baseball Cap&quot;,
    &quot;category&quot;: &quot;CLOTHES&quot;,
    &quot;description&quot;: &quot;Blue &amp; white&quot;,
    &quot;price&quot;: NumberDecimal(&quot;18.99&quot;),
    &quot;rating&quot;: NumberDecimal(&quot;4.0&quot;),
  },
  {
    &quot;name&quot;: &quot;Tots Flower Pots&quot;,
    &quot;category&quot;: &quot;GARDEN&quot;,
    &quot;description&quot;: &quot;Set of three&quot;,
    &quot;price&quot;: NumberDecimal(&quot;9.78&quot;),
    &quot;rating&quot;: NumberDecimal(&quot;4.1&quot;),
  },  
  {
    &quot;name&quot;: &quot;Picky Pencil Sharpener&quot;,
    &quot;category&quot;: &quot;Stationery&quot;,
    &quot;description&quot;: &quot;Ultra budget&quot;,
    &quot;price&quot;: NumberDecimal(&quot;0.67&quot;),
    &quot;rating&quot;: NumberDecimal(&quot;1.2&quot;),
  },  
]); 
</code></pre>
<h2 id="aggregation-pipeline-8"><a class="header" href="#aggregation-pipeline-8">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Group products by 2 facets: 1) by price ranges, 2) by rating ranges
  {&quot;$facet&quot;: {

    // Group by price ranges
    &quot;by_price&quot;: [
      // Group into 3 ranges: inexpensive small price range to expensive large price range
      {&quot;$bucketAuto&quot;: {
        &quot;groupBy&quot;: &quot;$price&quot;,
        &quot;buckets&quot;: 3,
        &quot;granularity&quot;: &quot;1-2-5&quot;,
        &quot;output&quot;: {
          &quot;count&quot;: {&quot;$sum&quot;: 1},
          &quot;products&quot;: {&quot;$push&quot;: &quot;$name&quot;},
        },
      }},
      
      // Tag range info as &quot;price_range&quot;
      {&quot;$set&quot;: {
        &quot;price_range&quot;: &quot;$_id&quot;,
      }},         
      
      // Omit unwanted fields
      {&quot;$unset&quot;: [
        &quot;_id&quot;,
      ]},         
    ],

    // Group by rating ranges
    &quot;by_rating&quot;: [
      // Group products evenly across 5 rating ranges from low to high
      {&quot;$bucketAuto&quot;: {
        &quot;groupBy&quot;: &quot;$rating&quot;,
        &quot;buckets&quot;: 5,
        &quot;output&quot;: {
          &quot;count&quot;: {&quot;$sum&quot;: 1},
          &quot;products&quot;: {&quot;$push&quot;: &quot;$name&quot;},
        },
      }},
      
      // Tag range info as &quot;rating_range&quot;
      {&quot;$set&quot;: {
        &quot;rating_range&quot;: &quot;$_id&quot;,
      }},         
      
      // Omit unwanted fields
      {&quot;$unset&quot;: [
        &quot;_id&quot;,
      ]},         
    ],
  }},  
];
</code></pre>
<h2 id="execution-8"><a class="header" href="#execution-8">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.products.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.products.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-8"><a class="header" href="#expected-results-8">Expected Results</a></h2>
<p>A single document should be returned, which contains 2 facets (keyed off <code>by_price</code> and <code>by_rating</code> respectively), where each facet shows its sub-ranges of values and the products belonging to each sub-range, as shown below:</p>
<pre><code class="language-javascript">[
  {
    by_price: [
      {
        count: 6,
        products: [
          'Picky Pencil Sharpener', 'One Day in the Life of Ivan Denisovich', 
          'The Day Of The Triffids', 'Tots Flower Pots', 'Russell Hobbs Chrome Kettle',
          'NY Baseball Cap'
        ],
        price_range: {
          min: NumberDecimal('0.500000000000000'), max: NumberDecimal('20.0000000000000')
        }
      },
      {
        count: 5,
        products: [
          'Karcher Hose Set', 'Oak Coffee Table', 'Jazz Silk Scarf',
          'Morphy Richardds Food Mixer', 'Diesel Flare Jeans'
        ],
        price_range: {
          min: NumberDecimal('20.0000000000000'), max: NumberDecimal('200.0000000000000')
        }
      },
      {
        count: 5,
        products: [
          'Asus Laptop', 'Raleigh Racer 21st Century Classic', 'Tiffany Gold Chain',
          'Lenovo Laptop', 'Dell XPS 13 Laptop'
        ],
        price_range: {
          min: NumberDecimal('200.0000000000000'), max: NumberDecimal('2000.0000000000000')
        }
      }
    ],
    by_rating: [
      {
        count: 4,
        products: [
          'Picky Pencil Sharpener', 'Jazz Silk Scarf', 'Morphy Richardds Food Mixer',
          'Oak Coffee Table'
        ],
        rating_range: { min: NumberDecimal('1.2'), max: NumberDecimal('3.9') }
      },
      {
        count: 3,
        products: [
          'Russell Hobbs Chrome Kettle', 'Tiffany Gold Chain', 'NY Baseball Cap'
        ],
        rating_range: { min: NumberDecimal('3.9'), max: NumberDecimal('4.1') }
      },
      {
        count: 3,
        products: [ 'Lenovo Laptop', 'Tots Flower Pots', 'Asus Laptop' ],
        rating_range: { min: NumberDecimal('4.1'), max: NumberDecimal('4.3') }
      },
      {
        count: 3,
        products: [
          'Karcher Hose Set', 'Diesel Flare Jeans', 'Dell XPS 13 Laptop'
        ],
        rating_range: { min: NumberDecimal('4.3'), max: NumberDecimal('4.5') }
      },
      {
        count: 3,
        products: [
          'Raleigh Racer 21st Century Classic', 'The Day Of The Triffids',
          'One Day in the Life of Ivan Denisovich'
        ],
        rating_range: { min: NumberDecimal('4.5'), max: NumberDecimal('4.9') }
      }
    ]
  }
]
</code></pre>
<h2 id="observations-8"><a class="header" href="#observations-8">Observations</a></h2>
<ul>
<li>
<p><strong>Multiple Pipelines.</strong> The <code>$facet</code> stage doesn't have to be employed for you to use the <code>$bucketAuto</code> stage. In most <em>faceted search</em> scenarios, you will want to understand a collection by multiple dimensions at once (<em>price</em> &amp; <em>rating</em> in this case). The <code>$facet</code> stage is convenient because it allows you to define various <code>$bucketAuto</code> dimensions in one go in a single pipeline. Otherwise, a client application must invoke an aggregation multiple times, each using a new <code>$bucketAuto</code> stage to process a different field. In fact, each section of a <code>$facet</code> stage is just a regular aggregation [sub-]pipeline, able to contain any type of stage (with a few specific <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/facet/#behavior">documented exceptions</a>) and may not even contain <code>$bucketAuto</code> or <code>$bucket</code> stages at all. </p>
</li>
<li>
<p><strong>Single Document Result.</strong> If the result of a <code>$facet</code> based aggregation is allowed to be multiple documents, this will cause a problem. The results will contain a mix of records originating from different facets but with no way of ascertaining the facet each result record belongs to. Consequently, when using <code>$facet</code>, a single document is always returned, containing top-level fields identifying each facet. Having only a single result record is not usually a problem. A typical requirement for faceted search is to return a small amount of grouped summary data about a collection rather than large amounts of raw data from the collection. Therefore the 16MB document size limit should not be an issue.</p>
</li>
<li>
<p><strong>Spread Of Ranges.</strong> In this example, each of the two employed bucketing facets uses a different granularity number scheme for spreading out the sub-ranges of values. You choose a numbering scheme based on what you know about the nature of the facet. For instance, most of the <em>ratings</em> values in the sample collection have scores bunched between late 3s and early 4s. If a numbering scheme is defined to reflect an even spread of ratings, most products will appear in the same sub-range bucket and some sub-ranges would contain no products (e.g. ratings 2 to 3 in this example). This wouldn't provide website customers with much selectivity on product ratings.</p>
</li>
<li>
<p><strong>Faster Facet Computation.</strong> The aggregation in this example has no choice but to perform a &quot;full-collection-scan&quot; to construct the faceted results. For large collections, the time the user has to wait on the website to see these results may be prohibitively long. However, there is an alternative mechanism you can employ to generate faceted results faster, using <a href="https://www.mongodb.com/docs/atlas/atlas-search/">Atlas Search</a>, as highlighted in a <a href="examples/trend-analysis/../full-text-search/facets-and-counts-text-search.html">later example in this book</a>. Therefore, if you can adopt Atlas Search, use its faceted search capability rather than MongoDB's general-purpose faceted search capability.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="largest-graph-network"><a class="header" href="#largest-graph-network">Largest Graph Network</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.2</p>
<h2 id="scenario-9"><a class="header" href="#scenario-9">Scenario</a></h2>
<p>Your organisation wants to know the best targets for a new marketing campaign based on a social network database similar to <em>Twitter</em>. You want to search the collection of social network users, each holding a user's name and the names of other people who follow them. You will execute an aggregation pipeline that walks each user record's <code>followed_by</code> array to determine which user has the largest <em>network reach</em>.</p>
<blockquote>
<p><em>This example uses a simple data model for brevity. However, this is unlikely to be an optimum data model for using <a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/graphLookup/"><code>$graphLookup</code></a> at scale for social network users with many followers or running in a sharded environment. For more guidance on such matters, see this reference application: <a href="https://github.com/mongodb-labs/socialite">Socialite</a>.</em></p>
</blockquote>
<h2 id="sample-data-population-9"><a class="header" href="#sample-data-population-9">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate a new <code>users</code> collection with 10 social network users documents, plus an index to help optimise the <em>graph traversal</em>:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-largest-graph-network&quot;);
db.dropDatabase();

// Create index on field which each graph traversal hop will connect to
db.users.createIndex({&quot;name&quot;: 1});

// Insert records into the users collection
db.users.insertMany([
  {&quot;name&quot;: &quot;Paul&quot;, &quot;followed_by&quot;: []},
  {&quot;name&quot;: &quot;Toni&quot;, &quot;followed_by&quot;: [&quot;Paul&quot;]},
  {&quot;name&quot;: &quot;Janet&quot;, &quot;followed_by&quot;: [&quot;Paul&quot;, &quot;Toni&quot;]},
  {&quot;name&quot;: &quot;David&quot;, &quot;followed_by&quot;: [&quot;Janet&quot;, &quot;Paul&quot;, &quot;Toni&quot;]},
  {&quot;name&quot;: &quot;Fiona&quot;, &quot;followed_by&quot;: [&quot;David&quot;, &quot;Paul&quot;]},
  {&quot;name&quot;: &quot;Bob&quot;, &quot;followed_by&quot;: [&quot;Janet&quot;]},
  {&quot;name&quot;: &quot;Carl&quot;, &quot;followed_by&quot;: [&quot;Fiona&quot;]},
  {&quot;name&quot;: &quot;Sarah&quot;, &quot;followed_by&quot;: [&quot;Carl&quot;, &quot;Paul&quot;]},
  {&quot;name&quot;: &quot;Carol&quot;, &quot;followed_by&quot;: [&quot;Helen&quot;, &quot;Sarah&quot;]},
  {&quot;name&quot;: &quot;Helen&quot;, &quot;followed_by&quot;: [&quot;Paul&quot;]},
]);
</code></pre>
<h2 id="aggregation-pipeline-9"><a class="header" href="#aggregation-pipeline-9">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // For each social network user, graph traverse their 'followed_by' list of people
  {&quot;$graphLookup&quot;: {
    &quot;from&quot;: &quot;users&quot;,
    &quot;startWith&quot;: &quot;$followed_by&quot;,
    &quot;connectFromField&quot;: &quot;followed_by&quot;,
    &quot;connectToField&quot;: &quot;name&quot;,
    &quot;depthField&quot;: &quot;depth&quot;,
    &quot;as&quot;: &quot;extended_network&quot;,
  }},

  // Add new accumulating fields
  {&quot;$set&quot;: {
    // Count the extended connection reach
    &quot;network_reach&quot;: {
      &quot;$size&quot;: &quot;$extended_network&quot;
    },

    // Gather the list of the extended connections' names
    &quot;extended_connections&quot;: {
      &quot;$map&quot;: {
        &quot;input&quot;: &quot;$extended_network&quot;,
        &quot;as&quot;: &quot;connection&quot;,
        &quot;in&quot;: &quot;$$connection.name&quot;, // Just get name field from each array element
      }
    },    
  }},
    
  // Omit unwanted fields
  {&quot;$unset&quot;: [
    &quot;_id&quot;,
    &quot;followed_by&quot;,
    &quot;extended_network&quot;,
  ]},   
  
  // Sort by person with greatest network reach first, in descending order
  {&quot;$sort&quot;: {
    &quot;network_reach&quot;: -1,
  }},   
];
</code></pre>
<h2 id="execution-9"><a class="header" href="#execution-9">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.users.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.users.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-9"><a class="header" href="#expected-results-9">Expected Results</a></h2>
<p>Ten documents should be returned, corresponding to the original ten source social network users, with each one including a count of the user's <em>network reach</em>, and the names of their <em>extended connections</em>, sorted by the user with the most extensive network reach first, as shown below:</p>
<pre><code class="language-javascript">[
  {
    name: 'Carol',
    network_reach: 8,
    extended_connections: [ 'David', 'Toni', 'Fiona', 'Sarah', 'Helen', 'Carl', 'Paul',  'Janet' ]
  },
  {
    name: 'Sarah',
    network_reach: 6,
    extended_connections: [ 'David', 'Toni', 'Fiona', 'Carl', 'Paul', 'Janet' ]
  },
  {
    name: 'Carl',
    network_reach: 5,
    extended_connections: [ 'David', 'Toni', 'Fiona', 'Paul', 'Janet' ]
  },
  {
    name: 'Fiona',
    network_reach: 4,
    extended_connections: [ 'David', 'Toni', 'Paul', 'Janet' ]
  },
  {
    name: 'David',
    network_reach: 3,
    extended_connections: [ 'Toni', 'Paul', 'Janet' ]
  },
  {
    name: 'Bob',
    network_reach: 3,
    extended_connections: [ 'Toni', 'Paul', 'Janet' ]
  },
  {
    name: 'Janet',
    network_reach: 2,
    extended_connections: [ 'Toni', 'Paul' ]
  },
  {
    name: 'Toni',
    network_reach: 1, 
    extended_connections: [ 'Paul']
  },
  { 
    name: 'Helen',
    network_reach: 1, 
    extended_connections: [ 'Paul' ] 
  },
  { name: 'Paul', 
    network_reach: 0, 
    extended_connections: [] 
  }
]
</code></pre>
<h2 id="observations-9"><a class="header" href="#observations-9">Observations</a></h2>
<ul>
<li>
<p><strong>Following Graphs.</strong> The <code>$graphLookup</code> stage helps you traverse relationships between records, looking for patterns that aren't necessarily evident from looking at each record in isolation. In this example, by looking at <em>Paul's</em> record in isolation, it is evident that <em>Paul</em> has no <em>friends</em> and thus has the lowest network reach. However, it is not obvious that <em>Carol</em> has the greatest network reach just by looking at the number of people <em>Carol</em> is directly followed by, which is two. <em>David</em>, for example, is followed by three people (one more than <em>Carol</em>). However, the executed aggregation pipeline can deduce that <em>Carol</em> has the most extensive network reach.</p>
</li>
<li>
<p><strong>Index Use.</strong> The <code>$graphLookup</code> stage can leverage the index on the field <code>name</code> for each of its <code>connectToField</code> hops.</p>
</li>
<li>
<p><strong>Extracting One Field From Each Array Element.</strong> The pipeline uses the <code>$map</code> array operator to only take one field from each <em>user</em> element matched by the <code>$graphLookup</code> stage. The <code>$map</code> logic loops through each matched <em>user</em>, adding the value of the user's <code>name</code> field to the <code>$map</code>'s array of results and ignoring the other field (<code>followed_by</code>). For more information about using the <code>$map</code> operator, see the <a href="examples/trend-analysis/../../guides/advanced-arrays.html">Advanced Use Of Expressions For Array Processing</a> chapter.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="incremental-analytics"><a class="header" href="#incremental-analytics">Incremental Analytics</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.2</p>
<h2 id="scenario-10"><a class="header" href="#scenario-10">Scenario</a></h2>
<p>You have a set of <em>shop orders</em> accumulated over many years, with the retail channel adding new order records continuously to the <em>orders</em> collection throughout each trading day. You want to frequently generate a summary report so management can understand the state of the business and react to changing business trends. Over the years, it takes increasingly longer to generate the report of all daily sums and averages because there is increasingly more days' worth of data to process. From now on, to address this problem, you will only generate each new day's summary analysis at the end of the day and store it in a different collection which accumulates the daily summary records over time.</p>
<blockquote>
<p><em>Unlike most examples in this book, the aggregation pipeline writes its output to a collection rather than streaming the results back to the calling application. This approach is sometimes referred to as <strong>On-Demand Materialized Views</strong>.</em></p>
</blockquote>
<h2 id="sample-data-population-10"><a class="header" href="#sample-data-population-10">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then add 9 documents to the <code>orders</code> collection representing 5 orders for 01-Feb-2021 and 4 orders for 02-Feb-2021:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-incremental-analytics&quot;);
db.dropDatabase();

// Create index for a daily_orders_summary collection
db.daily_orders_summary.createIndex({&quot;day&quot;: 1}, {&quot;unique&quot;: true});

// Create index for a orders collection
db.orders.createIndex({&quot;orderdate&quot;: 1});

// Insert records into the orders collection
// (5 orders for 1st Feb, 4 orders for 2nd Feb)
db.orders.insertMany([
  {
    &quot;orderdate&quot;: ISODate(&quot;2021-02-01T08:35:52Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;231.43&quot;),
  },
  {
    &quot;orderdate&quot;: ISODate(&quot;2021-02-01T09:32:07Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;99.99&quot;),
  },
  {
    &quot;orderdate&quot;: ISODate(&quot;2021-02-01T08:25:37Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;63.13&quot;),
  },
  {
    &quot;orderdate&quot;: ISODate(&quot;2021-02-01T19:13:32Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;2.01&quot;),
  },  
  {
    &quot;orderdate&quot;: ISODate(&quot;2021-02-01T22:56:53Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;187.99&quot;),
  },
  {
    &quot;orderdate&quot;: ISODate(&quot;2021-02-02T23:04:48Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;4.59&quot;),
  },
  {
    &quot;orderdate&quot;: ISODate(&quot;2021-02-02T08:55:46Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;48.50&quot;),
  },
  {
    &quot;orderdate&quot;: ISODate(&quot;2021-02-02T07:49:32Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;1024.89&quot;),
  },
  {
    &quot;orderdate&quot;: ISODate(&quot;2021-02-02T13:49:44Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;102.24&quot;),
  },
]);

</code></pre>
<h2 id="aggregation-pipeline-10"><a class="header" href="#aggregation-pipeline-10">Aggregation Pipeline</a></h2>
<p>Define a function to create a pipeline, but with the start and end date parameterised, ready to be used to perform the aggregation multiple times, for different days:</p>
<pre><code class="language-javascript">function getDayAggPipeline(startDay, endDay) {
  return [
    // Match orders for one day only
    {&quot;$match&quot;: {
      &quot;orderdate&quot;: {
        &quot;$gte&quot;: ISODate(startDay),
        &quot;$lt&quot;: ISODate(endDay),
      }
    }},
    
    // Group all orders together into one summary record for the day
    {&quot;$group&quot;: {
      &quot;_id&quot;: null,
      &quot;date_parts&quot;: {&quot;$first&quot;: {&quot;$dateToParts&quot;: {&quot;date&quot;: &quot;$orderdate&quot;}}},
      &quot;total_value&quot;: {&quot;$sum&quot;: &quot;$value&quot;},
      &quot;total_orders&quot;: {&quot;$sum&quot;: 1},
    }},
      
    // Get date parts from 1 order (need year+month+day, for UTC)
    {&quot;$set&quot;: {
      &quot;day&quot;: {
        &quot;$dateFromParts&quot;: {
          &quot;year&quot;: &quot;$date_parts.year&quot;, 
          &quot;month&quot;: &quot;$date_parts.month&quot;,
          &quot;day&quot;:&quot;$date_parts.day&quot;
       }
     },
    }},
        
    // Omit unwanted field
    {&quot;$unset&quot;: [
      &quot;_id&quot;,
      &quot;date_parts&quot;,
    ]},
    
    // Add day summary to summary collection (overwrite if already exists)
    {&quot;$merge&quot;: {
      &quot;into&quot;: &quot;daily_orders_summary&quot;,
      &quot;on&quot;: &quot;day&quot;,
      &quot;whenMatched&quot;: &quot;replace&quot;,
      &quot;whenNotMatched&quot;: &quot;insert&quot;
    }},   
  ];
}
</code></pre>
<h2 id="execution-10"><a class="header" href="#execution-10">Execution</a></h2>
<p>For 01-Feb-2021 orders only, build the pipeline and execute the aggregation: </p>
<pre><code class="language-javascript">// Get the pipeline for the 1st day
var pipeline = getDayAggPipeline(&quot;2021-02-01T00:00:00Z&quot;, &quot;2021-02-02T00:00:00Z&quot;);

// Run aggregation for 01-Feb-2021 orders &amp; put result in summary collection
db.orders.aggregate(pipeline);

// View the summary collection content (should be 1 record only)
db.daily_orders_summary.find();
</code></pre>
<p>From the results, you can see that only a single order summary has been generated, for 01-Feb-2021, containing the total value and number of orders for that day.</p>
<p>Now for the next day only (for 02-Feb-2021 orders), build the pipeline and execute the aggregation: </p>
<pre><code class="language-javascript">// Get the pipeline for the 2nd day
var pipeline = getDayAggPipeline(&quot;2021-02-02T00:00:00Z&quot;, &quot;2021-02-03T00:00:00Z&quot;);

// Run aggregation for 02-Feb-2021 orders &amp; put result in summary collection
db.orders.aggregate(pipeline);

// View the summary collection content (should be 2 record now)
db.daily_orders_summary.find();
</code></pre>
<p>From the results, you can see that order summaries exist for both days.</p>
<p>To simulate the organisation's occasional need to correct an old order retrospectively, go back and add a new &quot;high value&quot; order for the first day. Then re-run the aggregation for the first day only (01-Feb-2021) to show that you can safely and correctly recalculate the summary for just one day:</p>
<pre><code class="language-javascript">// Retrospectively add an order to an older day (01-Feb-2021)
db.orders.insertOne(
  {
    &quot;orderdate&quot;: ISODate(&quot;2021-02-01T09:32:07Z&quot;),
    &quot;value&quot;: NumberDecimal(&quot;11111.11&quot;),
  },
)

// Get the pipeline for the 1st day again
var pipeline = getDayAggPipeline(&quot;2021-02-01T00:00:00Z&quot;, &quot;2021-02-02T00:00:00Z&quot;);

// Re-run aggregation for 01-Feb-2021 overwriting 1st record in summary collections
db.orders.aggregate(pipeline);

// View the summary collection content (should still be 2 records but 1st changed)
db.daily_orders_summary.find();
</code></pre>
<p>From the results, you can see that two order summaries still exist, one for each of the two trading days, but the total value and order count for the first day has changed.</p>
<p>For completeness, also view the explain plan for the aggregation pipeline:</p>
<pre><code class="language-javascript">db.products.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-10"><a class="header" href="#expected-results-10">Expected Results</a></h2>
<p>The content of the <code>daily_orders_summary</code> collection after running the aggregation for just the 1st day should be similar to below:</p>
<pre><code class="language-javascript">[
  {
    _id: ObjectId('6062102e7eeb772e6ca96bc7'),
    total_value: NumberDecimal('584.55'),
    total_orders: 5,
    day: ISODate('2021-02-01T00:00:00.000Z')
  }
]
</code></pre>
<p>The content of the <code>daily_orders_summary</code> collection after running the aggregation for the 2nd day should be similar to below:</p>
<pre><code class="language-javascript">[
  {
    _id: ObjectId('6062102e7eeb772e6ca96bc7'),
    total_value: NumberDecimal('584.55'),
    total_orders: 5,
    day: ISODate('2021-02-01T00:00:00.000Z')
  },
  {
    _id: ObjectId('606210377eeb772e6ca96bcc'),
    total_value: NumberDecimal('1180.22'),
    total_orders: 4,
    day: ISODate('2021-02-02T00:00:00.000Z')
  }
]
</code></pre>
<p>After re-running the aggregation for the 1st day following the addition of the missed order, the content of the <code>daily_orders_summary</code> collection should be similar to below (notice the first record now shows a value of one greater than before for <code>total_orders</code>, and for <code>total_value</code> the value is now significantly higher):</p>
<pre><code class="language-javascript">[
  {
    _id: ObjectId('6062102e7eeb772e6ca96bc7'),
    total_value: NumberDecimal('11695.66'),
    total_orders: 6,
    day: ISODate('2021-02-01T00:00:00.000Z')
  },
  {
    _id: ObjectId('606210377eeb772e6ca96bcc'),
    total_value: NumberDecimal('1180.22'),
    total_orders: 4,
    day: ISODate('2021-02-02T00:00:00.000Z')
  }
]
</code></pre>
<h2 id="observations-10"><a class="header" href="#observations-10">Observations</a></h2>
<ul>
<li>
<p><strong>Merging Results.</strong> The pipeline uses a <code>$merge</code> stage to instruct the aggregation engine to write the output to a collection rather than returning a stream of results. In this example, with the options you provide to <code>$merge</code>, the aggregation inserts a new record in the destination collection if a matching one doesn't already exist. If a matching record already exists, it replaces the previous version.</p>
</li>
<li>
<p><strong>Incremental Updates.</strong> The example illustrates just two days of shop orders, albeit with only a few orders, to keep the example simple. At the end of each new trading day, you run the aggregation pipeline to generate the current day's summary only. Even after the source collection has increased in size over many years, the time it takes you to bring the summary collection up to date again stays constant. In a real-world scenario, the business might expose a graphical chart showing the changing daily orders trend over the last rolling year. This charting dashboard is not burdened by the cost of periodically regenerating values for all days in the year. There could be hundreds of thousands of orders received per day for real-world retailers, especially large ones. A day's summary may take many seconds to generate in that situation. Without an <em>incremental analytics</em> approach, if you need to generate a year's worth of daily summaries every time, it would take hours to refresh the business dashboard.</p>
</li>
<li>
<p><strong>Idempotency.</strong> If a retailer is aggregating tens of thousands of orders per day, then during end-of-day processing, it may choose to generate 24 hourly summary records rather than a single daily record. This provides the business with finer granularity to understand trends better. As with any software process, when generating hourly results into the summary collection, there is the risk of not fully completing if a system failure occurs. If an in-flight aggregation terminates abnormally, it may not have written all 24 summary collection records. The failure leaves the summary collection in an indeterminate and incomplete state for one of its days. However, this isn't a problem because of the way the aggregation pipeline uses the <code>$merge</code> stage. When an aggregation fails to complete, it can just be re-run. When re-run, it will regenerate all the results for the day, replacing existing summary records and filling in the missing ones. The aggregation pipeline is idempotent, and you can run it repeatedly without damaging the summary collection. The overall solution is self-healing and naturally tolerant of inadvertently aborted aggregation jobs.</p>
</li>
<li>
<p><strong>Retrospective Changes.</strong> Sometimes, an organisation may need to go back and correct records from the past, as illustrated in this example. For instance, a bank may need to fix a past payment record due to a settlement issue that only comes to light weeks later. With the approach used in this example, it is straightforward to re-execute the aggregation pipeline for a prior date, using the updated historical data. This will correctly update the specific day's summary data only, to reflect the business's current state.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="securing-data-examples"><a class="header" href="#securing-data-examples">Securing Data Examples</a></h1>
<p>This section provides examples of how aggregation pipelines can assist with the way data is accessed and distributed securely.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="redacted-view"><a class="header" href="#redacted-view">Redacted View</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.2</p>
<h2 id="scenario-11"><a class="header" href="#scenario-11">Scenario</a></h2>
<p>You have a user management system containing data about various people in a database, and you need to ensure a particular client application cannot view the sensitive parts of the data relating to each person. Consequently, you will provide a read-only view of peoples' data. You will use the view (named <em>adults</em>) to redact the personal data and expose this view to the client application as the only way it can access personal information. The view will apply the following two rules to restrict what data can be accessed:</p>
<ol>
<li>Only show people aged 18 and over (by checking each person's <code>dateofbirth</code> field)</li>
<li>Exclude each person's <code>social_security_num</code> field from results</li>
</ol>
<blockquote>
<p><em>In a real-world situation, you would also use MongoDB's Role-Based Access Control (RBAC) to limit the client application to only be able to access the view and not the original collection.</em></p>
</blockquote>
<h2 id="sample-data-population-11"><a class="header" href="#sample-data-population-11">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists), create an index and populate the new <code>persons</code> collection with 5 records:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-redacted-view&quot;);
db.dropDatabase();

// Create index for a persons collection
db.persons.createIndex({&quot;dateofbirth&quot;: -1});

// Create index for non-$expr part of filter in MongoDB version &lt; 5.0
db.persons.createIndex({&quot;gender&quot;: 1});

// Create index for combination of $expr &amp; non-$expr filter in MongoDB version &gt;= 5.0
db.persons.createIndex({&quot;gender&quot;: 1, &quot;dateofbirth&quot;: -1});

// Insert records into the persons collection
db.persons.insertMany([
  {
    &quot;person_id&quot;: &quot;6392529400&quot;,
    &quot;firstname&quot;: &quot;Elise&quot;,
    &quot;lastname&quot;: &quot;Smith&quot;,
    &quot;dateofbirth&quot;: ISODate(&quot;1972-01-13T09:32:07Z&quot;),
    &quot;gender&quot;: &quot;FEMALE&quot;,
    &quot;email&quot;: &quot;elise_smith@myemail.com&quot;,
    &quot;social_security_num&quot;: &quot;507-28-9805&quot;,
    &quot;address&quot;: { 
        &quot;number&quot;: 5625,
        &quot;street&quot;: &quot;Tipa Circle&quot;,
        &quot;city&quot;: &quot;Wojzinmoj&quot;,
    },
  },
  {
    &quot;person_id&quot;: &quot;1723338115&quot;,
    &quot;firstname&quot;: &quot;Olive&quot;,
    &quot;lastname&quot;: &quot;Ranieri&quot;,
    &quot;dateofbirth&quot;: ISODate(&quot;1985-05-12T23:14:30Z&quot;),    
    &quot;gender&quot;: &quot;FEMALE&quot;,
    &quot;email&quot;: &quot;oranieri@warmmail.com&quot;,
    &quot;social_security_num&quot;: &quot;618-71-2912&quot;,
    &quot;address&quot;: {
        &quot;number&quot;: 9303,
        &quot;street&quot;: &quot;Mele Circle&quot;,
        &quot;city&quot;: &quot;Tobihbo&quot;,
    },
  },
  {
    &quot;person_id&quot;: &quot;8732762874&quot;,
    &quot;firstname&quot;: &quot;Toni&quot;,
    &quot;lastname&quot;: &quot;Jones&quot;,
    &quot;dateofbirth&quot;: ISODate(&quot;2014-11-23T16:53:56Z&quot;),    
    &quot;gender&quot;: &quot;FEMALE&quot;,
    &quot;email&quot;: &quot;tj@wheresmyemail.com&quot;,
    &quot;social_security_num&quot;: &quot;001-10-3488&quot;,
    &quot;address&quot;: {
        &quot;number&quot;: 1,
        &quot;street&quot;: &quot;High Street&quot;,
        &quot;city&quot;: &quot;Upper Abbeywoodington&quot;,
    },
  },
  {
    &quot;person_id&quot;: &quot;7363629563&quot;,
    &quot;firstname&quot;: &quot;Bert&quot;,
    &quot;lastname&quot;: &quot;Gooding&quot;,
    &quot;dateofbirth&quot;: ISODate(&quot;1941-04-07T22:11:52Z&quot;),    
    &quot;gender&quot;: &quot;MALE&quot;,
    &quot;email&quot;: &quot;bgooding@tepidmail.com&quot;,
    &quot;social_security_num&quot;: &quot;230-43-7633&quot;,
    &quot;address&quot;: {
        &quot;number&quot;: 13,
        &quot;street&quot;: &quot;Upper Bold Road&quot;,
        &quot;city&quot;: &quot;Redringtonville&quot;,
    },
  },
  {
    &quot;person_id&quot;: &quot;1029648329&quot;,
    &quot;firstname&quot;: &quot;Sophie&quot;,
    &quot;lastname&quot;: &quot;Celements&quot;,
    &quot;dateofbirth&quot;: ISODate(&quot;2013-07-06T17:35:45Z&quot;),    
    &quot;gender&quot;: &quot;FEMALE&quot;,
    &quot;email&quot;: &quot;sophe@celements.net&quot;,
    &quot;social_security_num&quot;: &quot;377-30-5364&quot;,
    &quot;address&quot;: {
        &quot;number&quot;: 5,
        &quot;street&quot;: &quot;Innings Close&quot;,
        &quot;city&quot;: &quot;Basilbridge&quot;,
    },
  },
]);
</code></pre>
<h2 id="aggregation-pipeline-11"><a class="header" href="#aggregation-pipeline-11">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Filter out any persons aged under 18 ($expr required to reference '$$NOW')
  {&quot;$match&quot;:
    {&quot;$expr&quot;:{
      &quot;$lt&quot;: [&quot;$dateofbirth&quot;, {&quot;$subtract&quot;: [&quot;$$NOW&quot;, 18*365.25*24*60*60*1000]}]
    }},
  },

  // Exclude fields to be filtered out by the view
  {&quot;$unset&quot;: [
    &quot;_id&quot;,
    &quot;social_security_num&quot;,
  ]},    
];
</code></pre>
<h2 id="execution-11"><a class="header" href="#execution-11">Execution</a></h2>
<p>Firstly, to test the defined aggregation pipeline (before using it to create a view), execute the aggregation for the pipeline and also observe its explain plan:</p>
<pre><code class="language-javascript">db.persons.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.persons.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<p>Now create the new <em>adults</em> view, which will automatically apply the aggregation pipeline whenever anyone queries the view: </p>
<pre><code class="language-javascript">db.createView(&quot;adults&quot;, &quot;persons&quot;, pipeline);
</code></pre>
<p>Execute a regular MQL query against the view, without any filter criteria, and also observe its explain plan:</p>
<pre><code class="language-javascript">db.adults.find();
</code></pre>
<pre><code class="language-javascript">db.adults.explain(&quot;executionStats&quot;).find();
</code></pre>
<p>Execute a MQL query against the view, but this time with a filter to return only adults who are female, and again observe its explain plan to see how the <code>gender</code> filter affects the plan:</p>
<pre><code class="language-javascript">db.adults.find({&quot;gender&quot;: &quot;FEMALE&quot;});
</code></pre>
<pre><code class="language-javascript">db.adults.explain(&quot;executionStats&quot;).find({&quot;gender&quot;: &quot;FEMALE&quot;});
</code></pre>
<h2 id="expected-results-11"><a class="header" href="#expected-results-11">Expected Results</a></h2>
<p>The result for both the <code>aggregate()</code> command and the <code>find()</code> executed on the <em>view</em> should be the same, with three documents returned, representing the three persons who are over 18 but not showing their social security numbers, as shown below:</p>
<pre><code class="language-javascript">[
  {
    person_id: '6392529400',
    firstname: 'Elise',
    lastname: 'Smith',
    dateofbirth: ISODate('1972-01-13T09:32:07.000Z'),
    gender: 'FEMALE',
    email: 'elise_smith@myemail.com',
    address: { number: 5625, street: 'Tipa Circle', city: 'Wojzinmoj' }
  },
  {
    person_id: '1723338115',
    firstname: 'Olive',
    lastname: 'Ranieri',
    dateofbirth: ISODate('1985-05-12T23:14:30.000Z'),
    gender: 'FEMALE',
    email: 'oranieri@warmmail.com',
    address: { number: 9303, street: 'Mele Circle', city: 'Tobihbo' }
  },
  {
    person_id: '7363629563',
    firstname: 'Bert',
    lastname: 'Gooding',
    dateofbirth: ISODate('1941-04-07T22:11:52.000Z'),
    gender: 'MALE',
    email: 'bgooding@tepidmail.com',
    address: { number: 13, street: 'Upper Bold Road', city: 'Redringtonville' }
  }
]
</code></pre>
<p>The result of running the <code>find()</code> against the <em>view</em> with the filter <code>&quot;gender&quot;: &quot;FEMALE&quot;</code> should be two females records only because the male record has been excluded, as shown below:</p>
<pre><code class="language-javascript">[
  {
    person_id: '6392529400',
    firstname: 'Elise',
    lastname: 'Smith',
    dateofbirth: ISODate('1972-01-13T09:32:07.000Z'),
    gender: 'FEMALE',
    email: 'elise_smith@myemail.com',
    address: { number: 5625, street: 'Tipa Circle', city: 'Wojzinmoj' }
  },
  {
    person_id: '1723338115',
    firstname: 'Olive',
    lastname: 'Ranieri',
    dateofbirth: ISODate('1985-05-12T23:14:30.000Z'),
    gender: 'FEMALE',
    email: 'oranieri@warmmail.com',
    address: { number: 9303, street: 'Mele Circle', city: 'Tobihbo' }
  }
]
</code></pre>
<h2 id="observations-11"><a class="header" href="#observations-11">Observations</a></h2>
<ul>
<li>
<p><strong><code>$expr</code> &amp; Indexes.</strong> The <a href="https://docs.mongodb.com/manual/reference/aggregation-variables/"><code>NOW</code> system variable</a> used here returns the current system date-time. However, you can only access this system variable via an <a href="https://docs.mongodb.com/manual/meta/aggregation-quick-reference/#expressions">aggregation expression</a> and not directly via the regular MongoDB query syntax used by MQL and <code>$match</code>. You must wrap an expression using <code>$$NOW</code> inside an <code>$expr</code> operator. As described in the section <em><a href="examples/securing-data/../../guides/expressions.html#restrictions-when-using-expressions-with-match">Restrictions When Using Expressions</a></em> in an earlier chapter, if you use an <a href="https://docs.mongodb.com/manual/reference/operator/query/expr/"><code>$expr</code></a> query operator to perform a range comparison, you can't make use of an index in versions of MongoDB earlier then 5.0. Therefore, in this example, unless you use MongoDB 5.0 or greater, the aggregation will not take advantage of an index on <code>dateofbirth</code>. For a view, because you specify the pipeline earlier than it is ever run, you cannot obtain the current date-time at runtime by other means.</p>
</li>
<li>
<p><strong>View Finds &amp; Indexes.</strong> Even for versions of MongoDB before 5.0, the explain plan for the <em>gender query</em> run against the view shows an index has been used (the index defined for the <code>gender</code> field). At runtime, a view is essentially just an aggregation pipeline you define &quot;ahead of time&quot;. When <code>db.adults.find({&quot;gender&quot;: &quot;FEMALE&quot;})</code> is executed, the database engine dynamically appends a new <code>$match</code> stage to the end of the pipeline for the gender match. It then optimises the pipeline by moving the content of the new <code>$match</code> stage to the pipeline's start, where possible. Finally, it adds the filter extracted from the extended <code>$match</code> stage to the aggregation's initial query, and hence it can leverage an index containing the <code>gender</code> field. The following two excerpts, from an explain plan from a MongoDB version before 5.0, illustrate how the filter on <code>gender</code> and the filter on <code>dateofbirth</code> combine at runtime and how the index for <code>gender</code> is used to avoid a full collection scan:</p>
<pre><code class="language-javascript">'$cursor': {
  queryPlanner: {
    plannerVersion: 1,
    namespace: 'book-redacted-view.persons',
    indexFilterSet: false,
    parsedQuery: {
      '$and': [
        { gender: { '$eq': 'FEMALE' } },
        {
          '$expr': {
            '$lt': [
              '$dateofbirth',
              {
                '$subtract': [ '$$NOW', { '$const': 568036800000 } ]
                ...
</code></pre>
<pre><code class="language-javascript">inputStage: {
  stage: 'IXSCAN',
  keyPattern: { gender: 1 },
  indexName: 'gender_1',
  direction: 'forward',
  indexBounds: { gender: [ '[&quot;FEMALE&quot;, &quot;FEMALE&quot;]' ] }
}
</code></pre>
<p>In MongoDB 5.0 and greater, the explain plan will show the aggregation runtime executing the pipeline more optimally by entirely using the compound index based on both the fields <code>gender</code> and <code>dateofbirth</code>.</p>
<p>Note that just because the aggregation runtime moves the content of the <code>$match</code> stage from the base of the pipeline to the top of the pipeline, it doesn't imply this optimisation can happen in all pipelines. For example, if the middle part of the pipeline includes a <code>$group</code> stage, then the runtime can't move the <code>$match</code> stage ahead of the <code>$group</code> stage because this would change the functional behaviour and outcome of the pipeline. See the <a href="https://docs.mongodb.com/manual/core/aggregation-pipeline-optimization/">Aggregation Pipeline Optimization</a> documentation for the runtime optimisations the MongoDB database engine can apply.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mask-sensitive-fields"><a class="header" href="#mask-sensitive-fields">Mask Sensitive Fields</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.4    <em>(due to use of <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/rand/"><code>$rand</code></a> operator)</em></p>
<h2 id="scenario-12"><a class="header" href="#scenario-12">Scenario</a></h2>
<p>You want to perform irreversible masking on the sensitive fields of a collection of <em>credit card payments</em>, ready to provide the output data set to a 3rd party for analysis, without exposing sensitive information to the 3rd party. The specific changes that you need to make to the payments' fields are:</p>
<ul>
<li>Partially obfuscate the card holder's name</li>
<li>Obfuscate the first 12 digits of the card's number, retaining only the final 4 digits</li>
<li>Adjust the card's expiry date-time by adding or subtracting a random amount up to a maximum of 30 days (~1 month)</li>
<li>Replace the card's 3 digit security code with a random set of 3 digits</li>
<li>Adjust the transaction's amount by adding or subtracting a random amount up to a maximum of 10% of the original amount</li>
<li>Change the <code>reported</code> field's boolean value to the opposite value for roughly 20% of the records</li>
<li>If the embedded <code>customer_info</code> sub-document's <code>category</code> field is set to <em>RESTRICTED</em>, exclude the whole <code>customer_info</code> sub-document</li>
</ul>
<h2 id="sample-data-population-12"><a class="header" href="#sample-data-population-12">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate a new <code>payments</code> collection with 2 credit card payment documents, containing sensitive data:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-mask-sensitive-fields&quot;);
db.dropDatabase();

// Insert records into the payments collection
db.payments.insertMany([
  {
    &quot;card_name&quot;: &quot;Mrs. Jane A. Doe&quot;,
    &quot;card_num&quot;: &quot;1234567890123456&quot;,
    &quot;card_expiry&quot;: ISODate(&quot;2023-08-31T23:59:59Z&quot;),
    &quot;card_sec_code&quot;: &quot;123&quot;,
    &quot;card_type&quot;: &quot;CREDIT&quot;,        
    &quot;transaction_id&quot;: &quot;eb1bd77836e8713656d9bf2debba8900&quot;,
    &quot;transaction_date&quot;: ISODate(&quot;2021-01-13T09:32:07Z&quot;),
    &quot;transaction_amount&quot;: NumberDecimal(&quot;501.98&quot;),
    &quot;reported&quot;: false,
    &quot;customer_info&quot;: {
      &quot;category&quot;: &quot;RESTRICTED&quot;,
      &quot;rating&quot;: 89,
      &quot;risk&quot;: 3,
    },
  },
  {
    &quot;card_name&quot;: &quot;Jim Smith&quot;,
    &quot;card_num&quot;: &quot;9876543210987654&quot;,
    &quot;card_expiry&quot;: ISODate(&quot;2022-12-31T23:59:59Z&quot;),
    &quot;card_sec_code&quot;: &quot;987&quot;,
    &quot;card_type&quot;: &quot;DEBIT&quot;,        
    &quot;transaction_id&quot;: &quot;634c416a6fbcf060bb0ba90c4ad94f60&quot;,
    &quot;transaction_date&quot;: ISODate(&quot;2020-11-24T19:25:57Z&quot;),
    &quot;transaction_amount&quot;: NumberDecimal(&quot;64.01&quot;),
    &quot;reported&quot;: true,
    &quot;customer_info&quot;: {
      &quot;category&quot;: &quot;NORMAL&quot;,
      &quot;rating&quot;: 78,
      &quot;risk&quot;: 55,
    },
  },
]);
</code></pre>
<h2 id="aggregation-pipeline-12"><a class="header" href="#aggregation-pipeline-12">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Replace a subset of fields with new values
  {&quot;$set&quot;: {
    // Extract the last word from the name , eg: 'Doe' from 'Mrs. Jane A. Doe'
    &quot;card_name&quot;: {&quot;$regexFind&quot;: {&quot;input&quot;: &quot;$card_name&quot;, &quot;regex&quot;: /(\S+)$/}},
          
    // Mask card num 1st part retaining last 4 chars, eg: '1234567890123456' -&gt; 'XXXXXXXXXXXX3456'
    &quot;card_num&quot;: {&quot;$concat&quot;: [
                  &quot;XXXXXXXXXXXX&quot;,
                  {&quot;$substrCP&quot;: [&quot;$card_num&quot;, 12, 4]},
                ]},                     

    // Add/subtract a random time amount of a maximum of 30 days (~1 month) each-way
    &quot;card_expiry&quot;: {&quot;$add&quot;: [
                     &quot;$card_expiry&quot;,
                     {&quot;$floor&quot;: {&quot;$multiply&quot;: [{&quot;$subtract&quot;: [{&quot;$rand&quot;: {}}, 0.5]}, 2*30*24*60*60*1000]}},
                   ]},                     

    // Replace each digit with random digit, eg: '133' -&gt; '472'
    &quot;card_sec_code&quot;: {&quot;$concat&quot;: [
                       {&quot;$toString&quot;: {&quot;$floor&quot;: {&quot;$multiply&quot;: [{&quot;$rand&quot;: {}}, 10]}}},
                       {&quot;$toString&quot;: {&quot;$floor&quot;: {&quot;$multiply&quot;: [{&quot;$rand&quot;: {}}, 10]}}},
                       {&quot;$toString&quot;: {&quot;$floor&quot;: {&quot;$multiply&quot;: [{&quot;$rand&quot;: {}}, 10]}}},
                     ]},
                     
    // Add/subtract a random percent of the amount's value up to 10% maximum each-way
    &quot;transaction_amount&quot;: {&quot;$add&quot;: [
                            &quot;$transaction_amount&quot;,
                            {&quot;$multiply&quot;: [{&quot;$subtract&quot;: [{&quot;$rand&quot;: {}}, 0.5]}, 0.2, &quot;$transaction_amount&quot;]},
                          ]},
                          
    // Retain field's bool value 80% of time on average, setting to the opposite value 20% of time
    &quot;reported&quot;: {&quot;$cond&quot;: {
                   &quot;if&quot;:   {&quot;$lte&quot;: [{&quot;$rand&quot;: {}}, 0.8]},
                   &quot;then&quot;: &quot;$reported&quot;,
                   &quot;else&quot;: {&quot;$not&quot;: [&quot;$reported&quot;]},
                }},      

    // Exclude sub-doc if the sub-doc's category field's value is 'RESTRICTED'
    &quot;customer_info&quot;: {&quot;$cond&quot;: {
                        &quot;if&quot;:   {&quot;$eq&quot;: [&quot;$customer_info.category&quot;, &quot;RESTRICTED&quot;]}, 
                        &quot;then&quot;: &quot;$$REMOVE&quot;,     
                        &quot;else&quot;: &quot;$customer_info&quot;,
                     }},                                         
                
    // Mark _id field to excluded from results
    &quot;_id&quot;: &quot;$$REMOVE&quot;,                
  }},
  
  // Take regex matched last word from the card name and prefix it with hardcoded value
  {&quot;$set&quot;: {
    &quot;card_name&quot;: {&quot;$concat&quot;: [&quot;Mx. Xxx &quot;, {&quot;$ifNull&quot;: [&quot;$card_name.match&quot;, &quot;Anonymous&quot;]}]},                       
  }},
];
</code></pre>
<h2 id="execution-12"><a class="header" href="#execution-12">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.payments.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.payments.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-12"><a class="header" href="#expected-results-12">Expected Results</a></h2>
<p>Two documents should be returned, corresponding to the original two source documents, but this time with many of their fields redacted and obfuscated, plus the <code>customer_info</code> embedded document omitted for one record due to it having been marked as <code>RESTRICTED</code>, as shown below:</p>
<pre><code class="language-javascript">[
  {
    card_name: 'Mx. Xxx Doe',
    card_num: 'XXXXXXXXXXXX3456',
    card_expiry: ISODate('2023-08-31T23:29:46.460Z'),
    card_sec_code: '295',
    card_type: 'CREDIT',
    transaction_id: 'eb1bd77836e8713656d9bf2debba8900',
    transaction_date: ISODate('2021-01-13T09:32:07.000Z'),
    transaction_amount: NumberDecimal('492.4016988351474881660000000000000'),
    reported: false
  },
  {
    card_name: 'Mx. Xxx Smith',
    card_num: 'XXXXXXXXXXXX7654',
    card_expiry: ISODate('2023-01-01T00:34:49.330Z'),
    card_sec_code: '437',
    card_type: 'DEBIT',
    transaction_id: '634c416a6fbcf060bb0ba90c4ad94f60',
    transaction_date: ISODate('2020-11-24T19:25:57.000Z'),
    transaction_amount: NumberDecimal('58.36081337486762223600000000000000'),
    reported: false,
    customer_info: { category: 'NORMAL', rating: 78, risk: 55 }
  }
]
</code></pre>
<h2 id="observations-12"><a class="header" href="#observations-12">Observations</a></h2>
<ul>
<li>
<p><strong>Targeted Redaction.</strong> The pipeline uses a <code>$cond</code> operator to return the <code>$$REMOVE</code> marker variable if the <code>category</code> field is equal to <code>RESTRICTED</code>. This informs the aggregation engine to exclude the whole <code>customer_info</code> sub-document from the stage's output for the record. Alternatively, the pipeline could have used a <code>$redact</code> stage to achieve the same. However, <code>$redact</code> typically has to perform more processing work due to needing to check every field in the document. Hence, if a pipeline is only to redact out one specific sub-document, use the approach outlined in this example.</p>
</li>
<li>
<p><strong>Regular Expression.</strong> For masking the <code>card_name</code> field, a regular expression operator is used to extract the last word of the field's original value. <code>$regexFind</code> returns metadata into the stage's output records, indicating if the match succeeded and what the matched value is. Therefore, an additional <code>$set</code> stage is required later in the pipeline to extract the actual matched word from this metadata and prefix it with some hard-coded text. MongoDB version 5.0 introduced a new <a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/getField/"><code>$getField</code></a> operator, which you can instead use to directly extract the &quot;regex&quot; result field (<code>match</code>). Consequently, if you are using MongoDB 5.0 or greater, you can eliminate the second <code>$set</code> stage from the end of your pipeline and then replace the line of code which sets the masked value of the <code>card_name</code> field to the following: </p>
<pre><code class="language-javascript">// Prefix with a hard-coded value followed by the regex extracted last word of the card name
&quot;card_name&quot;: {&quot;$concat&quot;: [&quot;Mx. Xxx &quot;, {&quot;$ifNull&quot;: [{&quot;$getField&quot;: {&quot;field&quot;: &quot;match&quot;, &quot;input&quot;: {&quot;$regexFind&quot;: {&quot;input&quot;: &quot;$card_name&quot;, &quot;regex&quot;: /(\S+)$/}}}}, &quot;Anonymous&quot;]}]},
</code></pre>
</li>
<li>
<p><strong>Meaningful Insight.</strong> Even though the pipeline is irreversibly obfuscating fields, it doesn't mean that the masked data is useless for performing analytics to gain insight. The pipeline masks some fields by fluctuating the original values by a small but limited random percentage (e.g. <code>card_expiry</code>, <code>transaction_amount</code>), rather than replacing them with completely random values (e.g. <code>card_sec_code</code>). In such cases, if the input data set is sufficiently large, then minor variances will be equalled out. For the fields that are only varied slightly, users can derive similar trends and patterns from analysing the masked data as they would the original data.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="role-programmatic-restricted-view"><a class="header" href="#role-programmatic-restricted-view">Role Programmatic Restricted View</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 7.0    <em>(due to use of <code>USER_ROLES</code> system variable)</em></p>
<h2 id="scenario-13"><a class="header" href="#scenario-13">Scenario</a></h2>
<p>At a medical establishment, the central IT system holds patient data that you need to surface to different applications (and their users) according to the application's role: Receptionist, Nurse, and Doctor. Consequently, you will provide a read-only view of patient data, but the view will filter out specific sensitive fields depending on the application's role. For example, the Receptionist's application should not be able to access the patient's current weight and medication. However, the Doctor's application needs this information to enable them to perform their job.</p>
<blockquote>
<p><em>Essentially, this example illustrates how you can apply both &quot;record-level&quot; (a.k.a. &quot;row-level&quot;) and &quot;field-level&quot; (a.k.a. &quot;column-level&quot;) access control in MongoDB. Its pipeline applies programmatic role-based access control rules rather than declarative ones to enforce what data users can access within a view. In a real-world situation, you would additionally use a declarative role to limit the client application with access only to the view and not the underlying collection.</em></p>
</blockquote>
<h2 id="sample-data-population-13"><a class="header" href="#sample-data-population-13">Sample Data Population</a></h2>
<p>If you are using a self-installed MongoDB deployment, run the commands below to create the necessary roles and users to help with implementing programmatic access control:</p>
<blockquote>
<p><em>If you are using a <a href="https://www.mongodb.com/atlas/database">MongoDB Atlas Database Cluster</a>, then instead, use the <a href="https://cloud.mongodb.com/">Atlas console</a> to define the roles and users for your Atlas project and its database cluster.</em></p>
</blockquote>
<pre><code class="language-javascript">var dbName = &quot;book-role-programmatic-restricted-view&quot;;
db = db.getSiblingDB(dbName);
db.dropDatabase();
db.dropAllRoles();
db.dropAllUsers();

// Create 3 roles to use for programmatic access control
db.createRole({&quot;role&quot;: &quot;Receptionist&quot;, &quot;roles&quot;: [], &quot;privileges&quot;: []});
db.createRole({&quot;role&quot;: &quot;Nurse&quot;, &quot;roles&quot;: [], &quot;privileges&quot;: []});
db.createRole({&quot;role&quot;: &quot;Doctor&quot;, &quot;roles&quot;: [], &quot;privileges&quot;: []});

// Create 3 users where each user will have a different role
db.createUser({
  &quot;user&quot;: &quot;front-desk&quot;,
  &quot;pwd&quot;: &quot;abc123&quot;,
  &quot;roles&quot;: [
    {&quot;role&quot;: &quot;Receptionist&quot;, &quot;db&quot;: dbName},
  ]
});
db.createUser({
  &quot;user&quot;: &quot;nurse-station&quot;,
  &quot;pwd&quot;: &quot;xyz789&quot;,
  &quot;roles&quot;: [
    {&quot;role&quot;: &quot;Nurse&quot;, &quot;db&quot;: dbName},
  ]
});
db.createUser({
  &quot;user&quot;: &quot;exam-room&quot;,
  &quot;pwd&quot;: &quot;mno456&quot;,
  &quot;roles&quot;: [
    {&quot;role&quot;: &quot;Doctor&quot;, &quot;db&quot;: dbName},
  ]
});
</code></pre>
<p>Populate the new <code>patients</code> collection with four records:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-role-programmatic-restricted-view&quot;);

// Insert 4 records into the patients collection
db.patients.insertMany([
  {
    &quot;id&quot;: &quot;D40230&quot;,
    &quot;first_name&quot;: &quot;Chelsea&quot;,
    &quot;last_Name&quot;: &quot;Chow&quot;,
    &quot;birth_date&quot;: ISODate(&quot;1984-11-07T10:12:00Z&quot;),
    &quot;weight&quot;: 145,
    &quot;medication&quot;: [&quot;Insulin&quot;, &quot;Methotrexate&quot;],
  },
  {
    &quot;id&quot;: &quot;R83165&quot;,
    &quot;first_name&quot;: &quot;Pharrell&quot;,
    &quot;last_Name&quot;: &quot;Phillips&quot;,
    &quot;birth_date&quot;: ISODate(&quot;1993-05-30T19:44:00Z&quot;),
    &quot;weight&quot;: 137,
    &quot;medication&quot;: [&quot;Fluoxetine&quot;],
  },  
  {
    &quot;id&quot;: &quot;X24046&quot;,
    &quot;first_name&quot;: &quot;Billy&quot;,
    &quot;last_Name&quot;: &quot;Boaty&quot;,
    &quot;birth_date&quot;: ISODate(&quot;1976-02-07T23:58:00Z&quot;),
    &quot;weight&quot;: 223,
    &quot;medication&quot;: [],
  },
  {
    &quot;id&quot;: &quot;P53212&quot;,
    &quot;first_name&quot;: &quot;Yazz&quot;,
    &quot;last_Name&quot;: &quot;Yodeler&quot;,
    &quot;birth_date&quot;: ISODate(&quot;1999-12-25T12:51:00Z&quot;),
    &quot;weight&quot;: 156,
    &quot;medication&quot;: [&quot;Tylenol&quot;, &quot;Naproxen&quot;],
  }, 
]);
</code></pre>
<h2 id="aggregation-pipeline-13"><a class="header" href="#aggregation-pipeline-13">Aggregation Pipeline</a></h2>
<p>Define an aggregation pipeline ready to be used as the basis of a new view:</p>
<pre><code class="language-javascript">var pipeline = [
  {&quot;$set&quot;: {
    // Exclude weight if user does not have right role
    &quot;weight&quot;: {
      &quot;$cond&quot;: {
        &quot;if&quot;: {
          &quot;$eq&quot;: [{&quot;$setIntersection&quot;: [&quot;$$USER_ROLES.role&quot;, [&quot;Doctor&quot;, &quot;Nurse&quot;]]}, []]
        },
        &quot;then&quot;: &quot;$$REMOVE&quot;,
        &quot;else&quot;: &quot;$weight&quot;
      }
    },
      
    // Exclude weight if user does not have right role
    &quot;medication&quot;: {
      &quot;$cond&quot;: {
        &quot;if&quot;: {
          &quot;$eq&quot;: [{&quot;$setIntersection&quot;: [&quot;$$USER_ROLES.role&quot;, [&quot;Doctor&quot;]]}, []]
        },
        &quot;then&quot;: &quot;$$REMOVE&quot;,
        &quot;else&quot;: &quot;$medication&quot;
      }
    },

    // Always exclude _id
    &quot;_id&quot;: &quot;$$REMOVE&quot;,
  }},
]
</code></pre>
<p>Create a new view called <code>patients_view</code>, which will automatically apply the aggregation pipeline whenever anyone queries the view: </p>
<pre><code class="language-javascript">db.createView(&quot;patients_view&quot;, &quot;patients&quot;, pipeline);
</code></pre>
<h2 id="execution-13"><a class="header" href="#execution-13">Execution</a></h2>
<p>Authenticate as <strong>front-desk</strong>, which has the <strong>Receptionist</strong> role, and execute a query against the view to observe which fields of each record the application can see:</p>
<pre><code class="language-javascript">db.auth(&quot;front-desk&quot;, &quot;abc123&quot;);

db.patients_view.find();
</code></pre>
<p>Authenticate as <strong>nurse-station</strong>, which has the <strong>Nurse</strong> role, and execute a query against the view to observe which fields of each record the application can see:</p>
<pre><code class="language-javascript">db.auth(&quot;nurse-station&quot;, &quot;xyz789&quot;);

db.patients_view.find();
</code></pre>
<p>Authenticate as <strong>exam-room</strong>, which has the <strong>Doctor</strong> role, and execute a query against the view to observe which fields of each record the application can see:</p>
<pre><code class="language-javascript">db.auth(&quot;exam-room&quot;, &quot;mno456&quot;);

db.patients_view.find();
</code></pre>
<p>For completeness, also view the explain plan for the aggregation pipeline:</p>
<pre><code class="language-javascript">db.patients_view.explain(&quot;executionStats&quot;).find();
</code></pre>
<h2 id="expected-results-13"><a class="header" href="#expected-results-13">Expected Results</a></h2>
<p>Running a query on the view for the <strong>front-desk</strong> (<strong>Receptionist</strong>) includes patient data in the results but omits each patient's weight and medication fields because the user's role does not have sufficient privileges to access those fields. </p>
<pre><code class="language-javascript">[
  {
    id: 'D40230',
    first_name: 'Chelsea',
    last_Name: 'Chow',
    birth_date: ISODate(&quot;1984-11-07T10:12:00.000Z&quot;)
  },
  {
    id: 'R83165',
    first_name: 'Pharrell',
    last_Name: 'Phillips',
    birth_date: ISODate(&quot;1993-05-30T19:44:00.000Z&quot;)
  },
  {
    id: 'X24046',
    first_name: 'Billy',
    last_Name: 'Boaty',
    birth_date: ISODate(&quot;1976-02-07T23:58:00.000Z&quot;)
  },
  {
    id: 'P53212',
    first_name: 'Yazz',
    last_Name: 'Yodeler',
    birth_date: ISODate(&quot;1999-12-25T12:51:00.000Z&quot;)
  }
]
</code></pre>
<p>Running a query on the view for the <strong>nurse-station</strong> (<strong>Nurse</strong>) includes patient data in the results similar to the previous user, but with the <strong>weight</strong> field also shown for each record.</p>
<pre><code class="language-javascript">[
  {
    id: 'D40230',
    first_name: 'Chelsea',
    last_Name: 'Chow',
    birth_date: ISODate(&quot;1984-11-07T10:12:00.000Z&quot;),
    weight: 145
  },
  {
    id: 'R83165',
    first_name: 'Pharrell',
    last_Name: 'Phillips',
    birth_date: ISODate(&quot;1993-05-30T19:44:00.000Z&quot;),
    weight: 137
  },
  {
    id: 'X24046',
    first_name: 'Billy',
    last_Name: 'Boaty',
    birth_date: ISODate(&quot;1976-02-07T23:58:00.000Z&quot;),
    weight: 223
  },
  {
    id: 'P53212',
    first_name: 'Yazz',
    last_Name: 'Yodeler',
    birth_date: ISODate(&quot;1999-12-25T12:51:00.000Z&quot;),
    weight: 156
  }
]
</code></pre>
<p>Running a query on the view for the <strong>exam-room</strong> (<strong>Doctor</strong>) includes each patient's entire data in the results, including the <strong>weight</strong> and <strong>medication</strong> fields, due to the user having sufficient privileges to access those fields. </p>
<pre><code class="language-javascript">[
  {
    id: 'D40230',
    first_name: 'Chelsea',
    last_Name: 'Chow',
    birth_date: ISODate(&quot;1984-11-07T10:12:00.000Z&quot;),
    weight: 145,
    medication: [ 'Insulin', 'Methotrexate' ]
  },
  {
    id: 'R83165',
    first_name: 'Pharrell',
    last_Name: 'Phillips',
    birth_date: ISODate(&quot;1993-05-30T19:44:00.000Z&quot;),
    weight: 137,
    medication: [ 'Fluoxetine' ]
  },
  {
    id: 'X24046',
    first_name: 'Billy',
    last_Name: 'Boaty',
    birth_date: ISODate(&quot;1976-02-07T23:58:00.000Z&quot;),
    weight: 223,
    medication: []
  },
  {
    id: 'P53212',
    first_name: 'Yazz',
    last_Name: 'Yodeler',
    birth_date: ISODate(&quot;1999-12-25T12:51:00.000Z&quot;),
    weight: 156,
    medication: [ 'Tylenol', 'Naproxen' ]
  }
]
</code></pre>
<h2 id="observations-13"><a class="header" href="#observations-13">Observations</a></h2>
<ul>
<li>
<p><strong>Programmatic Vs Declarative Role-Based Access Control (RBAC).</strong> MongoDB provides <a href="https://www.mongodb.com/docs/manual/core/authorization/">Role-Based Access Control</a> (RBAC) to enable an administrator to govern access to database resources. They achieve this by <strong>declaratively</strong> granting system users to one or more roles (e.g. <code>readWrite</code>, <code>find</code>) against one or more resources (e.g. <code>collectionABC</code>, <code>viewXYZ</code>). However, this example chapter goes further by allowing you to include business logic to enforce <strong>programmatic</strong> access rules based on the connecting system user's role. In the example, these &quot;rules&quot; are captured in Aggregation expressions which use the <code>$$USER_ROLES</code> system variable to look up the roles associated with the current requesting system user. The pipeline's logic for both <code>weight</code> and <code>medication</code> uses a condition expression (<code>$cond</code>) to see if the connected user is a member of a named role, and if not, it removes the field. Given the entire set of MongoDB Aggregation operators at your disposal, you can implement whatever custom access control logic you want.</p>
</li>
<li>
<p><strong>Avoid Proliferation Of Views.</strong> There is an alternative solution for this example, enabling a purely declarative RBAC approach by defining three different &quot;hard-coded&quot; views rather than mandating that you code programmatic rules in one view. You would specify one view per role (e.g. <code>receptionist_patients_view</code>, <code>nurse_patients_view</code>, <code>doctor_patients_view</code>). Each view would contain an almost identical aggregation pipeline, varying only in the specific fields it omits. However, such an approach introduces duplication; whenever developers change the view's core aggregation pipeline, they must apply the changes in three places. This proliferation of views will be exasperated when there are 100s of roles involved in a non-trivial application. Thus, adding a programmatic RBAC approach to &quot;fine-tune&quot; access rules reduces maintenance costs and friction to increase agility.</p>
</li>
<li>
<p><strong>Filtering On A View With Index Pushdowns.</strong> As with the <a href="examples/securing-data/redacted-view.html">redacted view example</a>, the view's aggregation pipeline can leverage an index, including the ability, in certain circumstances, to push down the filters passed to the <code>find()</code> operation run against the view. </p>
</li>
<li>
<p><strong>Field-Level Vs Record-Level Access Control.</strong> The example view's pipeline applies field-level access control rules (e.g. the <code>nurse</code> role cannot access a document's <code>medication</code> field). However, adding logic to the pipeline to filter out specific documents is also straightforward, using the approach highlighted in the <a href="examples/securing-data/redacted-view.html">redacted view example</a> to enforce record-level access control. You achieve this by optionally applying a <code>$match</code> operator in the pipeline if the user has a specific role (e.g. &quot;receptionist&quot;) rather than just filtering based on the value of some fields in each document (e.g. if a document's date field is less than a specific point in time).</p>
</li>
<li>
<p><strong>Potential To Factor Out Logic To Dynamic Metadata.</strong> This chapter's example uses &quot;hard-coded&quot; logic to enforce access control rules. Every time the business needs to change a rule (e.g. adjust what fields a Nurse can see), a developer must modify and re-test the code. When such business rules frequently change in dynamic applications, it may be undesirable to mandate a code change and application re-release for each change. Instead, you could factor out metadata into a new collection capturing the mappings of the names of fields each role can access. A business administrator could dynamically modify the mappings in this &quot;special&quot; collection via an administrative user interface. At runtime, the view's pipeline would use a <code>$lookup</code> stage to map the current user's role (using <code>USER_ROLES</code>) to the fields the role can access. The pipeline would then use this list to conditionally show or omit values of each field in its result.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="time-series-examples"><a class="header" href="#time-series-examples">Time-Series Examples</a></h1>
<p>This section provides examples of aggregating time-series data, common in use cases involving financial data sets and Internet-of-Things (IoT).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="iot-power-consumption"><a class="header" href="#iot-power-consumption">IoT Power Consumption</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 5.0    <em>(due to use of <a href="https://docs.mongodb.com/manual/core/timeseries-collections/">time series collections</a>, <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/setWindowFields/"><code>$setWindowFields</code></a> stage &amp; <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/integral/"><code>$integral</code></a> operator)</em></p>
<h2 id="scenario-14"><a class="header" href="#scenario-14">Scenario</a></h2>
<p>You are monitoring various air-conditioning units running in two buildings on an industrial campus. Every 30 minutes, a device in each unit sends the unit's current power consumption reading back to base, which a central database persists. You want to analyse this data to see how much energy in kilowatt-hours (kWh) each air-conditioning unit has consumed over the last hour for each reading received. Furthermore, you want to compute the total energy consumed by all the air-conditioning units combined in each building for every hour.</p>
<h2 id="sample-data-population-14"><a class="header" href="#sample-data-population-14">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate a new <code>device_readings</code> collection with device readings spanning 3 hours of a day for air-conditioning units in two different buildings.</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-iot-power-consumption&quot;);
db.dropDatabase();

// Use a time-series collection for optimal processing
// NOTE: This command can be commented out and the full example will still work
db.createCollection(&quot;device_readings&quot;, {
  &quot;timeseries&quot;: {
    &quot;timeField&quot;: &quot;timestamp&quot;,
    &quot;metaField&quot;: &quot;deviceID&quot;,
    &quot;granularity&quot;: &quot;minutes&quot;
  }
});

// Create compound index to aid performance for partitionBy &amp; sortBy of setWindowFields
db.device_readings.createIndex({&quot;deviceID&quot;: 1, &quot;timestamp&quot;: 1});

// Insert 18 records into the device readings collection
db.device_readings.insertMany([
  // 11:29am device readings
  {
    &quot;buildingID&quot;: &quot;Building-ABC&quot;, 
    &quot;deviceID&quot;: &quot;UltraAirCon-111&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T11:29:59Z&quot;),
    &quot;powerKilowatts&quot;: 8,     
  },
  {
    &quot;buildingID&quot;: &quot;Building-ABC&quot;,
    &quot;deviceID&quot;: &quot;UltraAirCon-222&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T11:29:59Z&quot;),
    &quot;powerKilowatts&quot;: 7,     
  },
  {
    &quot;buildingID&quot;: &quot;Building-XYZ&quot;,
    &quot;deviceID&quot;: &quot;UltraAirCon-666&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T11:29:59Z&quot;),
    &quot;powerKilowatts&quot;: 10,     
  },
  
  // 11:59am device readings
  {
    &quot;buildingID&quot;: &quot;Building-ABC&quot;,
    &quot;deviceID&quot;: &quot;UltraAirCon-222&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T11:59:59Z&quot;),
    &quot;powerKilowatts&quot;: 9,     
  },
  {
    &quot;buildingID&quot;: &quot;Building-ABC&quot;,
    &quot;deviceID&quot;: &quot;UltraAirCon-111&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T11:59:59Z&quot;),
    &quot;powerKilowatts&quot;: 8,     
  },
  {
    &quot;buildingID&quot;: &quot;Building-XYZ&quot;,
    &quot;deviceID&quot;: &quot;UltraAirCon-666&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T11:59:59Z&quot;),
    &quot;powerKilowatts&quot;: 11,     
  },
  
  // 12:29pm device readings
  {
    &quot;buildingID&quot;: &quot;Building-ABC&quot;,
    &quot;deviceID&quot;: &quot;UltraAirCon-222&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T12:29:59Z&quot;),
    &quot;powerKilowatts&quot;: 9,     
  },
  {
    &quot;buildingID&quot;: &quot;Building-ABC&quot;,
    &quot;deviceID&quot;: &quot;UltraAirCon-111&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T12:29:59Z&quot;),
    &quot;powerKilowatts&quot;: 9,     
  },
  {
    &quot;buildingID&quot;: &quot;Building-XYZ&quot;,
    &quot;deviceID&quot;: &quot;UltraAirCon-666&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T12:29:59Z&quot;),
    &quot;powerKilowatts&quot;: 10,     
  },

  // 12:59pm device readings
  {
    &quot;buildingID&quot;: &quot;Building-ABC&quot;,
    &quot;deviceID&quot;: &quot;UltraAirCon-222&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T12:59:59Z&quot;),
    &quot;powerKilowatts&quot;: 8,     
  },
  {
    &quot;buildingID&quot;: &quot;Building-ABC&quot;,
    &quot;deviceID&quot;: &quot;UltraAirCon-111&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T12:59:59Z&quot;),
    &quot;powerKilowatts&quot;: 8,     
  },
  {
    &quot;buildingID&quot;: &quot;Building-XYZ&quot;,
    &quot;deviceID&quot;: &quot;UltraAirCon-666&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T12:59:59Z&quot;),
    &quot;powerKilowatts&quot;: 11,     
  },

  // 13:29pm device readings
  {
    &quot;buildingID&quot;: &quot;Building-ABC&quot;,
    &quot;deviceID&quot;: &quot;UltraAirCon-222&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T13:29:59Z&quot;),
    &quot;powerKilowatts&quot;: 9,     
  },
  {
    &quot;buildingID&quot;: &quot;Building-ABC&quot;,
    &quot;deviceID&quot;: &quot;UltraAirCon-111&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T13:29:59Z&quot;),
    &quot;powerKilowatts&quot;: 9,     
  },
  {
    &quot;buildingID&quot;: &quot;Building-XYZ&quot;,
    &quot;deviceID&quot;: &quot;UltraAirCon-666&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T13:29:59Z&quot;),
    &quot;powerKilowatts&quot;: 10,     
  },

  // 13:59pm device readings
  {
    &quot;buildingID&quot;: &quot;Building-ABC&quot;,
    &quot;deviceID&quot;: &quot;UltraAirCon-222&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T13:59:59Z&quot;),
    &quot;powerKilowatts&quot;: 8,     
  },
  {
    &quot;buildingID&quot;: &quot;Building-ABC&quot;,
    &quot;deviceID&quot;: &quot;UltraAirCon-111&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T13:59:59Z&quot;),
    &quot;powerKilowatts&quot;: 8,     
  },
  {
    &quot;buildingID&quot;: &quot;Building-XYZ&quot;,
    &quot;deviceID&quot;: &quot;UltraAirCon-666&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T13:59:59Z&quot;),
    &quot;powerKilowatts&quot;: 11,     
  },
]);
</code></pre>
<h2 id="aggregation-pipeline-14"><a class="header" href="#aggregation-pipeline-14">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform an aggregation to calculate the energy an air-conditioning unit has consumed over the last hour for each reading received:</p>
<pre><code class="language-javascript">var pipelineRawReadings = [
  // Calculate each unit's energy consumed over the last hour for each reading
  {&quot;$setWindowFields&quot;: {
    &quot;partitionBy&quot;: &quot;$deviceID&quot;,
    &quot;sortBy&quot;: {&quot;timestamp&quot;: 1},    
    &quot;output&quot;: {
      &quot;consumedKilowattHours&quot;: {
        &quot;$integral&quot;: {
          &quot;input&quot;: &quot;$powerKilowatts&quot;,
          &quot;unit&quot;: &quot;hour&quot;,
        },
        &quot;window&quot;: {
          &quot;range&quot;: [-1, &quot;current&quot;],
          &quot;unit&quot;: &quot;hour&quot;,
        },
      },
    },
  }},
];
</code></pre>
<p>Define a pipeline ready to compute the total energy consumed by all the air-conditioning units combined in each building for every hour:</p>
<pre><code class="language-javascript">var pipelineBuildingsSummary = [
  // Calculate each unit's energy consumed over the last hour for each reading
  {&quot;$setWindowFields&quot;: {
    &quot;partitionBy&quot;: &quot;$deviceID&quot;,
    &quot;sortBy&quot;: {&quot;timestamp&quot;: 1},    
    &quot;output&quot;: {
      &quot;consumedKilowattHours&quot;: {
        &quot;$integral&quot;: {
          &quot;input&quot;: &quot;$powerKilowatts&quot;,
          &quot;unit&quot;: &quot;hour&quot;,
        },
        &quot;window&quot;: {
          &quot;range&quot;: [-1, &quot;current&quot;],
          &quot;unit&quot;: &quot;hour&quot;,
        },
      },
    },
  }},
  
  // Sort each reading by unit/device and then by timestamp
  {&quot;$sort&quot;: {
    &quot;deviceID&quot;: 1,
    &quot;timestamp&quot;: 1,
  }},    
  
  // Group readings together for each hour for each device using
  // the last calculated energy consumption field for each hour
  {&quot;$group&quot;: {
    &quot;_id&quot;: {
      &quot;deviceID&quot;: &quot;$deviceID&quot;,
      &quot;date&quot;: {
          &quot;$dateTrunc&quot;: {
            &quot;date&quot;: &quot;$timestamp&quot;,
            &quot;unit&quot;: &quot;hour&quot;,
          }
      },
    },
    &quot;buildingID&quot;: {&quot;$last&quot;: &quot;$buildingID&quot;},
    &quot;consumedKilowattHours&quot;: {&quot;$last&quot;: &quot;$consumedKilowattHours&quot;},
  }},    

  // Sum together the energy consumption for the whole building
  // for each hour across all the units in the building   
  {&quot;$group&quot;: {
    &quot;_id&quot;: {
      &quot;buildingID&quot;: &quot;$buildingID&quot;,
      &quot;dayHour&quot;: {&quot;$dateToString&quot;: {&quot;format&quot;: &quot;%Y-%m-%d  %H&quot;, &quot;date&quot;: &quot;$_id.date&quot;}},
    },
    &quot;consumedKilowattHours&quot;: {&quot;$sum&quot;: &quot;$consumedKilowattHours&quot;},
  }},    

  // Sort the results by each building and then by each hourly summary
  {&quot;$sort&quot;: {
    &quot;_id.buildingID&quot;: 1,
    &quot;_id.dayHour&quot;: 1,
  }},    

  // Make the results more presentable with meaningful field names
  {&quot;$set&quot;: {
    &quot;buildingID&quot;: &quot;$_id.buildingID&quot;,
    &quot;dayHour&quot;: &quot;$_id.dayHour&quot;,
    &quot;_id&quot;: &quot;$$REMOVE&quot;,
  }},      
];
</code></pre>
<h2 id="execution-14"><a class="header" href="#execution-14">Execution</a></h2>
<p>Execute an aggregation using the pipeline to calculate the energy an air-conditioning unit has consumed over the last hour for each reading received and also view its explain plan:</p>
<pre><code class="language-javascript">db.device_readings.aggregate(pipelineRawReadings);
</code></pre>
<pre><code class="language-javascript">db.device_readings.explain(&quot;executionStats&quot;).aggregate(pipelineRawReadings);
</code></pre>
<p>Execute an aggregation using the pipeline to compute the total energy consumed by all the air-conditioning units combined in each building for every hour and also view its explain plan:</p>
<pre><code class="language-javascript">db.device_readings.aggregate(pipelineBuildingsSummary);
</code></pre>
<pre><code class="language-javascript">db.device_readings.explain(&quot;executionStats&quot;).aggregate(pipelineBuildingsSummary);
</code></pre>
<h2 id="expected-results-14"><a class="header" href="#expected-results-14">Expected Results</a></h2>
<p>For the pipeline to calculate the energy an air-conditioning unit has consumed over the last hour for each reading received, results like the following should be returned (redacted for brevity - only showing the first few records):</p>
<pre><code class="language-javascript">[
  {
    _id: ObjectId(&quot;60ed5e679ea1f9f74814ca2b&quot;),
    buildingID: 'Building-ABC',
    deviceID: 'UltraAirCon-111',
    timestamp: ISODate(&quot;2021-07-03T11:29:59.000Z&quot;),
    powerKilowatts: 8,
    consumedKilowattHours: 0
  },
  {
    _id: ObjectId(&quot;60ed5e679ea1f9f74814ca2f&quot;),
    buildingID: 'Building-ABC',
    deviceID: 'UltraAirCon-111',
    timestamp: ISODate(&quot;2021-07-03T11:59:59.000Z&quot;),
    powerKilowatts: 8,
    consumedKilowattHours: 4
  },
  {
    _id: ObjectId(&quot;60ed5e679ea1f9f74814ca32&quot;),
    buildingID: 'Building-ABC',
    deviceID: 'UltraAirCon-111',
    timestamp: ISODate(&quot;2021-07-03T12:29:59.000Z&quot;),
    powerKilowatts: 9,
    consumedKilowattHours: 8.25
  },
  {
    _id: ObjectId(&quot;60ed5e679ea1f9f74814ca35&quot;),
    buildingID: 'Building-ABC',
    deviceID: 'UltraAirCon-111',
    timestamp: ISODate(&quot;2021-07-03T12:59:59.000Z&quot;),
    powerKilowatts: 8,
    consumedKilowattHours: 8.5
  },
  {
    _id: ObjectId(&quot;60ed5e679ea1f9f74814ca38&quot;),
    buildingID: 'Building-ABC',
    deviceID: 'UltraAirCon-111',
    timestamp: ISODate(&quot;2021-07-03T13:29:59.000Z&quot;),
    powerKilowatts: 9,
    consumedKilowattHours: 8.5
  },
  ...
  ...
]
</code></pre>
<p>For the pipeline to compute the total energy consumed by all the air-conditioning units combined in each building for every hour, the following results should be returned:</p>
<pre><code class="language-javascript">[
  {
    buildingID: 'Building-ABC',
    dayHour: '2021-07-03  11',
    consumedKilowattHours: 8
  },
  {
    buildingID: 'Building-ABC',
    dayHour: '2021-07-03  12',
    consumedKilowattHours: 17.25
  },
  {
    buildingID: 'Building-ABC',
    dayHour: '2021-07-03  13',
    consumedKilowattHours: 17
  },
  {
    buildingID: 'Building-XYZ',
    dayHour: '2021-07-03  11',
    consumedKilowattHours: 5.25
  },
  {
    buildingID: 'Building-XYZ',
    dayHour: '2021-07-03  12',
    consumedKilowattHours: 10.5
  },
  {
    buildingID: 'Building-XYZ',
    dayHour: '2021-07-03  13',
    consumedKilowattHours: 10.5
  }
]
</code></pre>
<h2 id="observations-14"><a class="header" href="#observations-14">Observations</a></h2>
<ul>
<li>
<p><strong>Integral Trapezoidal Rule.</strong> As <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/integral/">documented in the MongoDB Manual</a>, <code>$integral</code> <em>&quot;returns an approximation for the mathematical integral value, which is calculated using the trapezoidal rule&quot;</em>. For non-mathematicians, this explanation may be hard to understand. You may find it easier to comprehend the behaviour of the <code>$integral</code> operator by studying the illustration below and the explanation that follows:</p>
<p><img src="examples/time-series/./pics/trapezoidal-rule-example.png" alt="Example of calculating power consumption by approximating integrals using the trapezoidal rule" /></p>
<p>Essentially the <a href="https://en.wikipedia.org/wiki/Trapezoidal_rule">trapezoidal rule</a> determines the area of a region between two points under a graph by matching the region with a trapezoid shape that approximately fits this region and then calculating the area of this trapezoid. You can see a set of points on the illustrated graph with the matched trapezoid shape underneath each pair of points. For this IoT Power Consumption example, the points on the graph represent an air-conditioning unit's power readings captured every 30 minutes. The Y-axis is the <em>power rate</em> in Kilowatts, and the X-axis is <em>time</em> to indicate when the device captured each reading. Consequently, for this example, the energy consumed by the air-conditioning unit for a given hour's span is the area of the hour's specific section under the graph. This section's area is approximately the area of the two trapezoids shown. Using the <code>$integral</code> operator for the window of time you define in the <code>$setWindowFields</code> stage, you are asking for this approximate area to be calculated, which is the Kilowatt-hours consumed by the air-conditioning unit in one hour.</p>
</li>
<li>
<p><strong>Window Range Definition.</strong> For every captured document representing a device reading, this example's pipeline identifies a window of <em>1-hour</em> of previous documents relative to this <em>current</em> document. The pipeline uses this set of documents as the input for the <code>$integral</code> operator. It defines this window range in the setting <code>range: [-1, &quot;current&quot;], unit: &quot;hour&quot;</code>. The pipeline assigns the output of the <code>$integral</code> calculation to a new field called <code>consumedKilowattHours</code>.</p>
</li>
<li>
<p><strong>One Hour Range Vs Hours Output.</strong> The fact that the <code>$setWindowFields</code> stage in the pipeline defines <code>unit: &quot;hour&quot;</code> in two places may appear redundant at face value. However, this is not the case, and each serves a different purpose. As described in the previous observation, <code>unit: &quot;hour&quot;</code> for the <code>&quot;window&quot;</code> option helps dictate the size of the window of the previous number of documents to analyse. However, <code>unit: &quot;hour&quot;</code> for the <code>$integral</code> operator defines that the output should be in hours (&quot;Kilowatt-hours&quot; in this example), yielding the result <code>consumedKilowattHours: 8.5</code> for one of the processed device readings. However, if the pipeline defined this <code>$integral</code> parameter to be <code>&quot;unit&quot;: &quot;minute&quot;</code> instead, which is perfectly valid, the output value would be <code>510</code> Kilowatt-minutes (i.e. 8.5 x 60 minutes).</p>
</li>
<li>
<p><strong>Optional Time Series Collection.</strong> This example uses a <a href="https://docs.mongodb.com/manual/core/timeseries-collections/">time series collection</a> to store sequences of device measurements over time efficiently. Employing a time series collection is optional, as shown in the <code>NOTE</code> Javascript comment in the example code. The aggregation pipeline does not need to be changed and achieves the same output if you use a regular collection instead. However, when dealing with large data sets, the aggregation will complete quicker by employing a time series collection.</p>
</li>
<li>
<p><strong>Index For Partition By &amp; Sort By.</strong> In this example, you define the index <code>{deviceID: 1, timestamp: 1}</code> to optimise the use of the combination of the <code>partitionBy</code> and <code>sortBy</code> parameters in the <code>$setWindowFields</code> stage. This means that the aggregation runtime does not have to perform a slow in-memory sort based on these two fields, and it also avoids the pipeline stage memory limit of 100 MB. It is beneficial to use this index regardless of whether you employ a regular collection or adopt a time series collection.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="state-change-boundaries"><a class="header" href="#state-change-boundaries">State Change Boundaries</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 5.0    <em>(due to use of <a href="https://docs.mongodb.com/manual/core/timeseries-collections/">time series collections</a>, <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/setWindowFields/"><code>$setWindowFields</code></a> stage &amp; <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/shift/"><code>$shift</code></a> operator)</em></p>
<h2 id="scenario-15"><a class="header" href="#scenario-15">Scenario</a></h2>
<p>You are monitoring various industrial devices (e.g. heaters, fans) contained in the business locations of your clients. You want to understand the typical patterns of when these devices are on and off to help you optimise for sustainability by reducing energy costs and carbon footprint. The source database contains periodic readings for every device, capturing whether each is currently on or off. You need a less verbose view that condenses this data, highlighting each device's timespan in a particular on or off state.</p>
<h2 id="sample-data-population-15"><a class="header" href="#sample-data-population-15">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate the device status collection:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-state-change-boundaries&quot;);
db.dropDatabase();

// Use a time-series collection for optimal processing
// NOTE: This command can be commented out and the full example will still work
db.createCollection(&quot;device_status&quot;, {
  &quot;timeseries&quot;: {
    &quot;timeField&quot;: &quot;timestamp&quot;,
    &quot;metaField&quot;: &quot;deviceID&quot;,
    &quot;granularity&quot;: &quot;minutes&quot;
  }
});

// Create compound index for 'partitionBy' &amp; 'sortBy' in first $setWindowFields use
db.device_status.createIndex({&quot;deviceID&quot;: 1, &quot;timestamp&quot;: 1});

// Insert 9 records into the deployments collection
db.device_status.insertMany([
  {
    &quot;deviceID&quot;: &quot;HEATER-111&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T11:09:00Z&quot;),
    &quot;state&quot;: &quot;on&quot;,     
  },
  {
    &quot;deviceID&quot;: &quot;FAN-999&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T11:09:00Z&quot;),
    &quot;state&quot;: &quot;on&quot;,     
  },
  {
    &quot;deviceID&quot;: &quot;HEATER-111&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T11:19:00Z&quot;),
    &quot;state&quot;: &quot;on&quot;,     
  },
  {
    &quot;deviceID&quot;: &quot;HEATER-111&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T11:29:00Z&quot;),
    &quot;state&quot;: &quot;on&quot;,     
  },
  {
    &quot;deviceID&quot;: &quot;FAN-999&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T11:39:00Z&quot;),
    &quot;state&quot;: &quot;off&quot;,     
  },
  {
    &quot;deviceID&quot;: &quot;HEATER-111&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T11:39:00Z&quot;),
    &quot;state&quot;: &quot;off&quot;,     
  },
  {
    &quot;deviceID&quot;: &quot;HEATER-111&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T11:49:00Z&quot;),
    &quot;state&quot;: &quot;off&quot;,     
  },
  {
    &quot;deviceID&quot;: &quot;HEATER-111&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T11:59:00Z&quot;),
    &quot;state&quot;: &quot;on&quot;,     
  },
  {
    &quot;deviceID&quot;: &quot;DEHUMIDIFIER-555&quot;,    
    &quot;timestamp&quot;: ISODate(&quot;2021-07-03T11:29:00Z&quot;),
    &quot;state&quot;: &quot;on&quot;,     
  },
]);
</code></pre>
<h2 id="aggregation-pipeline-15"><a class="header" href="#aggregation-pipeline-15">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform an aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Capture previous and next records' state into new fields in this current record
  {&quot;$setWindowFields&quot;: {
    &quot;partitionBy&quot;: &quot;$deviceID&quot;,
    &quot;sortBy&quot;: {&quot;timestamp&quot;: 1},    
    &quot;output&quot;: {
      &quot;previousState&quot;: {
        &quot;$shift&quot;: {
          &quot;output&quot;: &quot;$state&quot;,
          &quot;by&quot;: -1,
        }
      },
      &quot;nextState&quot;: {
        &quot;$shift&quot;: {
          &quot;output&quot;: &quot;$state&quot;,
          &quot;by&quot;: 1,
        }
      },
    }
  }},

  // Use current record's timestamp as &quot;startTimestamp&quot; only if state changed from
  // previous record in series, and only set &quot;endMarkerDate&quot; to current record's
  // timestamp if the state changes between current and next records in the series
  {&quot;$set&quot;: {
    &quot;startTimestamp&quot; : {
      &quot;$cond&quot;: [
        {&quot;$eq&quot;: [&quot;$state&quot;, &quot;$previousState&quot;]}, 
        &quot;$$REMOVE&quot;,
        &quot;$timestamp&quot;,
      ]    
    },
    &quot;endMarkerDate&quot; : {
      &quot;$cond&quot;: [
        {&quot;$eq&quot;: [&quot;$state&quot;, &quot;$nextState&quot;]}, 
        &quot;$$REMOVE&quot;,
        &quot;$timestamp&quot;,
      ]    
    },
  }},
  
  // Only keep records where state has just changed or is just about to change (so
  // mostly start/end pairs, but not always if state change only lasted one record)
  {&quot;$match&quot;: {
    &quot;$expr&quot;: {
      &quot;$or&quot;: [     
        {&quot;$ne&quot;: [&quot;$state&quot;, &quot;$previousState&quot;]},
        {&quot;$ne&quot;: [&quot;$state&quot;, &quot;$nextState&quot;]},
      ]
    }
  }},    
  
  // Set &quot;nextMarkerDate&quot; to the timestamp of the next record in the series (will
  // be set to 'null' if no next record to indicate 'unbounded')
  {&quot;$setWindowFields&quot;: {
    &quot;partitionBy&quot;: &quot;$deviceID&quot;,
    &quot;sortBy&quot;: {&quot;timestamp&quot;: 1},    
    &quot;output&quot;: {
      &quot;nextMarkerDate&quot;: {
        &quot;$shift&quot;: {
          &quot;output&quot;: &quot;$timestamp&quot;,
          &quot;by&quot;: 1,
        }
      },
    }
  }},  

  // Only keep records at the start of the state change boundaries (throw away
  // matching pair end records, if any)
  {&quot;$match&quot;: {
    &quot;$expr&quot;: {
      &quot;$ne&quot;: [&quot;$state&quot;, &quot;$previousState&quot;],
    }
  }},
    
  // If no boundary after this record (it's the last matching record in the series),
  // set &quot;endTimestamp&quot; as unbounded (null)
  // Otherwise, if this start boundary record was also an end boundary record (not
  //  paired - only 1 record before state changed), set &quot;endTimestamp&quot; to end timestamp
  // Otherwise, set &quot;endTimestamp&quot; to what was the captured timestamp from the original
  //  matching pair in the series (where the end paired record has since been removed)
  {&quot;$set&quot;: {
    &quot;endTimestamp&quot; : {
      &quot;$switch&quot;: {
        &quot;branches&quot;: [
          // Unbounded, so no final timestamp in series
          {&quot;case&quot;: {&quot;$eq&quot;: [{&quot;$type&quot;: &quot;$nextMarkerDate&quot;}, &quot;null&quot;]}, &quot;then&quot;: null},  
          // Use end timestamp from what was same end record as start record 
          {&quot;case&quot;: {&quot;$ne&quot;: [{&quot;$type&quot;: &quot;$endMarkerDate&quot;}, &quot;missing&quot;]}, &quot;then&quot;: &quot;$endMarkerDate&quot;},  
        ],
        // Use timestamp from what was an end record paired with separate start record
        &quot;default&quot;: &quot;$nextMarkerDate&quot;,  
      }
    },   
  }},

  // Remove unwanted fields from the final result
  {&quot;$unset&quot;: [
    &quot;_id&quot;,
    &quot;timestamp&quot;,
    &quot;previousState&quot;,
    &quot;nextState&quot;,
    &quot;endMarkerDate&quot;,
    &quot;nextMarkerDate&quot;,
  ]}
];
</code></pre>
<h2 id="execution-15"><a class="header" href="#execution-15">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.device_status.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.device_status.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-15"><a class="header" href="#expected-results-15">Expected Results</a></h2>
<p>Six documents should be returned, each of which captures the duration between two state change boundaries (<code>on→off</code> or <code>off→on</code>) for a device, as shown below:</p>
<pre><code class="language-javascript">[
  {
    deviceID: 'DEHUMIDIFIER-555',
    state: 'on',
    startTimestamp: ISODate(&quot;2021-07-03T11:29:00.000Z&quot;),
    endTimestamp: null
  },
  {
    deviceID: 'FAN-999',
    state: 'on',
    startTimestamp: ISODate(&quot;2021-07-03T11:09:00.000Z&quot;),
    endTimestamp: ISODate(&quot;2021-07-03T11:09:00.000Z&quot;)
  },
  {
    deviceID: 'FAN-999',
    state: 'off',
    startTimestamp: ISODate(&quot;2021-07-03T11:39:00.000Z&quot;),
    endTimestamp: null
  },
  {
    deviceID: 'HEATER-111',
    state: 'on',
    startTimestamp: ISODate(&quot;2021-07-03T11:09:00.000Z&quot;),
    endTimestamp: ISODate(&quot;2021-07-03T11:29:00.000Z&quot;)
  },
  {
    deviceID: 'HEATER-111',
    state: 'off',
    startTimestamp: ISODate(&quot;2021-07-03T11:39:00.000Z&quot;),
    endTimestamp: ISODate(&quot;2021-07-03T11:49:00.000Z&quot;)
  },
  {
    deviceID: 'HEATER-111',
    state: 'on',
    startTimestamp: ISODate(&quot;2021-07-03T11:59:00.000Z&quot;),
    endTimestamp: null
  }
]
</code></pre>
<h2 id="observations-15"><a class="header" href="#observations-15">Observations</a></h2>
<ul>
<li>
<p><strong>Null End Timestamps.</strong> The last record for each specific device has the value of its <code>endTimestamp</code> field set to <code>null</code>. The null value indicates that this record contains the final known state of the device.</p>
</li>
<li>
<p><strong>Peeking At One Document From Another.</strong> By using the windowing stage (<code>$setWindowFields</code>), you can apply aggregation operations that span multiple documents. Combined with its shift operator (<code>$shift</code>), you can peek at the content of preceding or following documents and bring some of that other document's content into the current document. In this example, you copy the device's state from the previous document (<code>-1</code> offset) and the following document (<code>+1</code> offset) into the current document. Capturing these adjacent values enables subsequent stages in your pipeline to determine if the current device has changed state. Using <code>$shift</code> relies on the documents already being partitioned (e.g. by device ID) and sorted (e.g. by timestamp), which the containing <code>$setWindowFields</code> stage is enforcing.</p>
</li>
<li>
<p><strong>Double Use Of A Windowing Stage.</strong> The pipeline's first windowing stage and the subsequent matching stage capture device documents where the device's state has changed from <code>on</code> to <code>off</code> or vice versa. In many cases, this results in adjacent pairs of documents where the first document in the pair captures the first time the device has a new state, and the second document captures the last time it was in that same state before changing again. The example's pipeline requires a later second windowing stage to condense each pair of 'boundary' documents into one document. This second windowing stage again uses a shift operator to bring the timestamp of the pair's second document into a new field in the pair's first document. Consequently, single documents now exist which contain both the start and end timestamps of a particular device's state. Finally, the pipeline employs further logic to clean things up because, in some situations, there won't be a pair of related documents. For example, if a device's recorded state changes and immediately changes again, or it's the last recorded state of the device, there will be no paired document marking the end state.</p>
</li>
<li>
<p><strong>Time Series Collection &amp; Indexes.</strong> As with the <a href="examples/time-series/iot-power-consumption.html">previous example</a>, the aggregation can optionally use a <a href="https://docs.mongodb.com/manual/core/timeseries-collections/">time series collection</a> to store sequences of device measurements over time for efficient storage and fast querying. Additionally, as with the previous example, the aggregation can leverage an index for <code>{deviceID: 1, timestamp: 1}</code> to avoid the <code>$setWindowFields</code> stage having to perform a slow in-memory sort operation.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="array-manipulation-examples"><a class="header" href="#array-manipulation-examples">Array Manipulation Examples</a></h1>
<p>This section provides examples for processing and manipulating array fields contained in documents, without having to <em>unwind</em> and <em>re-group</em> the content of each array.</p>
<p>You should ensure you have read the <a href="examples/array-manipulations/../../guides/advanced-arrays.html">Advanced Use Of Expressions For Array Processing</a> chapter first before digesting the examples in this book's section.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="summarising-arrays-for-first-last-minimum-maximum--average-values"><a class="header" href="#summarising-arrays-for-first-last-minimum-maximum--average-values">Summarising Arrays For First, Last, Minimum, Maximum &amp; Average Values</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.4    <em>(due to use of <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/first-array-element/"><code>$first</code></a> and <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/last-array-element/"><code>$last</code></a> array operators)</em></p>
<h2 id="scenario-16"><a class="header" href="#scenario-16">Scenario</a></h2>
<p>You want to generate daily summaries for the exchange rates of foreign currency &quot;pairs&quot; (e.g. &quot;Euro-to-USDollar&quot;). You need to analyse an array of persisted hourly rates for each currency pair for each day. You will output a daily summary of the open (first), close (last), low (minimum), high (maximum) and average exchange rate values for each currency pair.</p>
<h2 id="sample-data-population-16"><a class="header" href="#sample-data-population-16">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate the new <em>currency-pair daily</em> collection:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-array-high-low-avg&quot;);
db.dropDatabase();

// Inserts records into the currency_pair_values collection
db.currency_pair_values.insertMany([
  {
    &quot;currencyPair&quot;: &quot;USD/GBP&quot;,
    &quot;day&quot;: ISODate(&quot;2021-07-05T00:00:00.000Z&quot;),
    &quot;hour_values&quot;: [
      NumberDecimal(&quot;0.71903411&quot;), NumberDecimal(&quot;0.72741832&quot;), NumberDecimal(&quot;0.71997271&quot;),
      NumberDecimal(&quot;0.73837282&quot;), NumberDecimal(&quot;0.75262621&quot;), NumberDecimal(&quot;0.74739202&quot;),
      NumberDecimal(&quot;0.72972612&quot;), NumberDecimal(&quot;0.73837292&quot;), NumberDecimal(&quot;0.72393721&quot;),
      NumberDecimal(&quot;0.72746837&quot;), NumberDecimal(&quot;0.73787372&quot;), NumberDecimal(&quot;0.73746483&quot;),
      NumberDecimal(&quot;0.73373632&quot;), NumberDecimal(&quot;0.75737372&quot;), NumberDecimal(&quot;0.76783263&quot;),
      NumberDecimal(&quot;0.75632828&quot;), NumberDecimal(&quot;0.75362823&quot;), NumberDecimal(&quot;0.74682282&quot;),
      NumberDecimal(&quot;0.74628263&quot;), NumberDecimal(&quot;0.74726262&quot;), NumberDecimal(&quot;0.75376722&quot;),
      NumberDecimal(&quot;0.75799222&quot;), NumberDecimal(&quot;0.75545352&quot;), NumberDecimal(&quot;0.74998835&quot;),
    ],
  },
  {
    &quot;currencyPair&quot;: &quot;EUR/GBP&quot;,
    &quot;day&quot;: ISODate(&quot;2021-07-05T00:00:00.000Z&quot;),
    &quot;hour_values&quot;: [
      NumberDecimal(&quot;0.86739394&quot;), NumberDecimal(&quot;0.86763782&quot;), NumberDecimal(&quot;0.87362937&quot;),
      NumberDecimal(&quot;0.87373652&quot;), NumberDecimal(&quot;0.88002736&quot;), NumberDecimal(&quot;0.87866372&quot;),
      NumberDecimal(&quot;0.87862628&quot;), NumberDecimal(&quot;0.87374621&quot;), NumberDecimal(&quot;0.87182626&quot;),
      NumberDecimal(&quot;0.86892723&quot;), NumberDecimal(&quot;0.86373732&quot;), NumberDecimal(&quot;0.86017236&quot;),
      NumberDecimal(&quot;0.85873636&quot;), NumberDecimal(&quot;0.85762283&quot;), NumberDecimal(&quot;0.85362373&quot;),
      NumberDecimal(&quot;0.85306218&quot;), NumberDecimal(&quot;0.85346632&quot;), NumberDecimal(&quot;0.84647462&quot;),
      NumberDecimal(&quot;0.84694720&quot;), NumberDecimal(&quot;0.84723232&quot;), NumberDecimal(&quot;0.85002222&quot;),
      NumberDecimal(&quot;0.85468322&quot;), NumberDecimal(&quot;0.85675656&quot;), NumberDecimal(&quot;0.84811122&quot;),
    ],
  },
]);
</code></pre>
<h2 id="aggregation-pipeline-16"><a class="header" href="#aggregation-pipeline-16">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Generate day summaries from the hourly array values
  {&quot;$set&quot;: {
    &quot;summary.open&quot;: {&quot;$first&quot;: &quot;$hour_values&quot;},
    &quot;summary.low&quot;: {&quot;$min&quot;: &quot;$hour_values&quot;},
    &quot;summary.high&quot;: {&quot;$max&quot;: &quot;$hour_values&quot;},
    &quot;summary.close&quot;: {&quot;$last&quot;: &quot;$hour_values&quot;},
    &quot;summary.average&quot;: {&quot;$avg&quot;: &quot;$hour_values&quot;},
  }},

  // Exclude unrequired fields from each daily currency pair record
  {&quot;$unset&quot;: [
    &quot;_id&quot;,
    &quot;hour_values&quot;,
  ]},
];
</code></pre>
<h2 id="execution-16"><a class="header" href="#execution-16">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.currency_pair_values.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.currency_pair_values.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-16"><a class="header" href="#expected-results-16">Expected Results</a></h2>
<p>Two documents should be returned, now showing the daily summary open, low, high, close and average prices for each currency pair:</p>
<pre><code class="language-javascript">[
  {
    currencyPair: 'USD/GBP',
    day: ISODate(&quot;2021-07-05T00:00:00.000Z&quot;),
    summary: {
      open: NumberDecimal(&quot;0.71903411&quot;),
      low: NumberDecimal(&quot;0.71903411&quot;),
      high: NumberDecimal(&quot;0.76783263&quot;),
      close: NumberDecimal(&quot;0.74998835&quot;),
      average: NumberDecimal(&quot;0.74275533&quot;)
    }
  },
  {
    currencyPair: 'EUR/GBP',
    day: ISODate(&quot;2021-07-05T00:00:00.000Z&quot;),
    summary: {
      open: NumberDecimal(&quot;0.86739394&quot;),
      low: NumberDecimal(&quot;0.84647462&quot;),
      high: NumberDecimal(&quot;0.88002736&quot;),
      close: NumberDecimal(&quot;0.84811122&quot;),
      average: NumberDecimal(&quot;0.86186929875&quot;)
    }
  }
]
</code></pre>
<h2 id="observations-16"><a class="header" href="#observations-16">Observations</a></h2>
<ul>
<li>
<p><strong><code>$first</code> &amp; <code>$last</code> For Earlier MongoDB Versions.</strong> MongoDB only introduced the <code>$first</code> and <code>$last</code> array operator expressions in version 4.4. However, it is straightforward for you to replace each one in the pipeline with an equivalent solution, using the <code>$arrayElemAt</code> operator. Below are the alternatives you can use instead of <code>$first</code> and <code>$last</code> to operate correctly in MongoDB versions before 4.4:</p>
<pre><code class="language-javascript">// $first equivalent
&quot;summary.open&quot;: {&quot;$arrayElemAt&quot;: [&quot;$hour_values&quot;, 0]},
  
// $last equivalent  
&quot;summary.close&quot;: {&quot;$arrayElemAt&quot;: [&quot;$hour_values&quot;, -1]},
</code></pre>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pivot-array-items-by-a-key"><a class="header" href="#pivot-array-items-by-a-key">Pivot Array Items By A Key</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.2</p>
<h2 id="scenario-17"><a class="header" href="#scenario-17">Scenario</a></h2>
<p>You have a set of geographically dispersed weather station zones where each zone has multiple sensor devices collecting readings such as temperature, humidity and pressure. Each weather station assembles readings from its devices and once per hour transmits this set of measurements to a central database to store. The set of persisted readings are randomly ordered measurements for different devices in the zone. You need to take the mix of readings and group these by device, so the weather data is easier to consume by downstream dashboards and applications.</p>
<blockquote>
<p><em>This example's pipeline relies on some of the more difficult to understand array operator expressions, like <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/map/"><code>$map</code></a>, <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/mergeObjects/"><code>$mergeObjects</code></a> and <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/filter/"><code>$filter</code></a>. Consequently, ensure you have digested the <a href="examples/array-manipulations/../../guides/advanced-arrays.html">Advanced Use Of Expressions For Array Processing</a> chapter first, which explains how to use these operators. The pipeline also uses the <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/setUnion/"><code>$setUnion</code></a> operator for finding unique values in an array. The <em>Observations</em> part of this chapter explains this in more detail.</em></p>
</blockquote>
<h2 id="sample-data-population-17"><a class="header" href="#sample-data-population-17">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate the new hourly weather station measurements collection:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-pivot-array-by-key&quot;);
db.dropDatabase();

// Inserts records into the weather_measurements collection
db.weather_measurements.insertMany([
  {
    &quot;weatherStationsZone&quot;: &quot;FieldZone-ABCD&quot;,
    &quot;dayHour&quot;: ISODate(&quot;2021-07-05T15:00:00.000Z&quot;),
    &quot;readings&quot;: [
      {&quot;device&quot;: &quot;ABCD-Device-123&quot;, &quot;tempCelsius&quot;: 18},        
      {&quot;device&quot;: &quot;ABCD-Device-789&quot;, &quot;pressureMBar&quot;: 1004},        
      {&quot;device&quot;: &quot;ABCD-Device-123&quot;, &quot;humidityPercent&quot;: 31},        
      {&quot;device&quot;: &quot;ABCD-Device-123&quot;, &quot;tempCelsius&quot;: 19},        
      {&quot;device&quot;: &quot;ABCD-Device-123&quot;, &quot;pressureMBar&quot;: 1005},        
      {&quot;device&quot;: &quot;ABCD-Device-789&quot;, &quot;humidityPercent&quot;: 31},        
      {&quot;device&quot;: &quot;ABCD-Device-123&quot;, &quot;humidityPercent&quot;: 30},        
      {&quot;device&quot;: &quot;ABCD-Device-789&quot;, &quot;tempCelsius&quot;: 20},        
      {&quot;device&quot;: &quot;ABCD-Device-789&quot;, &quot;pressureMBar&quot;: 1003},        
    ],
  },
  {
    &quot;weatherStationsZone&quot;: &quot;FieldZone-ABCD&quot;,
    &quot;dayHour&quot;: ISODate(&quot;2021-07-05T16:00:00.000Z&quot;),
    &quot;readings&quot;: [
      {&quot;device&quot;: &quot;ABCD-Device-789&quot;, &quot;humidityPercent&quot;: 33},        
      {&quot;device&quot;: &quot;ABCD-Device-123&quot;, &quot;humidityPercent&quot;: 32},        
      {&quot;device&quot;: &quot;ABCD-Device-123&quot;, &quot;tempCelsius&quot;: 22},        
      {&quot;device&quot;: &quot;ABCD-Device-123&quot;, &quot;pressureMBar&quot;: 1007},        
      {&quot;device&quot;: &quot;ABCD-Device-789&quot;, &quot;pressureMBar&quot;: 1008},        
      {&quot;device&quot;: &quot;ABCD-Device-789&quot;, &quot;tempCelsius&quot;: 22},        
      {&quot;device&quot;: &quot;ABCD-Device-789&quot;, &quot;humidityPercent&quot;: 34},        
    ],
  },
]);
</code></pre>
<h2 id="aggregation-pipeline-17"><a class="header" href="#aggregation-pipeline-17">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform the aggregation that groups the weather readings by device. </p>
<pre><code class="language-javascript">var pipeline = [
  // Loop for each unique device, to accumulate an array of devices and their readings
  {&quot;$set&quot;: {
    &quot;readings_device_summary&quot;: {
      &quot;$map&quot;: {
        &quot;input&quot;: {
          &quot;$setUnion&quot;: &quot;$readings.device&quot;  // Get only unique device ids from the array
        },
        &quot;as&quot;: &quot;device&quot;,
        &quot;in&quot;: {
          &quot;$mergeObjects&quot;: {  // Merge array of key:values elements into single object
            &quot;$filter&quot;: {
              &quot;input&quot;: &quot;$readings&quot;,  // Iterate the &quot;readings&quot; array field
              &quot;as&quot;: &quot;reading&quot;,  // Name the current array element &quot;reading&quot;
              &quot;cond&quot;: {  // Only include device properties matching the current device
                &quot;$eq&quot;: [&quot;$$reading.device&quot;, &quot;$$device&quot;]
              }
            }
          }
        }
      }
    },
  }},
  
  // Exclude unrequired fields from each record
  {&quot;$unset&quot;: [
    &quot;_id&quot;,
    &quot;readings&quot;,
  ]},  
];
</code></pre>
<h2 id="execution-17"><a class="header" href="#execution-17">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.weather_measurements.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.weather_measurements.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-17"><a class="header" href="#expected-results-17">Expected Results</a></h2>
<p>Two documents should be returned, with the weather station hourly records containing a new array field of elements representing each device and its measurements, as shown below:</p>
<pre><code class="language-javascript">[
  {
    weatherStationsZone: 'FieldZone-ABCD',
    dayHour: ISODate(&quot;2021-07-05T15:00:00.000Z&quot;),
    readings_device_summary: [
      {
        device: 'ABCD-Device-123',
        tempCelsius: 19,
        humidityPercent: 30,
        pressureMBar: 1005
      },
      {
        device: 'ABCD-Device-789',
        pressureMBar: 1003,
        humidityPercent: 31,
        tempCelsius: 20
      }
    ]
  },
  {
    weatherStationsZone: 'FieldZone-ABCD',
    dayHour: ISODate(&quot;2021-07-05T16:00:00.000Z&quot;),
    readings_device_summary: [
      {
        device: 'ABCD-Device-123',
        humidityPercent: 32,
        tempCelsius: 22,
        pressureMBar: 1007
      },
      {
        device: 'ABCD-Device-789',
        humidityPercent: 34,
        pressureMBar: 1008,
        tempCelsius: 22
      }
    ]
  }
]
</code></pre>
<h2 id="observations-17"><a class="header" href="#observations-17">Observations</a></h2>
<ul>
<li>
<p><strong>Pivoting Items By A Key.</strong> The pipeline does not use the source array field directly to provide the initial list of items for the <code>$map</code> operator to loop through. Instead, it uses the <code>$setUnion</code> operator to capture each unique device name from the array of readings. This approach essentially allows you to group subsets of array items by a key. The array processing and grouping work is self-contained within each document for optimum aggregation performance.</p>
</li>
<li>
<p><strong>Merging Subset Of Array Elements Into One Item.</strong> For each <code>$map</code> iteration, a <code>$filter</code> operator collects the subset of readings from the original array which match the unique device's name. The <code>$mergeObjects</code> operator then takes this subset of readings and turns it into an object, with the measurement type (e.g. <code>temperature</code>) as the key and the measurement (e.g. <code>21℃</code>) as the value. Suppose more than one reading of the same type exists for a device (e.g. <code>temporature=22</code>, <code>temperature=23</code>). In that case, the <code>$mergeObject</code> operator retains the last value only (e.g. <code>23℃</code>), which is the desired behaviour for this example scenario.</p>
</li>
<li>
<p><strong>Potentially Adopt A Better Data Model.</strong> In this example, the weather station hourly data is just persisted directly into the database in the exact structure that the system receives it. However, if it is possible for you to take control of exactly what structure you persist the data in initially, you should take this opportunity. You want to land the data in the database using a model that lends itself to the optimum way for consuming applications to access it. For the Internet of Things (IoT) type use case, where time-series data is collected and then analysed downstream, you should be adopting the <a href="https://www.mongodb.com/blog/post/building-with-patterns-the-bucket-pattern">Bucketing pattern</a>. However, if you are fortunate enough to be using MongoDB version 5.0 or greater, you can instead use MongoDB's <em>time series</em> collection feature. This particular type of collection efficiently stores sequences of measurements over time to improve subsequent query efficiency. It automatically adopts a <em>bucketing pattern</em> internally, meaning that you don't have to design your data model for this explicitly.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="array-sorting--percentiles"><a class="header" href="#array-sorting--percentiles">Array Sorting &amp; Percentiles</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.2</p>
<h2 id="scenario-18"><a class="header" href="#scenario-18">Scenario</a></h2>
<p>You've conducted performance testing of an application with the results of each &quot;test run&quot; captured in a database. Each record contains a set of response times for the test run. You want to analyse the data from multiple runs to identify the slowest ones. You calculate the median (50th percentile) and 90th percentile response times for each test run and only keep results where the 90th percentile response time is greater than 100 milliseconds.</p>
<blockquote>
<p><em>For MongoDB version 5.1 and earlier, the example will use a modified version of an &quot;expression generator&quot; <a href="https://github.com/asya999/bits-n-pieces/blob/master/scripts/sortArray.js">function for inline sorting of arrays</a> created by <a href="https://twitter.com/asya999">Asya Kamsky</a>. Adopting this approach avoids the need for you to use the combination of <code>$unwind</code>, <code>$sort</code>, and <code>$group</code> stages. Instead, you process each document's array in isolation for <a href="examples/array-manipulations/../../guides/performance.html#2-avoid-unwinding--regrouping-documents-just-to-process-array-elements">optimum performance</a>. You can reuse this chapter's custom <code>sortArray()</code> function as-is for your own situations where you need to sort an array field's contents. For MongoDB version 5.2 and greater, you can instead use MongoDB's new <a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/sortArray/"><code>$sortArray</code></a> operator.</em></p>
</blockquote>
<blockquote>
<p><em>MongoDB version 7.0 introduced the <code>$percentile</code> and <code>$median</code> operators. Combining these with the <code>$sortArray</code> operator can significantly simplify the solution. The final observation in this chapter provides a far simpler pipeline for this example.</em></p>
</blockquote>
<h2 id="sample-data-population-18"><a class="header" href="#sample-data-population-18">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate the test run results collection:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-array-sort-percentiles&quot;);
db.dropDatabase();

// Insert 7 records into the performance_test_results collection
db.performance_test_results.insertMany([
  {
    &quot;testRun&quot;: 1,
    &quot;datetime&quot;: ISODate(&quot;2021-08-01T22:51:27.638Z&quot;),
    &quot;responseTimesMillis&quot;: [
      62, 97, 59, 104, 97, 71, 62, 115, 82, 87,
    ],
  },
  {
    &quot;testRun&quot;: 2,
    &quot;datetime&quot;: ISODate(&quot;2021-08-01T22:56:32.272Z&quot;),
    &quot;responseTimesMillis&quot;: [
      34, 63, 51, 104, 87, 63, 64, 86, 105, 51, 73,
      78, 59, 108, 65, 58, 69, 106, 87, 93, 65,
    ],
  },
  {
    &quot;testRun&quot;: 3,
    &quot;datetime&quot;: ISODate(&quot;2021-08-01T23:01:08.908Z&quot;),
    &quot;responseTimesMillis&quot;: [
      56, 72, 83, 95, 107, 83, 85,
    ],
  },
  {
    &quot;testRun&quot;: 4,
    &quot;datetime&quot;: ISODate(&quot;2021-08-01T23:17:33.526Z&quot;),
    &quot;responseTimesMillis&quot;: [
      78, 67, 107, 110,
    ],
  },
  {
    &quot;testRun&quot;: 5,
    &quot;datetime&quot;: ISODate(&quot;2021-08-01T23:24:39.998Z&quot;),
    &quot;responseTimesMillis&quot;: [
      75, 91, 75, 87, 99, 88, 55, 72, 99, 102,
    ],
  },
  {
    &quot;testRun&quot;: 6,
    &quot;datetime&quot;: ISODate(&quot;2021-08-01T23:27:52.272Z&quot;),
    &quot;responseTimesMillis&quot;: [
      88, 89,
    ],
  },
  {
    &quot;testRun&quot;: 7,
    &quot;datetime&quot;: ISODate(&quot;2021-08-01T23:31:59.917Z&quot;),
    &quot;responseTimesMillis&quot;: [
      101,
    ],
  },
]);
</code></pre>
<h2 id="aggregation-pipeline-18"><a class="header" href="#aggregation-pipeline-18">Aggregation Pipeline</a></h2>
<p>If you are using version 5.1 or earlier of MongoDB, you need to define the following custom <code>sortArray()</code> function for inline sorting of the contents of an array field, ready for you to use in a pipeline:</p>
<pre><code class="language-javascript">// Macro function to generate a complex aggregation expression for sorting an array
// This function isn't required for MongoDB version 5.2+ due to the new $sortArray operator
function sortArray(sourceArrayField) {
  return {
    // GENERATE BRAND NEW ARRAY TO CONTAIN THE ELEMENTS FROM SOURCE ARRAY BUT NOW SORTED
    &quot;$reduce&quot;: {
      &quot;input&quot;: sourceArrayField, 
      &quot;initialValue&quot;: [],  // THE FIRST VERSION OF TEMP SORTED ARRAY WILL BE EMPTY
      &quot;in&quot;: {
        &quot;$let&quot;: {
          &quot;vars&quot;: {  // CAPTURE $$this &amp; $$value FROM OUTER $reduce BEFORE OVERRIDDEN
            &quot;resultArray&quot;: &quot;$$value&quot;,
            &quot;currentSourceArrayElement&quot;: &quot;$$this&quot;
          },   
          &quot;in&quot;: {
            &quot;$let&quot;: {
              &quot;vars&quot;: { 
                // FIND EACH SOURCE ARRAY'S CURRENT ELEMENT POSITION IN NEW SORTED ARRAY
                &quot;targetArrayPosition&quot;: {
                  &quot;$reduce&quot;: { 
                    &quot;input&quot;: {&quot;$range&quot;: [0, {&quot;$size&quot;: &quot;$$resultArray&quot;}]},  // &quot;0,1,2..&quot;
                    &quot;initialValue&quot;: {  // INITIALISE SORTED POSITION TO BE LAST ARRAY ELEMENT
                      &quot;$size&quot;: &quot;$$resultArray&quot;
                    },
                    &quot;in&quot;: {  // LOOP THRU &quot;0,1,2...&quot;
                      &quot;$cond&quot;: [ 
                        {&quot;$lt&quot;: [
                          &quot;$$currentSourceArrayElement&quot;,
                          {&quot;$arrayElemAt&quot;: [&quot;$$resultArray&quot;, &quot;$$this&quot;]}
                        ]}, 
                        {&quot;$min&quot;: [&quot;$$value&quot;, &quot;$$this&quot;]},  // ONLY USE IF LOW VAL NOT YET FOUND
                        &quot;$$value&quot;  // RETAIN INITIAL VAL AGAIN AS NOT YET FOUND CORRECT POSTN
                      ]
                    }
                  }
                }
              },
              &quot;in&quot;: {
                // BUILD NEW SORTED ARRAY BY SLICING OLDER ONE &amp; INSERTING NEW ELEMENT BETWEEN
                &quot;$concatArrays&quot;: [
                  {&quot;$cond&quot;: [  // RETAIN THE EXISTING FIRST PART OF THE NEW ARRAY
                    {&quot;$eq&quot;: [0, &quot;$$targetArrayPosition&quot;]}, 
                    [],
                    {&quot;$slice&quot;: [&quot;$$resultArray&quot;, 0, &quot;$$targetArrayPosition&quot;]},
                  ]},
                  [&quot;$$currentSourceArrayElement&quot;],  // PULL IN THE NEW POSITIONED ELEMENT
                  {&quot;$cond&quot;: [  // RETAIN THE EXISTING LAST PART OF THE NEW ARRAY
                    {&quot;$gt&quot;: [{&quot;$size&quot;: &quot;$$resultArray&quot;}, 0]}, 
                    {&quot;$slice&quot;: [
                      &quot;$$resultArray&quot;,
                      &quot;$$targetArrayPosition&quot;,
                      {&quot;$size&quot;: &quot;$$resultArray&quot;}
                    ]},
                    [],
                  ]},
                ]
              } 
            }
          }
        }
      }
    }      
  };
}
</code></pre>
<p>Define the new <code>arrayElemAtPercentile()</code> function for capturing the element of a sorted array at the nth percentile position:</p>
<pre><code class="language-javascript">// Macro function to find nth percentile element of a sorted version of an array
function arrayElemAtPercentile(sourceArrayField, percentile) {
  return {    
    &quot;$let&quot;: {
      &quot;vars&quot;: {
        &quot;sortedArray&quot;: sortArray(sourceArrayField), 
        // Comment out the line above and uncomment the line below if running MDB 5.2 or greater
        // &quot;sortedArray&quot;: {&quot;$sortArray&quot;: {&quot;input&quot;: sourceArrayField, &quot;sortBy&quot;: 1}},        
      },
      &quot;in&quot;: {         
        &quot;$arrayElemAt&quot;: [  // FIND ELEMENT OF ARRAY AT NTH PERCENTILE POSITION
          &quot;$$sortedArray&quot;,
          {&quot;$subtract&quot;: [  // ARRAY IS 0-INDEX BASED SO SUBTRACT 1 TO GET POSITION
            {&quot;$ceil&quot;:  // FIND NTH ELEMENT IN THE ARRAY, ROUNDED UP TO NEAREST integer
              {&quot;$multiply&quot;: [
                {&quot;$divide&quot;: [percentile, 100]},
                {&quot;$size&quot;: &quot;$$sortedArray&quot;}
              ]}
            },
            1
          ]}
        ]
      }
    }
  };
}
</code></pre>
<blockquote>
<p><em>If running MongoDB version 5.2 or greater, you can instead comment/uncomment the specific lines indicated in the code above to leverage MongoDB's new <code>$sortArray</code> operator</em></p>
</blockquote>
<p>Define the pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Capture new fields for the ordered array + various percentiles
  {&quot;$set&quot;: {
    &quot;sortedResponseTimesMillis&quot;: sortArray(&quot;$responseTimesMillis&quot;),
    // Comment out the line above and uncomment the line below if running MDB 5.2 or greater
    // &quot;sortedResponseTimesMillis&quot;: {&quot;$sortArray&quot;: {&quot;input&quot;: &quot;$responseTimesMillis&quot;, &quot;sortBy&quot;: 1}},
    &quot;medianTimeMillis&quot;: arrayElemAtPercentile(&quot;$responseTimesMillis&quot;, 50),
    &quot;ninetiethPercentileTimeMillis&quot;: arrayElemAtPercentile(&quot;$responseTimesMillis&quot;, 90),
  }},

  // Only show results for tests with slow latencies (i.e. 90th%-ile responses &gt;100ms)
  {&quot;$match&quot;: {
    &quot;ninetiethPercentileTimeMillis&quot;: {&quot;$gt&quot;: 100},
  }},

  // Exclude unrequired fields from each record
  {&quot;$unset&quot;: [
    &quot;_id&quot;,
    &quot;datetime&quot;,
    &quot;responseTimesMillis&quot;,
  ]},    
];
</code></pre>
<blockquote>
<p><em>If running MongoDB version 5.2 or greater, you can instead comment/uncomment the specific lines indicated in the code above to leverage MongoDB's new <code>$sortArray</code> operator</em></p>
</blockquote>
<h2 id="execution-18"><a class="header" href="#execution-18">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.performance_test_results.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.performance_test_results.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-18"><a class="header" href="#expected-results-18">Expected Results</a></h2>
<p>Five documents should be returned, representing the subset of documents with a 90th percentile response time greater than 100 milliseconds, as shown below:</p>
<pre><code class="language-javascript">[
  {
    testRun: 1,
    sortedResponseTimesMillis: [
      59, 62, 62,  71,  82,
      87, 97, 97, 104, 115
    ],
    medianTimeMillis: 82,
    ninetiethPercentileTimeMillis: 104
  },
  {
    testRun: 2,
    sortedResponseTimesMillis: [
      34, 51, 51,  58,  59,  63,  63,
      64, 65, 65,  69,  73,  78,  86,
      87, 87, 93, 104, 105, 106, 108
    ],
    medianTimeMillis: 69,
    ninetiethPercentileTimeMillis: 105
  },
  {
    testRun: 3,
    sortedResponseTimesMillis: [
      56, 72,  83, 83,
      85, 95, 107
    ],
    medianTimeMillis: 83,
    ninetiethPercentileTimeMillis: 107
  },
  {
    testRun: 4,
    sortedResponseTimesMillis: [ 67, 78, 107, 110 ],
    medianTimeMillis: 78,
    ninetiethPercentileTimeMillis: 110
  },
  {
    testRun: 7,
    sortedResponseTimesMillis: [ 101 ],
    medianTimeMillis: 101,
    ninetiethPercentileTimeMillis: 101
  }
]
</code></pre>
<h2 id="observations-18"><a class="header" href="#observations-18">Observations</a></h2>
<ul>
<li>
<p><strong>Macro Functions.</strong> In this chapter, you employ two functions, <code>sortArray()</code> and <code>arrayElemAtPercentile()</code>, to generate portions of aggregation <a href="https://en.wikipedia.org/wiki/Boilerplate_code">boilerplate code</a>. These functions are essentially <a href="https://en.wikipedia.org/wiki/Macro_(computer_science)">macros</a>. You invoke these functions from within the pipeline you create in the MongoDB Shell. Each function you invoke embeds the returned boilerplate code into the pipeline's code. You can see this in action by typing the text <code>pipeline</code> into the Shell and pressing <em>enter</em>. Note, you may first have to increase the depth displayed in <em>mongosh</em> by issuing the <em>mongosh</em> command <code>config.set(&quot;inspectDepth&quot;, 100)</code>. This action will display a single large piece of code representing the whole pipeline, including the macro-generated code. The aggregation runtime never sees or runs the custom functions <code>sortArray()</code> and <code>arrayElemAtPercentile()</code> directly. Of course, you won't use JavaScript functions to generate composite expressions if you use a different programming language and <a href="https://docs.mongodb.com/drivers/">MongoDB Driver</a>. You will use the relevant features of your specific programming language to assemble composite expression objects.</p>
</li>
<li>
<p><strong>Sorting On Primitives Only.</strong> The custom <code>sortArray()</code> function used for MongoDB versions 5.1 and earlier will sort arrays containing just primitive values, such as integers, floats, date-times and strings. However, if an array's members are objects (i.e. each has its own fields and values), the code will not sort the array correctly. It is possible to enhance the function to enable sorting an array of objects, but this enhancement is not covered here. For MongoDB versions 5.2 and greater, the new <code>$sortArray</code> operator provides options to easily sort an array of objects.</p>
</li>
<li>
<p><strong>Comparison With Classic Sorting Algorithms.</strong> Despite being more optimal than unwinding and regrouping arrays to bring them back into the same documents, the custom sorting code will be slower than commonly recognised computer science <a href="https://en.wikipedia.org/wiki/Sorting_algorithm">sorting algorithms</a>. This situation is due to the limitations of the aggregation domain language compared with a general-purpose programming language. The performance difference will be negligible for arrays with few elements (probably up to a few tens of members). For larger arrays containing hundreds of members or more, the degradation in performance will be more profound. For MongoDB versions 5.2 and greater, the new <code>$sortArray</code> operator leverages a fully optimised sorting algorithm under the covers to avoid this issue.</p>
</li>
<li>
<p><strong>Simplified Pipeline In MongoDB 7.0.</strong> Combining the <code>$sortArray</code> operator introduced in MongoDB version 5.2 and the <code>$percentile</code> and <code>$median</code> operators introduced in MongoDB version 7.0, you can employ a significantly simpler pipeline for the solution, without requiring any macros, as shown below:</p>
<pre><code class="language-javascript"> var pipeline = [
   {&quot;$set&quot;: {
     &quot;sortedResponseTimesMillis&quot;: {
       &quot;$sortArray&quot;: {
         &quot;input&quot;: &quot;$responseTimesMillis&quot;,
         &quot;sortBy&quot;: 1
       }
     },
     
     &quot;medianTimeMillis&quot;: {
       &quot;$median&quot;: {
         &quot;input&quot;: &quot;$responseTimesMillis&quot;,
         &quot;method&quot;: &quot;approximate&quot;,
       }
     },

     &quot;ninetiethPercentileTimeMillis&quot;: {
       &quot;$first&quot;: {
         &quot;$percentile&quot;: {
           &quot;input&quot;: &quot;$responseTimesMillis&quot;,
           &quot;p&quot;: [0.90],
           &quot;method&quot;: &quot;approximate&quot;,
         }
       }
     }    
   }},

   {&quot;$match&quot;: {
     &quot;ninetiethPercentileTimeMillis&quot;: {&quot;$gt&quot;: 100},
   }},
 ];

</code></pre>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="array-element-grouping"><a class="header" href="#array-element-grouping">Array Element Grouping</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.2</p>
<h2 id="scenario-19"><a class="header" href="#scenario-19">Scenario</a></h2>
<p>You want to provide a report for your online game showing the total &quot;coin&quot; rewards each gaming user has accumulated. The challenge is that the source collection captures each time the game awards a user with a type of coin in a growing array field containing many elements. However,  for each gamer, you want to show totals for each coin type in an array instead. An extra complication exists in that you don't know ahead of time what all the possible coin types can be when developing the solution. For example, the game could introduce different coin types in the future (e.g. &quot;tungsten coins&quot;).</p>
<h2 id="sample-data-population-19"><a class="header" href="#sample-data-population-19">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate the user rewards collection:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-array-element-grouping&quot;);
db.dropDatabase();

// Insert 3 records into the user_rewards collection
db.user_rewards.insertMany([
  {
    &quot;userId&quot;: 123456789,
    &quot;rewards&quot;: [
      {&quot;coin&quot;: &quot;gold&quot;, &quot;amount&quot;: 25, &quot;date&quot;: ISODate(&quot;2022-11-01T09:25:23Z&quot;)},
      {&quot;coin&quot;: &quot;bronze&quot;, &quot;amount&quot;: 100, &quot;date&quot;: ISODate(&quot;2022-11-02T11:32:56Z&quot;)},
      {&quot;coin&quot;: &quot;silver&quot;, &quot;amount&quot;: 50, &quot;date&quot;: ISODate(&quot;2022-11-09T12:11:58Z&quot;)},
      {&quot;coin&quot;: &quot;gold&quot;, &quot;amount&quot;: 10, &quot;date&quot;: ISODate(&quot;2022-11-15T12:46:40Z&quot;)},
      {&quot;coin&quot;: &quot;bronze&quot;, &quot;amount&quot;: 75, &quot;date&quot;: ISODate(&quot;2022-11-22T12:57:01Z&quot;)},
      {&quot;coin&quot;: &quot;gold&quot;, &quot;amount&quot;: 50, &quot;date&quot;: ISODate(&quot;2022-11-28T19:32:33Z&quot;)},
    ],
  },
  {
    &quot;userId&quot;: 987654321,
    &quot;rewards&quot;: [
      {&quot;coin&quot;: &quot;bronze&quot;, &quot;amount&quot;: 200, &quot;date&quot;: ISODate(&quot;2022-11-21T14:35:56Z&quot;)},
      {&quot;coin&quot;: &quot;silver&quot;, &quot;amount&quot;: 50, &quot;date&quot;: ISODate(&quot;2022-11-21T15:02:48Z&quot;)},
      {&quot;coin&quot;: &quot;silver&quot;, &quot;amount&quot;: 50, &quot;date&quot;: ISODate(&quot;2022-11-27T23:04:32Z&quot;)},
      {&quot;coin&quot;: &quot;silver&quot;, &quot;amount&quot;: 50, &quot;date&quot;: ISODate(&quot;2022-11-27T23:29:47Z&quot;)},
      {&quot;coin&quot;: &quot;bronze&quot;, &quot;amount&quot;: 500, &quot;date&quot;: ISODate(&quot;2022-11-27T23:56:14Z&quot;)},
    ],
  },
  {
    &quot;userId&quot;: 888888888,
    &quot;rewards&quot;: [
      {&quot;coin&quot;: &quot;gold&quot;, &quot;amount&quot;: 500, &quot;date&quot;: ISODate(&quot;2022-11-13T13:42:18Z&quot;)},
      {&quot;coin&quot;: &quot;platinum&quot;, &quot;amount&quot;: 5, &quot;date&quot;: ISODate(&quot;2022-11-19T15:02:53Z&quot;)},
    ],
  },
]);
</code></pre>
<h2 id="aggregation-pipeline-19"><a class="header" href="#aggregation-pipeline-19">Aggregation Pipeline</a></h2>
<p>You first need to define the following two array element grouping functions ready for you to use in a pipeline (one to perform group &quot;counting&quot; and the other to perform group &quot;summing&quot;):</p>
<pre><code class="language-javascript">// Macro function to generate a complex expression to group an array field's
// content by the value of a field occurring in each array element, counting
// the number of times it occurs
function arrayGroupByCount(arraySubdocField, groupByKeyField) {
  return {
    &quot;$map&quot;: {
      &quot;input&quot;: {
        &quot;$setUnion&quot;: {
          &quot;$map&quot;: {
            &quot;input&quot;: `$${arraySubdocField}`,
            &quot;in&quot;: `$$this.${groupByKeyField}`
          }
        }
      },
      &quot;as&quot;: &quot;key&quot;,
      &quot;in&quot;: {
        &quot;id&quot;: &quot;$$key&quot;,
        &quot;count&quot;: {
          &quot;$size&quot;: {
            &quot;$filter&quot;: {
              &quot;input&quot;: `$${arraySubdocField}`,
              &quot;cond&quot;: {
                &quot;$eq&quot;: [`$$this.${groupByKeyField}`, &quot;$$key&quot;]
              }
            }
          }
        }
      }
    }
  };
}

// Macro function to generate a complex expression to group an array field's
// content by the value of a field occurring in each array element, summing
// the values from a corresponding amount field in each array element
function arrayGroupBySum(arraySubdocField, groupByKeyField, groupByValueField) {
  return {
    &quot;$map&quot;: {
      &quot;input&quot;: {
        &quot;$setUnion&quot;: {
          &quot;$map&quot;: {
            &quot;input&quot;: `$${arraySubdocField}`,
            &quot;in&quot;: `$$this.${groupByKeyField}`
          }
        }
      },
      &quot;as&quot;: &quot;key&quot;,
      &quot;in&quot;: {
        &quot;id&quot;: &quot;$$key&quot;,
        &quot;total&quot;: {
          &quot;$reduce&quot;: {
            &quot;input&quot;: `$${arraySubdocField}`,
            &quot;initialValue&quot;: 0,
            &quot;in&quot;: {
              &quot;$cond&quot;: { 
                &quot;if&quot;: {&quot;$eq&quot;: [`$$this.${groupByKeyField}`, &quot;$$key&quot;]},
                &quot;then&quot;: {&quot;$add&quot;: [`$$this.${groupByValueField}`, &quot;$$value&quot;]},  
                &quot;else&quot;: &quot;$$value&quot;  
              }            
            }            
          }
        }
      }
    }
  };
}
</code></pre>
<p>Define the pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Capture new fields grouping elements of each array and remove unwanted fields
  {&quot;$set&quot;: {
    &quot;coinTypeAwardedCounts&quot;: arrayGroupByCount(&quot;rewards&quot;, &quot;coin&quot;),
    &quot;coinTypeTotals&quot;: arrayGroupBySum(&quot;rewards&quot;, &quot;coin&quot;, &quot;amount&quot;),
    &quot;_id&quot;: &quot;$$REMOVE&quot;,
    &quot;rewards&quot;: &quot;$$REMOVE&quot;,
  }},
];
</code></pre>
<h2 id="execution-19"><a class="header" href="#execution-19">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.user_rewards.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.user_rewards.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-19"><a class="header" href="#expected-results-19">Expected Results</a></h2>
<p>Three documents should be returned, representing the three gamers and showing the number of times they received each coin type and its total, as shown below:</p>
<pre><code class="language-javascript">[
  {
    userId: 123456789,
    coinTypeAwardedCounts: [ 
      { id: 'bronze', count: 2 },
      { id: 'silver', count: 1 },
      { id: 'gold', count: 3 }
    ],
    coinTypeTotals: [
      { id: 'bronze', total: 175 },
      { id: 'silver', total: 50 },
      { id: 'gold', total: 85 }
    ]
  },
  {
    userId: 987654321,
    coinTypeAwardedCounts: [
      { id: 'bronze', count: 2 },
      { id: 'silver', count: 3 }
    ],
    coinTypeTotals: [
      { id: 'bronze', total: 700 },
      { id: 'silver', total: 150 }
    ]
  },
  {
    userId: 888888888,
    coinTypeAwardedCounts: [
      { id: 'gold', count: 1 },
      { id: 'platinum', count: 1 }
    ],
    coinTypeTotals: [
      { id: 'gold', total: 500 },
      { id: 'platinum', total: 5 }
    ]
  }
]
</code></pre>
<h2 id="observations-19"><a class="header" href="#observations-19">Observations</a></h2>
<ul>
<li>
<p><strong>Reusable Macro Functions.</strong> As with the <a href="examples/array-manipulations/array-sort-percentiles.html">previous example</a>, the aggregation uses macro functions to generate boilerplate code, which it inlines into the pipeline before the aggregation runtime executes it. In this example, both the <code>arrayGroupByCount()</code> and <code>arrayGroupBySum()</code> macro functions are general-purpose and reusable, which you can employ as-is for any other scenario where array elements need to be grouped and totalled.</p>
</li>
<li>
<p><strong>Grouping Array Elements Without Unwinding First.</strong> The <code>$group</code> stage is the standard tool for grouping elements and producing counts and totals for these groups. However, as discussed in the
<a href="examples/array-manipulations/../../guides/performance.html#2-avoid-unwinding--regrouping-documents-just-to-process-array-elements">optimising for performance</a> chapter, if you only need to manipulate each document's array field in isolation from other documents, this is inefficient. You must unwind a document's array, process the unpacked data and then regroup each array back into the same parent document. By regrouping, you are introducing a blocking and resource-limited step. This example's two macro functions enable you to avoid this overhead and achieve the array grouping you require, even when the keys you are grouping by are unknown to you ahead of time. The <code>$setUnion</code> operator used in both functions produces the set of unique keys to group by.</p>
</li>
<li>
<p><strong>Variable Reference And <code>$$</code> Potential Confusion.</strong> You may recall in the <a href="examples/array-manipulations/../../guides/expressions.html">Expressions Explained chapter</a> that for aggregation expressions, field paths begin with <code>$</code> (e.g. <code>$rewards</code>) and variables begin with <code>$$</code> (e.g. <code>$$currentItem</code>). Therefore you may be confused by the syntax <code>`$${arraySubdocField}`</code> used in both functions. This confusion is understandable. Employing <code>`</code> backticks is part of the <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals">Template Literals</a> feature of JavaScript. Therefore, before the pipeline executes, the JavaScript interpreter replaces <code>${arraySubdocField}</code> with the string <code>rewards</code>, which is the value of the <code>arraySubdocField</code> parameter passed to the function. So <code>`$${arraySubdocField}`</code> becomes the field path <code>&quot;$rewards&quot;</code> before the macro function embeds it into the larger complex expression it is constructing.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="array-fields-joining"><a class="header" href="#array-fields-joining">Array Fields Joining</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.2</p>
<h2 id="scenario-20"><a class="header" href="#scenario-20">Scenario</a></h2>
<p>You are developing a new dating website using a database to hold the profiles of all registered users. For each user profile, you will persist a set of the user's specified hobbies, each with a description of how the user says they conduct their pursuit. Each user's profile also captures what they prefer to do depending on their mood (e.g., &quot;happy&quot;, &quot;sad&quot;, &quot;chilling&quot;, etc.). When you show the user profiles on the website to a person searching for a date, you want to describe how each candidate user conducts their hobbies for each mood to help the person spot their ideal match.</p>
<h2 id="sample-data-population-20"><a class="header" href="#sample-data-population-20">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate a new user profiles collection:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-array-fields-joining&quot;);
db.dropDatabase();

// Insert 2 records into the users collection
db.users.insertMany([
  {
    &quot;firstName&quot;: &quot;Alice&quot;,
    &quot;lastName&quot;: &quot;Jones&quot;,
    &quot;dateOfBirth&quot;: ISODate(&quot;1985-07-21T00:00:00Z&quot;),
    &quot;hobbies&quot;: {
      &quot;music&quot;: &quot;Playing the guitar&quot;,
      &quot;reading&quot;: &quot;Science Fiction books&quot;,
      &quot;gaming&quot;: &quot;Video games, especially RPGs&quot;,
      &quot;sports&quot;: &quot;Long-distance running&quot;,
      &quot;traveling&quot;: &quot;Visiting exotic places&quot;,
      &quot;cooking&quot;: &quot;Trying out new recipes&quot;,
    },      
    &quot;moodFavourites&quot;: {
      &quot;sad&quot;: [&quot;music&quot;],
      &quot;happy&quot;: [&quot;sports&quot;],
      &quot;chilling&quot;: [&quot;music&quot;, &quot;cooking&quot;],
    },
  },
  {
    &quot;firstName&quot;: &quot;Sam&quot;,
    &quot;lastName&quot;: &quot;Brown&quot;,
    &quot;dateOfBirth&quot;: ISODate(&quot;1993-12-01T00:00:00Z&quot;),
    &quot;hobbies&quot;: {
      &quot;cycling&quot;: &quot;Mountain biking&quot;,
      &quot;writing&quot;: &quot;Poetry and short stories&quot;,
      &quot;knitting&quot;: &quot;Knitting scarves and hats&quot;,
      &quot;hiking&quot;: &quot;Hiking in the mountains&quot;,
      &quot;volunteering&quot;: &quot;Helping at the local animal shelter&quot;,
      &quot;music&quot;: &quot;Listening to Jazz&quot;,
      &quot;photography&quot;: &quot;Nature photography&quot;,
      &quot;gardening&quot;: &quot;Growing herbs and vegetables&quot;,
      &quot;yoga&quot;: &quot;Practicing Hatha Yoga&quot;,
      &quot;cinema&quot;: &quot;Watching classic movies&quot;,
    },
    &quot;moodFavourites&quot;: {
      &quot;happy&quot;: [&quot;gardening&quot;, &quot;cycling&quot;],
      &quot;sad&quot;: [&quot;knitting&quot;],
    },
  },
]);
</code></pre>
<h2 id="aggregation-pipeline-20"><a class="header" href="#aggregation-pipeline-20">Aggregation Pipeline</a></h2>
<p>You first need to define the following function to get the array values of named fields in a sub-document where each field's name is only known at runtime:</p>
<pre><code class="language-javascript">// Macro function to generate a complex expression to get the array values of
// named fields in a sub-document where each field's name is only known at runtime 
function getValuesOfNamedFieldsAsArray(obj, fieldnames) {
  return {
    &quot;$map&quot;: { 
      &quot;input&quot;: {
        &quot;$filter&quot;: { 
          &quot;input&quot;: {&quot;$objectToArray&quot;: obj}, 
          &quot;as&quot;: &quot;currElem&quot;,
          &quot;cond&quot;: {&quot;$in&quot;: [&quot;$$currElem.k&quot;, fieldnames]},
        }
      }, 
      &quot;in&quot;: &quot;$$this.v&quot; 
    }, 
  };
}
</code></pre>
<p>Define the pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Set a field with activities each user likes doing according to their mood
  {&quot;$set&quot;: {
    &quot;moodActivities&quot;: {      
      &quot;$arrayToObject&quot;: {
        &quot;$map&quot;: { 
          &quot;input&quot;: {&quot;$objectToArray&quot;: &quot;$moodFavourites&quot;},
          &quot;in&quot;: {              
            &quot;k&quot;: &quot;$$this.k&quot;,
            &quot;v&quot;: getValuesOfNamedFieldsAsArray(&quot;$hobbies&quot;, &quot;$$this.v&quot;),
          }
        }, 
      }
    }
  }},

  // Remove unwanted fields  
  {&quot;$unset&quot;: [
    &quot;_id&quot;,
    &quot;hobbies&quot;,
    &quot;moodFavourites&quot;,
  ]},  
]
</code></pre>
<h2 id="execution-20"><a class="header" href="#execution-20">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.users.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.users.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-20"><a class="header" href="#expected-results-20">Expected Results</a></h2>
<p>Two documents should be returned, each showing a new <code>moodActivities</code> array field containing descriptions of how a user conducts their preferred hobby for each mood, as shown below:</p>
<pre><code class="language-javascript">[
  {
    firstName: 'Alice',
    lastName: 'Jones',
    dateOfBirth: ISODate(&quot;1985-07-21T00:00:00.000Z&quot;),
    moodActivities: {
      sad: [ 'Playing the guitar' ],
      happy: [ 'Long-distance running' ],
      chilling: [ 'Playing the guitar', 'Trying out new recipes' ]
    }
  },
  {
    firstName: 'Sam',
    lastName: 'Brown',
    dateOfBirth: ISODate(&quot;1993-12-01T00:00:00.000Z&quot;),
    moodActivities: {
      happy: [ 'Mountain biking', 'Growing herbs and vegetables' ],
      sad: [ 'Knitting scarves and hats' ]
    }
  }
]
</code></pre>
<h2 id="observations-20"><a class="header" href="#observations-20">Observations</a></h2>
<ul>
<li>
<p><strong>Joining Between Two Fields In Each Record.</strong> Each user document contains two sub-document fields the pipeline must join: <code>hobbies</code> and <code>moodFavourites</code>. The <code>moodFavourites</code> sub-document properties hold arrays with values mapped to properties of the <code>hobbies</code> sub-document, and consequently, there is a many-to-many relationship between the two fields. A user's given hobby can appear as a favourite for more than one of their moods, and each user's mood can have multiple preferred hobbies. The <code>getValuesOfNamedFieldsAsArray()</code> function lets the pipeline look up multiple hobbies in one go for each 'mood' that it iterates through.</p>
</li>
<li>
<p><strong>Reusable Macro Functions.</strong> As with many of the other Array Manipulation Examples, the aggregation uses a macro function to generate boilerplate code for use in the pipeline. This is a general-purpose function and reusable as-is in other solutions.</p>
</li>
<li>
<p><strong>Grouping Array Elements Without Unwinding First.</strong> As with the <a href="examples/array-manipulations/array-element-grouping.html">previous example</a>, the aggregation avoids unnecessarily unwinding each document's two arrays to group back together again, just to manipulate each document's array fields in isolation from other documents. It avoids introducing a blocking and resource-limited grouping step in the pipeline.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="comparison-of-two-arrays"><a class="header" href="#comparison-of-two-arrays">Comparison Of Two Arrays</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.4    <em>(due to use of <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/first-array-element/"><code>$first</code></a> array operator)</em></p>
<h2 id="scenario-21"><a class="header" href="#scenario-21">Scenario</a></h2>
<p>You are an IT administrator managing some virtual machine deployments in a data centre to host a critical business application in a few environments (e.g. &quot;Production&quot;, &quot;QA&quot;). A database collection captured the configuration state of each virtual machine across two days. You want to generate a report showing what configuration changes people made to the virtual machines (if any) between these two days.</p>
<h2 id="sample-data-population-21"><a class="header" href="#sample-data-population-21">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate the deployments collection:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-comparison-of-two-arrays&quot;);
db.dropDatabase();

// Insert 5 records into the deployments collection
db.deployments.insertMany([
  {
    &quot;name&quot;: &quot;ProdServer&quot;,
    &quot;beforeTimestamp&quot;: ISODate(&quot;2022-01-01T00:00:00Z&quot;),
    &quot;afterTimestamp&quot;: ISODate(&quot;2022-01-02T00:00:00Z&quot;),
    &quot;beforeConfig&quot;: {
      &quot;vcpus&quot;: 8,
      &quot;ram&quot;: 128,
      &quot;storage&quot;: 512,
      &quot;state&quot;: &quot;running&quot;,
    },
    &quot;afterConfig&quot;: {
      &quot;vcpus&quot;: 16,
      &quot;ram&quot;: 256,
      &quot;storage&quot;: 512,
      &quot;state&quot;: &quot;running&quot;,
    },    
  },
  {
    &quot;name&quot;: &quot;QAServer&quot;,
    &quot;beforeTimestamp&quot;: ISODate(&quot;2022-01-01T00:00:00Z&quot;),
    &quot;afterTimestamp&quot;: ISODate(&quot;2022-01-02T00:00:00Z&quot;),
    &quot;beforeConfig&quot;: {
      &quot;vcpus&quot;: 4,
      &quot;ram&quot;: 64,
      &quot;storage&quot;: 512,
      &quot;state&quot;: &quot;paused&quot;,
    },
    &quot;afterConfig&quot;: {
      &quot;vcpus&quot;: 4,
      &quot;ram&quot;: 64,
      &quot;storage&quot;: 256,
      &quot;state&quot;: &quot;running&quot;,
      &quot;extraParams&quot;: &quot;disableTLS;disableCerts;&quot;
    },    
  },
  {
    &quot;name&quot;: &quot;LoadTestServer&quot;,
    &quot;beforeTimestamp&quot;: ISODate(&quot;2022-01-01T00:00:00Z&quot;),
    &quot;beforeConfig&quot;: {
      &quot;vcpus&quot;: 8,
      &quot;ram&quot;: 128,
      &quot;storage&quot;: 256,
      &quot;state&quot;: &quot;running&quot;,
    },
  },
  {
    &quot;name&quot;: &quot;IntegrationServer&quot;,
    &quot;beforeTimestamp&quot;: ISODate(&quot;2022-01-01T00:00:00Z&quot;),
    &quot;afterTimestamp&quot;: ISODate(&quot;2022-01-02T00:00:00Z&quot;),
    &quot;beforeConfig&quot;: {
      &quot;vcpus&quot;: 4,
      &quot;ram&quot;: 32,
      &quot;storage&quot;: 64,
      &quot;state&quot;: &quot;running&quot;,
    },
    &quot;afterConfig&quot;: {
      &quot;vcpus&quot;: 4,
      &quot;ram&quot;: 32,
      &quot;storage&quot;: 64,
      &quot;state&quot;: &quot;running&quot;,
    },
  },
  {
    &quot;name&quot;: &quot;DevServer&quot;,
    &quot;afterTimestamp&quot;: ISODate(&quot;2022-01-02T00:00:00Z&quot;),
    &quot;afterConfig&quot;: {
      &quot;vcpus&quot;: 2,
      &quot;ram&quot;: 16,
      &quot;storage&quot;: 64,
      &quot;state&quot;: &quot;running&quot;,
    },
  },
]);
</code></pre>
<h2 id="aggregation-pipeline-21"><a class="header" href="#aggregation-pipeline-21">Aggregation Pipeline</a></h2>
<p>You first need to define the following two functions, one to get all the unique keys from two different arrays, the other to get the value of a field only known at runtime, ready for you to use in a pipeline:</p>
<pre><code class="language-javascript">// Macro function to generate a complex expression to get all the unique keys
// from two sub-documents returned as an array of the unique keys
function getArrayOfTwoSubdocsKeysNoDups(firstArrayRef, secondArrayRef) {
  return {
    &quot;$setUnion&quot;: {
      &quot;$concatArrays&quot;: [
        {&quot;$map&quot;: {
          &quot;input&quot;: {&quot;$objectToArray&quot;: firstArrayRef},
          &quot;in&quot;: &quot;$$this.k&quot;,
        }},
        {&quot;$map&quot;: {
          &quot;input&quot;: {&quot;$objectToArray&quot;: secondArrayRef},
          &quot;in&quot;: &quot;$$this.k&quot;,
        }},
      ]
    }
  };
}

// Macro function to generate a complex expression to get the value of a field
// in a document where the field's name is only known at runtime 
function getDynamicField(obj, fieldname) {
  return {
    &quot;$first&quot;: [ 
      {&quot;$map&quot;: { 
        &quot;input&quot;: {
          &quot;$filter&quot;: { 
            &quot;input&quot;: {&quot;$objectToArray&quot;: obj}, 
            &quot;as&quot;: &quot;currObj&quot;,
            &quot;cond&quot;: {&quot;$eq&quot;: [&quot;$$currObj.k&quot;, fieldname]},
            &quot;limit&quot;: 1
          }
        }, 
        &quot;in&quot;: &quot;$$this.v&quot; 
      }}, 
    ]
  };
}
</code></pre>
<p>Define the pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Compare two different arrays in the same document &amp; get the differences (if any)
  {&quot;$set&quot;: {
    &quot;differences&quot;: {
      &quot;$reduce&quot;: {
        &quot;input&quot;: getArrayOfTwoSubdocsKeysNoDups(&quot;$beforeConfig&quot;, &quot;$afterConfig&quot;),
        &quot;initialValue&quot;: [],
        &quot;in&quot;: {
          &quot;$concatArrays&quot;: [
            &quot;$$value&quot;,
            {&quot;$cond&quot;: {
              &quot;if&quot;: {
                &quot;$ne&quot;: [
                  getDynamicField(&quot;$beforeConfig&quot;, &quot;$$this&quot;),
                  getDynamicField(&quot;$afterConfig&quot;, &quot;$$this&quot;),
                ]
              },
              &quot;then&quot;: [{
                &quot;field&quot;: &quot;$$this&quot;,
                &quot;change&quot;: {
                  &quot;$concat&quot;: [
                    {&quot;$ifNull&quot;: [{&quot;$toString&quot;: getDynamicField(&quot;$beforeConfig&quot;, &quot;$$this&quot;)}, &quot;&lt;not-set&gt;&quot;]},
                    &quot; --&gt; &quot;,
                    {&quot;$ifNull&quot;: [{&quot;$toString&quot;: getDynamicField(&quot;$afterConfig&quot;, &quot;$$this&quot;)}, &quot;&lt;not-set&gt;&quot;]},
                  ]
                },
              }],
              &quot;else&quot;: [],
            }}
          ]
        }
      }
    },
  }},

  // Add 'status' field and only show 'differences' field if there are differences
  {&quot;$set&quot;: {
    // Set 'status' to ADDED, REMOVED, MODIFIED or UNCHANGED accordingly
    &quot;status&quot;: {
      &quot;$switch&quot;: {        
        &quot;branches&quot;: [
          {
            &quot;case&quot;: {
              &quot;$and&quot;: [
                {&quot;$in&quot;: [{&quot;$type&quot;: &quot;$differences&quot;}, [&quot;missing&quot;, &quot;null&quot;]]},
                {&quot;$in&quot;: [{&quot;$type&quot;: &quot;$beforeConfig&quot;}, [&quot;missing&quot;, &quot;null&quot;]]},
              ]
            },
            &quot;then&quot;: &quot;ADDED&quot;
          },
          {
            &quot;case&quot;: {
              &quot;$and&quot;: [
                {&quot;$in&quot;: [{&quot;$type&quot;: &quot;$differences&quot;}, [&quot;missing&quot;, &quot;null&quot;]]},
                {&quot;$in&quot;: [{&quot;$type&quot;: &quot;$afterConfig&quot;}, [&quot;missing&quot;, &quot;null&quot;]]},
              ]
            },
            &quot;then&quot;: &quot;REMOVED&quot;
          },
          {&quot;case&quot;: {&quot;$lte&quot;: [{&quot;$size&quot;: &quot;$differences&quot;}, 0]}, &quot;then&quot;: &quot;UNCHANGED&quot;},
          {&quot;case&quot;: {&quot;$gt&quot;:  [{&quot;$size&quot;: &quot;$differences&quot;}, 0]}, &quot;then&quot;: &quot;MODIFIED&quot;},
        ],
        &quot;default&quot;: &quot;UNKNOWN&quot;,
      }
    },

    // If there are differences, keep the differences field, otherwise remove it
    &quot;differences&quot;: {
      &quot;$cond&quot;: [
        {&quot;$or&quot;: [
          {&quot;$in&quot;: [{&quot;$type&quot;: &quot;$differences&quot;}, [&quot;missing&quot;, &quot;null&quot;]]},
          {&quot;$lte&quot;: [{&quot;$size&quot;: &quot;$differences&quot;}, 0]},
        ]},
        &quot;$$REMOVE&quot;, 
        &quot;$differences&quot;
      ]
    },
  }},         

  // Remove unwanted fields
  {&quot;$unset&quot;: [
    &quot;_id&quot;,
    &quot;beforeTimestamp&quot;,
    &quot;afterTimestamp&quot;,
    &quot;beforeConfig&quot;,
    &quot;afterConfig&quot;,
  ]},
];   
</code></pre>
<h2 id="execution-21"><a class="header" href="#execution-21">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.deployments.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.deployments.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-21"><a class="header" href="#expected-results-21">Expected Results</a></h2>
<p>Five documents should be returned, showing whether anyone added, removed or modified a deployment or left it unchanged, with the deployment's changes shown if modified, as shown below:</p>
<pre><code class="language-javascript">[
  {
    &quot;name&quot;: &quot;ProdServer&quot;,
    &quot;status&quot;: &quot;MODIFIED&quot;,
    &quot;differences&quot;: [
      {
        &quot;field&quot;: &quot;vcpus&quot;,
        &quot;change&quot;: &quot;8 --&gt; 16&quot;
      },
      {
        &quot;field&quot;: &quot;ram&quot;,
        &quot;change&quot;: &quot;128 --&gt; 256&quot;
      }
    ]
  },
  {
    &quot;name&quot;: &quot;QAServer&quot;,
    &quot;status&quot;: &quot;MODIFIED&quot;,
    &quot;differences&quot;: [
      {
        &quot;field&quot;: &quot;storage&quot;,
        &quot;change&quot;: &quot;512 --&gt; 256&quot;
      },
      {
        &quot;field&quot;: &quot;state&quot;,
        &quot;change&quot;: &quot;paused --&gt; running&quot;
      },
      {
        &quot;field&quot;: &quot;extraParams&quot;,
        &quot;change&quot;: &quot;&lt;not-set&gt; --&gt; disableTLS;disableCerts;&quot;
      }
    ]
  },
  {
    &quot;name&quot;: &quot;LoadTestServer&quot;,
    &quot;status&quot;: &quot;REMOVED&quot;
  },
  {
    &quot;name&quot;: &quot;IntegrationServer&quot;,
    &quot;status&quot;: &quot;UNCHANGED&quot;
  },
  {
    &quot;name&quot;: &quot;DevServer&quot;,
    &quot;status&quot;: &quot;ADDED&quot;
  }
]
</code></pre>
<h2 id="observations-21"><a class="header" href="#observations-21">Observations</a></h2>
<ul>
<li>
<p><strong>Reusable Macro Functions.</strong> As with many of the other <em>Array Manipulation Examples</em>, the aggregation uses macro functions to generate boilerplate code for use in the pipeline. These functions are general-purpose and reusable as-is in other solutions.</p>
</li>
<li>
<p><strong>Sub-Document Comparison.</strong> The pipeline provides a generic way to compare the topmost fields hanging off two sub-document fields. The comparison will only work for sub-document fields with primitive values (e.g. String, Double, Null, Date, Boolean, etc.). The comparison will not work if a sub-document's field is an Array or Object. The pipeline finds all the field names ('keys') appearing in either sub-document. For each field name, the pipeline then compares if it exists in both sub-documents, and if the values don't match, it incorporates the two different values in the output.</p>
</li>
<li>
<p><strong>Potential Need For Earlier Stages.</strong> The example source documents already embed two fields to compare, each corresponding to the deployment's configuration captured at a different point in time (<code>beforeTimestamp</code> and <code>afterTimestamp</code>). In real-world data models, these two configuration snapshots would be more likely to correspond to two different records in a collection, not one combined record. However, it doesn't mean that this example is redundant. In such cases, you would include the following additional stages at the start of the example's pipeline: </p>
<ul>
<li><code>$sort</code> to sort all records by timestamp regardless of which deployment each corresponds to.</li>
<li><code>$group</code> to group on the name of the deployment. Inside this group stage, you would use a <code>$first</code> operator to capture the first document's <code>config</code> into a new <code>beforeConfig</code> field and a <code>$last</code> operator to capture the last document's <code>config</code> into a new <code>afterConfig</code> field.</li>
</ul>
<p>The rest of the pipeline from the example would then be used unchanged.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="full-text-search-examples"><a class="header" href="#full-text-search-examples">Full-Text Search Examples</a></h1>
<p>This section provides examples for building an aggregation pipeline to perform a full-text search on one or more fields of documents in a collection.</p>
<p>The examples leverage Lucene-based search indexes delivered by <a href="https://www.mongodb.com/docs/atlas/atlas-search/atlas-search-overview/">Atlas Search</a>. Consequently, using these aggregations is only possible with an <a href="https://www.mongodb.com/atlas/database">Atlas Cluster</a> rather than a self-installed MongoDB deployment.</p>
<p> </p>
<hr />
<p> </p>
<p>Atlas Search makes adding fast relevance-based full-text search to your applications easy. Atlas Search deploys an Apache Lucene index next to your database, automatically handling data and schema synchronisation between the source collection and this new search index. Atlas Search provides fuzzy matching; auto-complete; fast facets and counts; highlighting; relevance scoring; geospatial queries; and synonyms, all backed by support for multiple language analysers. You reduce your cognitive load by invoking searches via the regular MongoDB drivers and Aggregations API rather than a 3rd party Lucene API.</p>
<p>With the Atlas platform's integration of database, search engine, and synchronisation pipeline into a single, unified, and fully managed service, you can quickly build search features into your applications. Consequently, this ease of use and its performance and functionality advantages mean you should use Atlas Search over MongoDB's <code>$text</code> and <code>$regex</code> query operators when running your database in Atlas.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="compound-text-search-criteria"><a class="header" href="#compound-text-search-criteria">Compound Text Search Criteria</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.2</p>
<h2 id="scenario-22"><a class="header" href="#scenario-22">Scenario</a></h2>
<p>You want to search a collection of e-commerce products to find specific movie DVDs. Based on each DVD's full-text plot description, you want movies with a <em>post-apocalyptic</em> theme, especially those related to a <em>nuclear</em> disaster where some people <em>survive</em>. However, you aren't interested in seeing movies involving <em>zombies</em>.</p>
<blockquote>
<p><em>To execute this example, you need to be using an Atlas Cluster rather than a self-managed MongoDB deployment. The simplest way to achieve this is to <a href="https://www.mongodb.com/cloud/atlas">provision a Free Tier Atlas Cluster</a>.</em></p>
</blockquote>
<h2 id="sample-data-population-22"><a class="header" href="#sample-data-population-22">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate a new <em>products</em> collection with some <em>DVD</em> and <em>Book</em> records:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-compound-text-search&quot;);
db.products.remove({});

// Insert 7 records into the products collection
db.products.insertMany([
  {
    &quot;name&quot;: &quot;The Road&quot;,
    &quot;category&quot;: &quot;DVD&quot;,
    &quot;description&quot;: &quot;In a dangerous post-apocalyptic world, a dying father protects his surviving son as they try to reach the coast&quot;,
  },
  {
    &quot;name&quot;: &quot;The Day Of The Triffids&quot;,
    &quot;category&quot;: &quot;BOOK&quot;,
    &quot;description&quot;: &quot;Post-apocalyptic disaster where most people are blinded by a meteor shower and then die at the hands of a new type of plant&quot;,
  },
  {
    &quot;name&quot;: &quot;The Road&quot;,
    &quot;category&quot;: &quot;BOOK&quot;,
    &quot;description&quot;: &quot;In a dangerous post-apocalyptic world, a dying father protects his surviving son as they try to reach the coast&quot;,
  },  
  {
    &quot;name&quot;: &quot;The Day the Earth Caught Fire&quot;,
    &quot;category&quot;: &quot;DVD&quot;,
    &quot;description&quot;: &quot;A series of nuclear explosions cause fires and earthquakes to ravage cities, with some of those that survive trying to rescue the post-apocalyptic world&quot;,
  },
  {
    &quot;name&quot;: &quot;28 Days Later&quot;,
    &quot;category&quot;: &quot;DVD&quot;,
    &quot;description&quot;: &quot;A caged chimp infected with a virus is freed from a lab, and the infection spreads to people who become zombie-like with just a few surviving in a post-apocalyptic country&quot;,
  },  
  {
    &quot;name&quot;: &quot;Don't Look Up&quot;,
    &quot;category&quot;: &quot;DVD&quot;,
    &quot;description&quot;: &quot;Pre-apocalyptic situation where some astronomers warn humankind of an approaching comet that will destroy planet Earth&quot;,
  },
  {
    &quot;name&quot;: &quot;Thirteen Days&quot;,
    &quot;category&quot;: &quot;DVD&quot;,
    &quot;description&quot;: &quot;Based on the true story of the Cuban nuclear misile threat, crisis is averted at the last minute and the workd survives&quot;,
  },
]); 
</code></pre>
<p> </p>
<p>Now, using the simple procedure described in the <a href="examples/full-text-search/../../appendices/create-search-index.html">Create Atlas Search Index</a> appendix, define a <strong>Search Index</strong>. Select the new database collection <strong>book-compound-text-search.products</strong> and enter the following JSON search index definition:</p>
<pre><code class="language-javascript">{
  &quot;searchAnalyzer&quot;: &quot;lucene.english&quot;,
  &quot;mappings&quot;: {
    &quot;dynamic&quot;: true
  }
}
</code></pre>
<blockquote>
<p><em>This definition indicates that the index should use the <em>lucene-english</em> analyzer and include all document fields to be searchable with their inferred data types.</em></p>
</blockquote>
<h2 id="aggregation-pipeline-22"><a class="header" href="#aggregation-pipeline-22">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // Search for DVDs where the description must contain &quot;apocalyptic&quot; but not &quot;zombie&quot;
  {&quot;$search&quot;: {
    &quot;index&quot;: &quot;default&quot;,    
    &quot;compound&quot;: {
      &quot;must&quot;: [
        {&quot;text&quot;: {
          &quot;path&quot;: &quot;description&quot;,
          &quot;query&quot;: &quot;apocalyptic&quot;,
        }},
      ],
      &quot;should&quot;: [
        {&quot;text&quot;: {
          &quot;path&quot;: &quot;description&quot;,
          &quot;query&quot;: &quot;nuclear survives&quot;,
        }},
      ],
      &quot;mustNot&quot;: [
        {&quot;text&quot;: {
          &quot;path&quot;: &quot;description&quot;,
          &quot;query&quot;: &quot;zombie&quot;,
        }},
      ],
      &quot;filter&quot;: [
        {&quot;text&quot;: {
          &quot;path&quot;: &quot;category&quot;,
          &quot;query&quot;: &quot;DVD&quot;,
        }},      
      ],
    }
  }},

  // Capture the search relevancy score in the output and omit the _id field
  {&quot;$set&quot;: {
    &quot;score&quot;: {&quot;$meta&quot;: &quot;searchScore&quot;},
    &quot;_id&quot;: &quot;$$REMOVE&quot;,
  }},
];
</code></pre>
<h2 id="execution-22"><a class="header" href="#execution-22">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline and also view its explain plan:</p>
<pre><code class="language-javascript">db.products.aggregate(pipeline);
</code></pre>
<pre><code class="language-javascript">db.products.explain(&quot;executionStats&quot;).aggregate(pipeline);
</code></pre>
<h2 id="expected-results-22"><a class="header" href="#expected-results-22">Expected Results</a></h2>
<p>Three documents should be returned, showing products which are post-apocalyptic themed DVDs, as shown below:</p>
<pre><code class="language-javascript">[
  {
    name: 'The Day the Earth Caught Fire',
    category: 'DVD',
    description: 'A series of nuclear explosions cause fires and earthquakes to ravage cities, with some of those that survive trying to rescue the post-apocalyptic world',
    score: 0.8468831181526184
  },
  {
    name: 'The Road',
    category: 'DVD',
    description: 'In a dangerous post-apocalyptic world, a dying father protects his surviving son as they try to reach the coast',
    score: 0.3709350824356079
  },
  {
    name: &quot;Don't Look Up&quot;,
    category: 'DVD',
    description: 'Pre-apocalyptic situation where some astronomers warn humankind of an approaching comet that will destroy planet Earth',
    score: 0.09836573898792267
  }
]
</code></pre>
<p>If you don't see any results, double-check that the system has finished generating your new index.</p>
<h2 id="observations-22"><a class="header" href="#observations-22">Observations</a></h2>
<ul>
<li>
<p><strong>Search Stage.</strong> The <a href="https://www.mongodb.com/docs/atlas/atlas-search/query-syntax/"><code>$search</code></a> stage is only available in aggregation pipelines run against an Atlas-based MongoDB database which leverages <a href="https://www.mongodb.com/docs/atlas/atlas-search/">Atlas Search</a>. A <code>$search</code> stage must be the first stage of an aggregation pipeline, and <a href="https://www.mongodb.com/docs/atlas/atlas-search/atlas-search-overview/#fts-architecture">under the covers</a>, it instructs the system to execute a text search operation against an internally synchronised <em>Lucene</em> full-text index. Inside the <code>$search</code> stage, you can only use one of a small set of <a href="https://www.mongodb.com/docs/atlas/atlas-search/operators-and-collectors/">text-search specific pipeline operators</a>. In this example, the pipeline uses a <a href="https://www.mongodb.com/docs/atlas/atlas-search/compound/"><code>$compound</code></a> operator to define a combination of multiple <a href="https://www.mongodb.com/docs/atlas/atlas-search/text/#std-label-text-ref">$text</a> text-search operators.</p>
</li>
<li>
<p><strong>Results &amp; Relevancy Explanation.</strong> The executed pipeline ignores four of the seven input documents and sorts the remaining three documents by highest relevancy first. It achieves this by applying the following actions:</p>
<ul>
<li>It excludes two <em>book-related</em> records because the <code>filter</code> option executes a <code>$text</code> match on just <code>DVD</code> in the <em>category</em> field.</li>
<li>It ignores the &quot;28 Days Later&quot; DVD record because the <code>mustNot</code> option's <code>$text</code> matches &quot;zombie&quot; in the <em>description</em> field.</li>
<li>It excludes the movie &quot;Thirteen Days&quot; because even though its description contains two of the optional terms (&quot;nuclear&quot; and &quot;survives&quot;), it doesn't include the mandatory term &quot;apocalyptic&quot;.</li>
<li>It deduces the score of the remaining records based on the ratio of the number of matching terms (&quot;apocalyptic&quot;, &quot;nuclear&quot;, and &quot;survives&quot;) in each document's <code>description</code> field versus how infrequently those terms appear in other documents in the same collection.</li>
</ul>
</li>
<li>
<p><strong>English Language Analyzer.</strong> Atlas Search provides <a href="https://www.mongodb.com/docs/atlas/atlas-search/analyzers/">multiple <em>Analyzer</em> options</a> for breaking down generated text indexes and executing text queries into searchable tokens. The default analyzer, <em>Standard</em>, is not used here because the pipeline needs to match variations of the same English words. For example, &quot;survives&quot; and &quot;surviving&quot; need to refer to the same term, and hence the text index uses the <em>lucene.english</em> analyzer.</p>
</li>
<li>
<p><strong>Meta Operator.</strong> The <a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/meta/"><code>$meta</code></a> operator provides supplementary metadata about the results of a text search performed earlier in a pipeline. When leveraging an Atlas Search based text search, the pipeline can look up a <code>searchScore</code> field in the metadata to access the relevancy score attributed to each text search result. This example uses <code>searchScore</code> to help you understand why the results are in a particular order, with some records having higher relevancy than others. In this example, it serves no other purpose, and you can omit it. However, in a different situation, you might want to use the search score to filter out low relevancy results in a later <code>$match</code> stage of a pipeline, for example.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="facets-and-counts-text-search"><a class="header" href="#facets-and-counts-text-search">Facets And Counts Text Search</a></h1>
<p><strong>Minimum MongoDB Version:</strong> 4.4    <em>(due to use of the <a href="https://www.mongodb.com/docs/atlas/atlas-search/facet/"><code>facet</code></a> option in the <a href="https://www.mongodb.com/docs/atlas/atlas-search/query-syntax/#mongodb-pipeline-pipe.-searchMeta"><code>$searchMeta</code></a> stage)</em></p>
<h2 id="scenario-23"><a class="header" href="#scenario-23">Scenario</a></h2>
<p>You help run a bank's call centre and want to analyse the summary descriptions of customer telephone enquiries recorded by call centre staff. You want to look for customer calls that mention <em>fraud</em> and understand what periods of a specific day these fraud-related calls occur. This insight will help the bank plan its future staffing rotas for the fraud department.</p>
<blockquote>
<p><em>To execute this example, you need to be using an Atlas Cluster rather than a self-managed MongoDB deployment. The simplest way to achieve this is to <a href="https://www.mongodb.com/cloud/atlas">provision a Free Tier Atlas Cluster</a>.</em></p>
</blockquote>
<h2 id="sample-data-population-23"><a class="header" href="#sample-data-population-23">Sample Data Population</a></h2>
<p>Drop any old version of the database (if it exists) and then populate a new <em>enquiries</em> collection with new records:</p>
<pre><code class="language-javascript">db = db.getSiblingDB(&quot;book-facets-text-search&quot;);
db.enquiries.remove({});

// Insert records into the enquiries collection
db.enquiries.insertMany([
  {
    &quot;acountId&quot;: &quot;9913183&quot;,
    &quot;datetime&quot;: ISODate(&quot;2022-01-30T08:35:52Z&quot;),
    &quot;summary&quot;: &quot;They just made a balance enquiry only - no other issues&quot;,
  },
  {
    &quot;acountId&quot;: &quot;9913183&quot;,
    &quot;datetime&quot;: ISODate(&quot;2022-01-30T09:32:07Z&quot;),
    &quot;summary&quot;: &quot;Reported suspected fraud - froze cards, initiated chargeback on the transaction&quot;,
  },
  {
    &quot;acountId&quot;: &quot;6830859&quot;,
    &quot;datetime&quot;: ISODate(&quot;2022-01-30T10:25:37Z&quot;),
    &quot;summary&quot;: &quot;Customer said they didn't make one of the transactions which could be fraud - passed on to the investigations team&quot;,
  },
  {
    &quot;acountId&quot;: &quot;9899216&quot;,
    &quot;datetime&quot;: ISODate(&quot;2022-01-30T11:13:32Z&quot;),
    &quot;summary&quot;: &quot;Struggling financially this month hence requiring extended overdraft - increased limit to 500 for 2 monts&quot;,
  },  
  {
    &quot;acountId&quot;: &quot;1766583&quot;,
    &quot;datetime&quot;: ISODate(&quot;2022-01-30T10:56:53Z&quot;),
    &quot;summary&quot;: &quot;Fraud reported - fradulent direct debit established 3 months ago - removed instruction and reported to crime team&quot;,
  },
  {
    &quot;acountId&quot;: &quot;9310399&quot;,
    &quot;datetime&quot;: ISODate(&quot;2022-01-30T14:04:48Z&quot;),
    &quot;summary&quot;: &quot;Customer rang on mobile whilst fraud call in progress on home phone to check if it was valid - advised to hang up&quot;,
  },
  {
    &quot;acountId&quot;: &quot;4542001&quot;,
    &quot;datetime&quot;: ISODate(&quot;2022-01-30T16:55:46Z&quot;),
    &quot;summary&quot;: &quot;Enquiring for loan - approved standard loan for 6000 over 4 years&quot;,
  },
  {
    &quot;acountId&quot;: &quot;7387756&quot;,
    &quot;datetime&quot;: ISODate(&quot;2022-01-30T17:49:32Z&quot;),
    &quot;summary&quot;: &quot;Froze customer account when they called in as multiple fraud transactions appearing even whilst call was active&quot;,
  },
  {
    &quot;acountId&quot;: &quot;3987992&quot;,
    &quot;datetime&quot;: ISODate(&quot;2022-01-30T22:49:44Z&quot;),
    &quot;summary&quot;: &quot;Customer called claiming fraud for a transaction which confirmed looks suspicious and so issued chargeback&quot;,
  },
  {
    &quot;acountId&quot;: &quot;7362872&quot;,
    &quot;datetime&quot;: ISODate(&quot;2022-01-31T07:07:14Z&quot;),
    &quot;summary&quot;: &quot;Worst case of fraud I've ever seen - customer lost millions - escalated to our high value team&quot;,
  },
]);
</code></pre>
<p> </p>
<p>Now, using the simple procedure described in the <a href="examples/full-text-search/../../appendices/create-search-index.html">Create Atlas Search Index</a> appendix, define a <strong>Search Index</strong>. Select the new database collection <strong>book-facets-text-search.enquiries</strong> and enter the following JSON search index definition:</p>
<pre><code class="language-javascript">{
  &quot;analyzer&quot;: &quot;lucene.english&quot;,
  &quot;searchAnalyzer&quot;: &quot;lucene.english&quot;,
  &quot;mappings&quot;: {
    &quot;dynamic&quot;: true,
    &quot;fields&quot;: {
      &quot;datetime&quot;: [
        {&quot;type&quot;: &quot;date&quot;},
        {&quot;type&quot;: &quot;dateFacet&quot;}
      ]
    }
  }
}
</code></pre>
<blockquote>
<p><em>This definition indicates that the index should use the <em>lucene-english</em> analyzer. It includes an explicit mapping for the <code>datetime</code> field to ask for the field to be indexed in two ways to simultaneously support a date range filter and faceting from the same pipeline. The mapping indicates that all other document fields will be searchable with inferred data types.</em></p>
</blockquote>
<h2 id="aggregation-pipeline-23"><a class="header" href="#aggregation-pipeline-23">Aggregation Pipeline</a></h2>
<p>Define a pipeline ready to perform the aggregation:</p>
<pre><code class="language-javascript">var pipeline = [
  // For 1 day match 'fraud' enquiries, grouped into periods of the day, counting them
  {&quot;$searchMeta&quot;: {
    &quot;index&quot;: &quot;default&quot;,    
    &quot;facet&quot;: {
      &quot;operator&quot;: {
        &quot;compound&quot;: {
          &quot;must&quot;: [
            {&quot;text&quot;: {
              &quot;path&quot;: &quot;summary&quot;,
              &quot;query&quot;: &quot;fraud&quot;,
            }},
          ],
          &quot;filter&quot;: [
            {&quot;range&quot;: {
              &quot;path&quot;: &quot;datetime&quot;,
              &quot;gte&quot;: ISODate(&quot;2022-01-30&quot;),
              &quot;lt&quot;: ISODate(&quot;2022-01-31&quot;),
            }},
          ],
        },
      },
      &quot;facets&quot;: {        
        &quot;fraudEnquiryPeriods&quot;: {
          &quot;type&quot;: &quot;date&quot;,
          &quot;path&quot;: &quot;datetime&quot;,
          &quot;boundaries&quot;: [
            ISODate(&quot;2022-01-30T00:00:00.000Z&quot;),
            ISODate(&quot;2022-01-30T06:00:00.000Z&quot;),
            ISODate(&quot;2022-01-30T12:00:00.000Z&quot;),
            ISODate(&quot;2022-01-30T18:00:00.000Z&quot;),
            ISODate(&quot;2022-01-31T00:00:00.000Z&quot;),
          ],
        }            
      }        
    }           
  }},
];
</code></pre>
<h2 id="execution-23"><a class="header" href="#execution-23">Execution</a></h2>
<p>Execute the aggregation using the defined pipeline:</p>
<pre><code class="language-javascript">db.enquiries.aggregate(pipeline);
</code></pre>
<p>Note, it is not currently possible to view the explain plan for a <code>$searchMeta</code> based aggregation.</p>
<h2 id="expected-results-23"><a class="header" href="#expected-results-23">Expected Results</a></h2>
<p>The results should show the pipeline matched 6 documents for a specific day on the text <code>fraud</code>, spread out over the four 6-hour periods, as shown below:</p>
<pre><code class="language-javascript">[
  {
    count: { lowerBound: Long(&quot;6&quot;) },
    facet: {
      fraudEnquiryPeriods: {
        buckets: [
          {
            _id: ISODate(&quot;2022-01-30T00:00:00.000Z&quot;),
            count: Long(&quot;0&quot;)
          },
          {
            _id: ISODate(&quot;2022-01-30T06:00:00.000Z&quot;),
            count: Long(&quot;3&quot;)
          },
          {
            _id: ISODate(&quot;2022-01-30T12:00:00.000Z&quot;),
            count: Long(&quot;2&quot;)
          },
          {
            _id: ISODate(&quot;2022-01-30T18:00:00.000Z&quot;),
            count: Long(&quot;1&quot;)
          }
        ]
      }
    }
  }
]
</code></pre>
<p>If you don't see any facet results and the value of <code>count</code> is zero, double-check that the system has finished generating your new index.</p>
<h2 id="observations-23"><a class="header" href="#observations-23">Observations</a></h2>
<ul>
<li>
<p><strong>Search Metadata Stage.</strong> The <a href="https://www.mongodb.com/docs/atlas/atlas-search/query-syntax/"><code>$searchMeta</code></a> stage is only available in aggregation pipelines run against an Atlas-based MongoDB database which leverages <a href="https://www.mongodb.com/docs/atlas/atlas-search/">Atlas Search</a>. A <code>$searchMeta</code> stage must be the first stage of an aggregation pipeline, and <a href="https://www.mongodb.com/docs/atlas/atlas-search/atlas-search-overview/#fts-architecture">under the covers</a>, it performs a text search operation against an internally synchronised Lucene full-text index. However, it is different from the <code>$search</code> operator used in the <a href="examples/full-text-search/compound-text-search.html">earlier search example chapter</a>. Instead, you use <code>$searchMeta</code> to ask the system to return metadata about the text search you executed, such as the match count, rather than returning the search result records. The <code>$searchMeta</code> stage takes a <code>facet</code> option, which takes two options, <code>operator</code> and <code>facet</code>, which you use to define the text search criteria and categorise the results in groups.</p>
</li>
<li>
<p><strong>Date Range Filter.</strong> The pipeline uses a <a href="https://www.mongodb.com/docs/atlas/atlas-search/text/"><code>$text</code></a> operator for matching descriptions containing the term <em>fraud</em>. Additionally, the search criteria include a <a href="https://www.mongodb.com/docs/atlas/atlas-search/return-stored-source/"><code>$range</code></a> operator. The <code>$range</code> operator allows you to match records between two numbers or two dates. The example pipeline applies a date range, only including documents where each <code>datetime</code> field's value is <em>30-January-2022</em>. </p>
</li>
<li>
<p><strong>Facet Boundaries.</strong> The pipeline uses a <code>facet</code> <em>collector</em> to group metadata results by date range boundaries. Each boundary in the example defines a 6-hour period of the same specific day for a document's <code>datetime</code> field. A single pipeline can declare multiple facets; hence you give each facet a different name. The pipeline only defines one facet in this example, labelling it <em>fraudEnquiryPeriods</em>. When the pipeline executes, it returns the total count of matched documents and the count of matches in each facet grouping. There were no <em>fraud-related</em> enquiries between midnight and 6am, indicating that perhaps the fraud department only requires &quot;skeleton-staffing&quot; for such periods. In contrast, the period between 6am and midday shows the highest number of fraud-related enquiries, suggesting the bank dedicates additional staff to those periods.</p>
</li>
<li>
<p><strong>Faster Facet Counts.</strong> A faceted index is a special type of Lucene index optimised to compute counts of dataset categories. An application can leverage the index to offload much of the work required to analyse facets ahead of time, thus avoiding some of the latency costs when invoking a <a href="https://en.wikipedia.org/wiki/Faceted_search">faceted search</a> at runtime. Therefore use the Atlas faceted search capability if you are in a position to adopt <a href="https://www.mongodb.com/docs/atlas/atlas-search/">Atlas Search</a>, rather than using MongoDB's general-purpose faceted search capability described in an <a href="examples/full-text-search/../trend-analysis/faceted-classifications.html">earlier example in this book</a>.</p>
</li>
<li>
<p><strong>Combining A Search Operation With Metadata.</strong> In this example, a pipeline uses <code>$searchMeta</code> to obtain metadata from a search (counts and facets). What if you also want the actual search results from running <code>$search</code> similar to the <a href="examples/full-text-search/./compound-text-search.html">previous example</a>? You could invoke two operations from your client application, one to retrieve the search results and one to retrieve the metadata results. However, Atlas Search provides a way of obtaining both aspects within a single aggregation. Instead of using a <code>$searchMeta</code> stage, you use a <code>$search</code> stage. The pipeline <a href="https://www.mongodb.com/docs/atlas/atlas-search/facet/#search_meta-aggregation-variable">automatically stores its metadata</a> in the <code>$$SEARCH_META</code> variable, ready for you to access it via subsequent stages in the same pipeline. For example: </p>
<pre><code class="language-javascript">{&quot;$set&quot;: {&quot;mymetadata&quot;: &quot;$$SEARCH_META&quot;}}
</code></pre>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="appendices"><a class="header" href="#appendices">Appendices</a></h1>
<p>The following sections contain reference material you may find useful on your MongoDB Aggregations journey.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="stages-cheatsheet"><a class="header" href="#stages-cheatsheet">Stages Cheatsheet</a></h1>
<p>A simple example for each <a href="https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/">stage in the MongoDB Aggregation Framework</a>. </p>
<h4 id="stages"><a class="header" href="#stages">Stages:</a></h4>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left">Query</th><th style="text-align: left">Mutate</th><th style="text-align: left">Summarise/Itemise</th><th style="text-align: left">Join</th><th style="text-align: left">Input/Output</th></tr></thead><tbody>
<tr><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_geoNear"><code>$geoNear</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_addFields"><code>$addFields</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_bucket"><code>$bucket</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_graphLookup"><code>$graphLookup</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_documents"><code>$documents</code></a></td></tr>
<tr><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_limit"><code>$limit</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_densify"><code>$densify</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_bucketAuto"><code>$bucketAuto</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_lookup"><code>$lookup</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_merge"><code>$merge</code></a></td></tr>
<tr><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_match"><code>$match</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_fill"><code>$fill</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_count"><code>$count</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_unionWith"><code>$unionWith</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_out"><code>$out</code></a></td></tr>
<tr><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_sample"><code>$sample</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_project"><code>$project</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_facet"><code>$facet</code></a></td><td style="text-align: left"></td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_search"><code>$search</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_redact"><code>$redact</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_group"><code>$group</code></a></td><td style="text-align: left"></td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_searchmeta"><code>$searchMeta</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_replaceRoot"><code>$replaceRoot</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_sortByCount"><code>$sortByCount</code></a></td><td style="text-align: left"></td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_skip"><code>$skip</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_replaceWith"><code>$replaceWith</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_unwind"><code>$unwind</code></a></td><td style="text-align: left"></td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_sort"><code>$sort</code></a></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_set"><code>$set</code></a></td><td style="text-align: left"></td><td style="text-align: left"></td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_setWindowFields"><code>$setWindowFields</code></a></td><td style="text-align: left"></td><td style="text-align: left"></td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"></td><td style="text-align: left"><a href="appendices/cheatsheet.html#stage_unset"><code>$unset</code></a></td><td style="text-align: left"></td><td style="text-align: left"></td><td style="text-align: left"></td></tr>
</tbody></table>
</div>
<blockquote>
<p><em>The following stages are not included because they are unrelated to aggregating business data:  <code>$changeStream</code>, <code>$changeStreamSplitLargeEvent</code>, <code>$collStats</code>, <code>$currentOp</code>, <code>$indexStats</code>, <code>$listLocalSessions</code>, <code>$listSearchIndexes</code>, <code>$listSessions</code>, <code>$planCacheStats</code></em></p>
</blockquote>
<h4 id="input-collections"><a class="header" href="#input-collections">Input Collections:</a></h4>
<pre><code class="language-javascript">// shapes
{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}

// lists
{_id: &quot;▤&quot;, a: &quot;●&quot;, b: [&quot;◰&quot;, &quot;◱&quot;]}
{_id: &quot;▥&quot;, a: &quot;▲&quot;, b: [&quot;◲&quot;]}
{_id: &quot;▦&quot;, a: &quot;▲&quot;, b: [&quot;◰&quot;, &quot;◳&quot;, &quot;◱&quot;]}
{_id: &quot;▧&quot;, a: &quot;●&quot;, b: [&quot;◰&quot;]}
{_id: &quot;▨&quot;, a: &quot;■&quot;, b: [&quot;◳&quot;, &quot;◱&quot;]}

// places
{_id: &quot;Bigtown&quot;, loc: {type: &quot;Point&quot;, coordinates: [1,1]}}
{_id: &quot;Smalltown&quot;, loc: {type: &quot;Point&quot;, coordinates: [3,3]}}
{_id: &quot;Happytown&quot;, loc: {type: &quot;Point&quot;, coordinates: [5,5]}}
{_id: &quot;Sadtown&quot;, loc: {type: &quot;LineString&quot;, coordinates: [[7,7],[8,8]]}}
</code></pre>
<p> </p>
<hr />
<p><a name="stage_addFields"></a></p>
<h2 id="addfields"><a class="header" href="#addfields"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/addFields/"><code>$addFields</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$addFields: {z: &quot;●&quot;}      
   ⬇︎      
{_id: '◐', x: '■', y: '▲', val: 10, ord: 0, z: '●'}
{_id: '◑', x: '■', y: '■', val: 60, z: '●'}
{_id: '◒', x: '●', y: '■', val: 80, z: '●'}
{_id: '◓', x: '▲', y: '▲', val: 85, z: '●'}
{_id: '◔', x: '■', y: '▲', val: 90, z: '●'}
{_id: '◕', x: '●', y: '■', val: 95 ord: 100, z: '●'}
</code></pre>
<p> </p>
<hr />
<p><a name="stage_bucket"></a></p>
<h2 id="bucket"><a class="header" href="#bucket"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/bucket/"><code>$bucket</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$bucket: {
  groupBy: &quot;$val&quot;,
  boundaries: [0, 25, 50, 75, 100],
  default: &quot;Other&quot;
}
   ⬇︎      
{_id: 0, count: 1}
{_id: 50, count: 1}
{_id: 75, count: 4}    
</code></pre>
<p> </p>
<hr />
<p><a name="stage_bucketAuto"></a></p>
<h2 id="bucketauto"><a class="header" href="#bucketauto"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/bucketAuto/"><code>$bucketAuto</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$bucketAuto: {groupBy: &quot;$val&quot;, buckets: 3}
   ⬇︎      
{_id: {min: 10, max: 80}, count: 2}
{_id: {min: 80, max: 90}, count: 2}
{_id: {min: 90, max: 95}, count: 2}   
</code></pre>
<p> </p>
<hr />
<p><a name="stage_count"></a></p>
<h2 id="count"><a class="header" href="#count"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/count/"><code>$count</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$count: &quot;amount&quot;
   ⬇︎      
{amount: 6}    
</code></pre>
<p> </p>
<hr />
<p><a name="stage_densify"></a></p>
<h2 id="densify"><a class="header" href="#densify"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/densify/"><code>$densify</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$densify: {
  field: &quot;val&quot;,
  partitionByFields: [&quot;x&quot;],
  range: {bounds: &quot;full&quot;, step: 25}
}
   ⬇︎      
{_id: '◐', x: '■', y: '▲', val: 10, ord: 0}
{x: '■', val: 35}
{_id: '◑', x: '■', y: '■', val: 60}
{x: '●', val: 10}
{x: '●', val: 35}
{x: '●', val: 60}
{_id: '◒', x: '●', y: '■', val: 80}
{x: '▲', val: 10}
{x: '▲', val: 35}
{x: '▲', val: 60}
{_id: '◓', x: '▲', y: '▲', val: 85}
{x: '■', val: 85}
{_id: '◔', x: '■', y: '▲', val: 90}
{x: '●', val: 85}
{_id: '◕', x: '●', y: '■', val: 95, ord: 100}
</code></pre>
<p> </p>
<hr />
<p><a name="stage_documents"></a></p>
<h2 id="documents"><a class="header" href="#documents"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/documents/"><code>$documents</code></a></a></h2>
<pre><code class="language-javascript">[     ]
   ⬇︎      
$documents: {
  {p: &quot;▭&quot;, q: &quot;▯&quot;},
  {p: &quot;▯&quot;, q: &quot;▭&quot;}
}
   ⬇︎      
{p: '▭', q: '▯'}
{p: '▯', q: '▭'}
</code></pre>
<p> </p>
<hr />
<p><a name="stage_facet"></a></p>
<h2 id="facet"><a class="header" href="#facet"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/facet/"><code>$facet</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$facet: {
  X_CIRCLE_FACET: [{$match: {x: &quot;●&quot;}}],
  FIRST_TWO_FACET: [{$limit: 2}]
}
   ⬇︎      
{
  X_CIRCLE_FACET: [
    {_id: '◒', x: '●', y: '■', val: 80},
    {_id: '◕', x: '●', y: '■', val: 95, ord: 100}
  ],
  FIRST_TWO_FACET: [
    {_id: '◐', x: '■', y: '▲', val: 10, ord: 0},
    {_id: '◑', x: '■', y: '■', val: 60}
  ]
}   
</code></pre>
<p> </p>
<hr />
<p><a name="stage_fill"></a></p>
<h2 id="fill"><a class="header" href="#fill"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/fill/"><code>$fill</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$fill: {
  sortBy: {val: 1},        
  output: {
    ord: {method: &quot;linear&quot;}
  }
}
   ⬇︎      
{_id: '◐', x: '■', y: '▲', val: 10, ord: 0}
{_id: '◑', x: '■', y: '■', val: 60, ord: 58.82352941176471}
{_id: '◒', x: '●', y: '■', val: 80, ord: 82.3529411764706}
{_id: '◓', x: '▲', y: '▲', val: 85, ord: 88.23529411764706}
{_id: '◔', x: '■', y: '▲', val: 90, ord: 94.11764705882354}
{_id: '◕', x: '●', y: '■', val: 95, ord: 100}
</code></pre>
<p> </p>
<hr />
<p><a name="stage_geoNear"></a></p>
<h2 id="geonear"><a class="header" href="#geonear"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/geoNear/"><code>$geoNear</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;Bigtown&quot;, loc: {type: &quot;Point&quot;, coordinates: [1,1]}}
{_id: &quot;Smalltown&quot;, loc: {type: &quot;Point&quot;, coordinates: [3,3]}}
{_id: &quot;Happytown&quot;, loc: {type: &quot;Point&quot;, coordinates: [5,5]}}
{_id: &quot;Sadtown&quot;, loc: {type: &quot;LineString&quot;, coordinates: [[7,7],[8,8]]}}
   ⬇︎      
$geoNear: {
  near: {type: &quot;Point&quot;, coordinates: [9,9]}, 
  distanceField: &quot;distance&quot;
}
   ⬇︎      
{_id: 'Sadtown', loc: { type: 'LineString', coordinates: [[7,7], [8,8]]}
      distance: 156565.32902203742}
{_id: 'Happytown', loc: { type: 'Point', coordinates: [5,5]}
      distance: 627304.9320885336}
{_id: 'Smalltown', loc: { type: 'Point', coordinates: [3,3]}
      distance: 941764.4675092621}
{_id: 'Bigtown', loc: { type: 'Point', coordinates: [1,1]}
      distance: 1256510.3666236876}   
</code></pre>
<p> </p>
<hr />
<p><a name="stage_graphLookup"></a></p>
<h2 id="graphlookup"><a class="header" href="#graphlookup"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/graphLookup/"><code>$graphLookup</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$graphLookup: {
  from: &quot;shapes&quot;,
  startWith: &quot;$x&quot;,
  connectFromField: &quot;x&quot;,
  connectToField: &quot;y&quot;,
  depthField: &quot;depth&quot;,
  as: &quot;connections&quot;
}
$project: {connections_count: {$size: &quot;$connections&quot;}}
   ⬇︎      
{_id: '◐', connections_count: 3}
{_id: '◑', connections_count: 3}
{_id: '◒', connections_count: 0}
{_id: '◓', connections_count: 6}
{_id: '◔', connections_count: 3}
{_id: '◕', connections_count: 0}
</code></pre>
<p> </p>
<hr />
<p><a name="stage_group"></a></p>
<h2 id="group"><a class="header" href="#group"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/group/"><code>$group</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$group: {_id: &quot;$x&quot;, ylist: {$push: &quot;$y&quot;}}
   ⬇︎   
{_id: '●', ylist: ['■', '■']}
{_id: '■', ylist: ['▲', '■', '▲']}
{_id: '▲', ylist: ['▲']}      
</code></pre>
<p> </p>
<hr />
<p><a name="stage_limit"></a></p>
<h2 id="limit"><a class="header" href="#limit"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/limit/"><code>$limit</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$limit: 2
   ⬇︎      
{_id: '◐', x: '■', y: '▲', val: 10, ord: 0}
{_id: '◑', x: '■', y: '■', val: 60}   
</code></pre>
<p> </p>
<hr />
<p><a name="stage_lookup"></a></p>
<h2 id="lookup"><a class="header" href="#lookup"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/lookup/"><code>$lookup</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ✚
{_id: &quot;▤&quot;, a: &quot;●&quot;, b: [&quot;◰&quot;, &quot;◱&quot;]}
{_id: &quot;▥&quot;, a: &quot;▲&quot;, b: [&quot;◲&quot;]}
{_id: &quot;▦&quot;, a: &quot;▲&quot;, b: [&quot;◰&quot;, &quot;◳&quot;, &quot;◱&quot;]}
{_id: &quot;▧&quot;, a: &quot;●&quot;, b: [&quot;◰&quot;]}
{_id: &quot;▨&quot;, a: &quot;■&quot;, b: [&quot;◳&quot;, &quot;◱&quot;]}
   ⬇︎      
$lookup: {
  from: &quot;lists&quot;,
  localField: &quot;y&quot;,
  foreignField: &quot;a&quot;,
  as: &quot;refs&quot;
}
   ⬇︎         
{_id: '◐', x: '■', y: '▲', val: 10, ord: 0, refs: [
  {_id: '▥', a: '▲', b: ['◲']},
  {_id: '▦', a: '▲', b: ['◰', '◳', '◱']}
]}
{_id: '◑', x: '■', y: '■', val: 60, refs: [
  {_id: '▨', a: '■', b: ['◳', '◱']}
]}
{_id: '◒', x: '●', y: '■', val: 80, refs: [
  {_id: '▨', a: '■', b: ['◳', '◱']}
]}
{_id: '◓', x: '▲', y: '▲', val: 85, refs: [
  {_id: '▥', a: '▲', b: ['◲']},
  {_id: '▦', a: '▲', b: ['◰', '◳', '◱']}
]}
{_id: '◔', x: '■', y: '▲', val: 90, refs: [
  {_id: '▥', a: '▲', b: ['◲']},
  {_id: '▦', a: '▲', b: ['◰', '◳', '◱']}
]}
{_id: '◕', x: '●', y: '■', val: 95, ord: 100, refs: [
  {_id: '▨', a: '■', b: ['◳', '◱']}
]}   
</code></pre>
<p> </p>
<hr />
<p><a name="stage_match"></a></p>
<h2 id="match"><a class="header" href="#match"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/match/"><code>$match</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$match: {y: &quot;▲&quot;}  
   ⬇︎      
{_id: '◐', x: '■', y: '▲', val: 10, ord: 0}
{_id: '◓', x: '▲', y: '▲', val: 85}
{_id: '◔', x: '■', y: '▲', val: 90}   
</code></pre>
<p> </p>
<hr />
<p><a name="stage_merge"></a></p>
<h2 id="merge"><a class="header" href="#merge"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/merge/"><code>$merge</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$merge: {into: &quot;results&quot;}
   ⬇︎      
db.results.find()   
{_id: '◐', x: '■', y: '▲', val: 10, ord: 0}
{_id: '◑', x: '■', y: '■', val: 60}
{_id: '◒', x: '●', y: '■', val: 80}
{_id: '◓', x: '▲', y: '▲', val: 85}
{_id: '◔', x: '■', y: '▲', val: 90}
{_id: '◕', x: '●', y: '■', val: 95, ord: 100}   
</code></pre>
<p> </p>
<hr />
<p><a name="stage_out"></a></p>
<h2 id="out"><a class="header" href="#out"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/out/"><code>$out</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$out: &quot;results&quot;
   ⬇︎      
db.results.find()   
{_id: '◐', x: '■', y: '▲', val: 10, ord: 0}
{_id: '◑', x: '■', y: '■', val: 60}
{_id: '◒', x: '●', y: '■', val: 80}
{_id: '◓', x: '▲', y: '▲', val: 85}
{_id: '◔', x: '■', y: '▲', val: 90}
{_id: '◕', x: '●', y: '■', val: 95, ord: 100}   
</code></pre>
<p> </p>
<hr />
<p><a name="stage_project"></a></p>
<h2 id="project"><a class="header" href="#project"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/project/"><code>$project</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$project: {x: 1}
   ⬇︎      
{_id: '◐', x: '■'}
{_id: '◑', x: '■'}
{_id: '◒', x: '●'}
{_id: '◓', x: '▲'}
{_id: '◔', x: '■'}
{_id: '◕', x: '●'}   
</code></pre>
<p> </p>
<hr />
<p><a name="stage_redact"></a></p>
<h2 id="redact"><a class="header" href="#redact"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/redact/"><code>$redact</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;Bigtown&quot;, loc: {type: &quot;Point&quot;, coordinates: [1,1]}}
{_id: &quot;Smalltown&quot;, loc: {type: &quot;Point&quot;, coordinates: [3,3]}}
{_id: &quot;Happytown&quot;, loc: {type: &quot;Point&quot;, coordinates: [5,5]}}
{_id: &quot;Sadtown&quot;, loc: {type: &quot;LineString&quot;, coordinates: [[7,7],[8,8]]}}
   ⬇︎      
$redact: {$cond: {
  if  : {$eq: [&quot;$type&quot;, &quot;LineString&quot;]},
  then: &quot;$$PRUNE&quot;,
  else: &quot;$$DESCEND&quot;
}}
   ⬇︎      
{_id: 'Bigtown', loc: { type: 'Point', coordinates: [1,1]}}
{_id: 'Smalltown', loc: { type: 'Point', coordinates: [3,3]}}
{_id: 'Happytown', loc: { type: 'Point', coordinates: [5,5]}}
{_id: 'Sadtown'}   
</code></pre>
<p> </p>
<hr />
<p><a name="stage_replaceRoot"></a></p>
<h2 id="replaceroot"><a class="header" href="#replaceroot"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/replaceRoot/"><code>$replaceRoot</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;▤&quot;, a: &quot;●&quot;, b: [&quot;◰&quot;, &quot;◱&quot;]}
{_id: &quot;▥&quot;, a: &quot;▲&quot;, b: [&quot;◲&quot;]}
{_id: &quot;▦&quot;, a: &quot;▲&quot;, b: [&quot;◰&quot;, &quot;◳&quot;, &quot;◱&quot;]}
{_id: &quot;▧&quot;, a: &quot;●&quot;, b: [&quot;◰&quot;]}
{_id: &quot;▨&quot;, a: &quot;■&quot;, b: [&quot;◳&quot;, &quot;◱&quot;]}
   ⬇︎      
$replaceRoot: {
  newRoot: {first: {$first: &quot;$b&quot;}, last: {$last: &quot;$b&quot;}}
}
   ⬇︎      
{first: '◰', last: '◱'}
{first: '◲', last: '◲'}
{first: '◰', last: '◱'}
{first: '◰', last: '◰'}
{first: '◳', last: '◱'}
</code></pre>
<p> </p>
<hr />
<p><a name="stage_replaceWith"></a></p>
<h2 id="replacewith"><a class="header" href="#replacewith"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/replaceWith/"><code>$replaceWith</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;▤&quot;, a: &quot;●&quot;, b: [&quot;◰&quot;, &quot;◱&quot;]}
{_id: &quot;▥&quot;, a: &quot;▲&quot;, b: [&quot;◲&quot;]}
{_id: &quot;▦&quot;, a: &quot;▲&quot;, b: [&quot;◰&quot;, &quot;◳&quot;, &quot;◱&quot;]}
{_id: &quot;▧&quot;, a: &quot;●&quot;, b: [&quot;◰&quot;]}
{_id: &quot;▨&quot;, a: &quot;■&quot;, b: [&quot;◳&quot;, &quot;◱&quot;]}
   ⬇︎      
$replaceWith: {
  first: {$first: &quot;$b&quot;}, last: {$last: &quot;$b&quot;}
}
   ⬇︎      
{first: '◰', last: '◱'}
{first: '◲', last: '◲'}
{first: '◰', last: '◱'}
{first: '◰', last: '◰'}
{first: '◳', last: '◱'}
</code></pre>
<p> </p>
<hr />
<p><a name="stage_sample"></a></p>
<h2 id="sample"><a class="header" href="#sample"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/sample/"><code>$sample</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$sample: {size: 3}
   ⬇︎     
{_id: '◔', x: '■', y: '▲', val: 90}
{_id: '◓', x: '▲', y: '▲', val: 85}
{_id: '◑', x: '■', y: '■', val: 60}
</code></pre>
<p> </p>
<hr />
<p><a name="stage_search"></a></p>
<h2 id="search"><a class="header" href="#search"><a href="https://www.mongodb.com/docs/atlas/atlas-search/query-syntax/#-search"><code>$search</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;Bigtown&quot;, loc: {type: &quot;Point&quot;, coordinates: [1,1]}}
{_id: &quot;Smalltown&quot;, loc: {type: &quot;Point&quot;, coordinates: [3,3]}}
{_id: &quot;Happytown&quot;, loc: {type: &quot;Point&quot;, coordinates: [5,5]}}
{_id: &quot;Sadtown&quot;, loc: {type: &quot;LineString&quot;, coordinates: [[7,7],[8,8]]}}
   ⬇︎      
$search: {
  text: {
    path: &quot;_id&quot;,
    query: &quot;Bigtown Happytown&quot;
  }
}
   ⬇︎      
{_id: 'Bigtown', loc: {type: 'Point', coordinates: [1, 1]}}
{_id: 'Happytown', loc: {type: 'Point', coordinates: [5, 5]}}
</code></pre>
<p> </p>
<hr />
<p><a name="stage_searchmeta"></a></p>
<h2 id="searchmeta"><a class="header" href="#searchmeta"><a href="https://www.mongodb.com/docs/atlas/atlas-search/query-syntax/#-searchmeta"><code>$searchMeta</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;Bigtown&quot;, loc: {type: &quot;Point&quot;, coordinates: [1,1]}}
{_id: &quot;Smalltown&quot;, loc: {type: &quot;Point&quot;, coordinates: [3,3]}}
{_id: &quot;Happytown&quot;, loc: {type: &quot;Point&quot;, coordinates: [5,5]}}
{_id: &quot;Sadtown&quot;, loc: {type: &quot;LineString&quot;, coordinates: [[7,7],[8,8]]}}
   ⬇︎      
$searchMeta: {
  facet: {
    operator: {
      exists: {
        path: &quot;_id&quot;
      }      
    },   
    facets: {        
      geotypes: {
        type: &quot;string&quot;,
        path: &quot;loc.type&quot;,
        numBuckets : 2
      }            
    }        
  }             
}
   ⬇︎      
{
  count: {lowerBound: 4},
  facet: {
    geotypes: {
      buckets: [
        {_id: 'Point', count: 3},
        {_id: 'LineString', count: 1}
      ]
    }
  }
}
</code></pre>
<p> </p>
<hr />
<p><a name="stage_set"></a></p>
<h2 id="set"><a class="header" href="#set"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/set/"><code>$set</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$set: {y: &quot;▲&quot;}
   ⬇︎      
{_id: '◐', x: '■', y: '▲', val: 10, ord: 0}
{_id: '◑', x: '■', y: '▲', val: 60}
{_id: '◒', x: '●', y: '▲', val: 80}
{_id: '◓', x: '▲', y: '▲', val: 85}
{_id: '◔', x: '■', y: '▲', val: 90}
{_id: '◕', x: '●', y: '▲', val: 95, ord: 100}        
</code></pre>
<p> </p>
<hr />
<p><a name="stage_setWindowFields"></a></p>
<h2 id="setwindowfields"><a class="header" href="#setwindowfields"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/setWindowFields/"><code>$setWindowFields</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$setWindowFields: {
  partitionBy: &quot;$x&quot;,
  sortBy: {&quot;_id&quot;: 1},    
  output: {
    cumulativeValShapeX: {
      $sum: &quot;$val&quot;,
      window: {
        documents: [&quot;unbounded&quot;, &quot;current&quot;]
      }
    }
 }
}
   ⬇︎      
{_id: '◐', x: '■', y: '▲', val: 10, ord: 0, cumulativeValShapeX: 10}
{_id: '◑', x: '■', y: '■', val: 60, cumulativeValShapeX: 70}
{_id: '◔', x: '■', y: '▲', val: 90, cumulativeValShapeX: 160}
{_id: '◓', x: '▲', y: '▲', val: 85, cumulativeValShapeX: 85}
{_id: '◒', x: '●', y: '■', val: 80, cumulativeValShapeX: 80}
{_id: '◕', x: '●', y: '■', val: 95, ord: 100, cumulativeValShapeX: 175}
</code></pre>
<p> </p>
<hr />
<p><a name="stage_skip"></a></p>
<h2 id="skip"><a class="header" href="#skip"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/skip/"><code>$skip</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$skip: 5
   ⬇︎      
{_id: '◕', x: '●', y: '■', val: 95, ord: 100}
</code></pre>
<p> </p>
<hr />
<p><a name="stage_sort"></a></p>
<h2 id="sort"><a class="header" href="#sort"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/sort/"><code>$sort</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$sort: {x: 1, y: 1}
   ⬇︎      
{_id: '◑', x: '■', y: '■', val: 60}
{_id: '◐', x: '■', y: '▲', val: 10, ord: 0}
{_id: '◔', x: '■', y: '▲', val: 90}
{_id: '◓', x: '▲', y: '▲', val: 85}
{_id: '◒', x: '●', y: '■', val: 80}
{_id: '◕', x: '●', y: '■', val: 95, ord: 100}   
</code></pre>
<p> </p>
<hr />
<p><a name="stage_sortByCount"></a></p>
<h2 id="sortbycount"><a class="header" href="#sortbycount"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/sortByCount/"><code>$sortByCount</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$sortByCount: &quot;$x&quot;
   ⬇︎    
{_id: '■', count: 3}
{_id: '●', count: 2}
{_id: '▲', count: 1}     
</code></pre>
<p> </p>
<hr />
<p><a name="stage_unionWith"></a></p>
<h2 id="unionwith"><a class="header" href="#unionwith"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/unionWith/"><code>$unionWith</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ✚
{_id: &quot;▤&quot;, a: &quot;●&quot;, b: [&quot;◰&quot;, &quot;◱&quot;]}
{_id: &quot;▥&quot;, a: &quot;▲&quot;, b: [&quot;◲&quot;]}
{_id: &quot;▦&quot;, a: &quot;▲&quot;, b: [&quot;◰&quot;, &quot;◳&quot;, &quot;◱&quot;]}
{_id: &quot;▧&quot;, a: &quot;●&quot;, b: [&quot;◰&quot;]}
{_id: &quot;▨&quot;, a: &quot;■&quot;, b: [&quot;◳&quot;, &quot;◱&quot;]}
   ⬇︎      
$unionWith: {coll: &quot;lists&quot;} 
   ⬇︎    
{_id: '◐', x: '■', y: '▲', val: 10, ord: 0}
{_id: '◑', x: '■', y: '■', val: 60}
{_id: '◒', x: '●', y: '■', val: 80}
{_id: '◓', x: '▲', y: '▲', val: 85}
{_id: '◔', x: '■', y: '▲', val: 90}
{_id: '◕', x: '●', y: '■', val: 95, ord: 100}
{_id: '▤', a: '●', b: ['◰', '◱']}
{_id: '▥', a: '▲', b: ['◲']}
{_id: '▦', a: '▲', b: ['◰', '◳', '◱']}
{_id: '▧', a: '●', b: ['◰']}
{_id: '▨', a: '■', b: ['◳', '◱']}     
</code></pre>
<p> </p>
<hr />
<p><a name="stage_unset"></a></p>
<h2 id="unset"><a class="header" href="#unset"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/unset/"><code>$unset</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;◐&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 10, ord: 0}
{_id: &quot;◑&quot;, x: &quot;■&quot;, y: &quot;■&quot;, val: 60}
{_id: &quot;◒&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 80}
{_id: &quot;◓&quot;, x: &quot;▲&quot;, y: &quot;▲&quot;, val: 85}
{_id: &quot;◔&quot;, x: &quot;■&quot;, y: &quot;▲&quot;, val: 90}
{_id: &quot;◕&quot;, x: &quot;●&quot;, y: &quot;■&quot;, val: 95, ord: 100}
   ⬇︎      
$unset: [&quot;x&quot;] 
   ⬇︎  
{_id: '◐', y: '▲', val: 10, ord: 0}
{_id: '◑', y: '■', val: 60}
{_id: '◒', y: '■', val: 80}
{_id: '◓', y: '▲', val: 85}
{_id: '◔', y: '▲', val: 90}
{_id: '◕', y: '■', val: 95, ord: 100}       
</code></pre>
<p> </p>
<hr />
<p><a name="stage_unwind"></a></p>
<h2 id="unwind"><a class="header" href="#unwind"><a href="https://www.mongodb.com/docs/manual/reference/operator/aggregation/unwind/"><code>$unwind</code></a></a></h2>
<pre><code class="language-javascript">{_id: &quot;▤&quot;, a: &quot;●&quot;, b: [&quot;◰&quot;, &quot;◱&quot;]}
{_id: &quot;▥&quot;, a: &quot;▲&quot;, b: [&quot;◲&quot;]}
{_id: &quot;▦&quot;, a: &quot;▲&quot;, b: [&quot;◰&quot;, &quot;◳&quot;, &quot;◱&quot;]}
{_id: &quot;▧&quot;, a: &quot;●&quot;, b: [&quot;◰&quot;]}
{_id: &quot;▨&quot;, a: &quot;■&quot;, b: [&quot;◳&quot;, &quot;◱&quot;]}
   ⬇︎      
$unwind: {path: &quot;$b&quot;}
   ⬇︎      
{_id: '▤', a: '●', b: '◰'}
{_id: '▤', a: '●', b: '◱'}
{_id: '▥', a: '▲', b: '◲'}
{_id: '▦', a: '▲', b: '◰'}
{_id: '▦', a: '▲', b: '◳'}
{_id: '▦', a: '▲', b: '◱'}
{_id: '▧', a: '●', b: '◰'}
{_id: '▨', a: '■', b: '◳'}
{_id: '▨', a: '■', b: '◱'}   
</code></pre>
<p> </p>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="stages-cheatsheet-source"><a class="header" href="#stages-cheatsheet-source">Stages Cheatsheet Source</a></h1>
<p>To test all the aggregation stage examples shown in the <a href="appendices/cheatsheet.html">Cheatsheet</a>, run the following JavaScript from the MongoDB Shell connected to a MongoDB database.</p>
<h2 id="collections-configuration--data-population"><a class="header" href="#collections-configuration--data-population">Collections Configuration &amp; Data Population</a></h2>
<pre><code class="language-javascript">// DB configuration
use cheatsheet;
db.dropDatabase();
db.places.createIndex({&quot;loc&quot;: &quot;2dsphere&quot;});

// 'shapes' collection
db.shapes.insertMany([
  {&quot;_id&quot;: &quot;◐&quot;, &quot;x&quot;: &quot;■&quot;, &quot;y&quot;: &quot;▲&quot;, &quot;val&quot;: 10, &quot;ord&quot;: 0},
  {&quot;_id&quot;: &quot;◑&quot;, &quot;x&quot;: &quot;■&quot;, &quot;y&quot;: &quot;■&quot;, &quot;val&quot;: 60},
  {&quot;_id&quot;: &quot;◒&quot;, &quot;x&quot;: &quot;●&quot;, &quot;y&quot;: &quot;■&quot;, &quot;val&quot;: 80},
  {&quot;_id&quot;: &quot;◓&quot;, &quot;x&quot;: &quot;▲&quot;, &quot;y&quot;: &quot;▲&quot;, &quot;val&quot;: 85},
  {&quot;_id&quot;: &quot;◔&quot;, &quot;x&quot;: &quot;■&quot;, &quot;y&quot;: &quot;▲&quot;, &quot;val&quot;: 90},
  {&quot;_id&quot;: &quot;◕&quot;, &quot;x&quot;: &quot;●&quot;, &quot;y&quot;: &quot;■&quot;, &quot;val&quot;: 95, &quot;ord&quot;: 100},
]);

// 'lists' collection
db.lists.insertMany([
  {&quot;_id&quot;: &quot;▤&quot;, &quot;a&quot;: &quot;●&quot;, &quot;b&quot;: [&quot;◰&quot;, &quot;◱&quot;]},
  {&quot;_id&quot;: &quot;▥&quot;, &quot;a&quot;: &quot;▲&quot;, &quot;b&quot;: [&quot;◲&quot;]},
  {&quot;_id&quot;: &quot;▦&quot;, &quot;a&quot;: &quot;▲&quot;, &quot;b&quot;: [&quot;◰&quot;, &quot;◳&quot;, &quot;◱&quot;]},
  {&quot;_id&quot;: &quot;▧&quot;, &quot;a&quot;: &quot;●&quot;, &quot;b&quot;: [&quot;◰&quot;]},
  {&quot;_id&quot;: &quot;▨&quot;, &quot;a&quot;: &quot;■&quot;, &quot;b&quot;: [&quot;◳&quot;, &quot;◱&quot;]},
]);

// 'places' collection
db.places.insertMany([
  {&quot;_id&quot;: &quot;Bigtown&quot;, &quot;loc&quot;: {&quot;type&quot;: &quot;Point&quot;, &quot;coordinates&quot;: [1,1]}},
  {&quot;_id&quot;: &quot;Smalltown&quot;, &quot;loc&quot;: {&quot;type&quot;: &quot;Point&quot;, &quot;coordinates&quot;: [3,3]}},
  {&quot;_id&quot;: &quot;Happytown&quot;, &quot;loc&quot;: {&quot;type&quot;: &quot;Point&quot;, &quot;coordinates&quot;: [5,5]}},
  {&quot;_id&quot;: &quot;Sadtown&quot;, &quot;loc&quot;: {&quot;type&quot;: &quot;LineString&quot;, &quot;coordinates&quot;: [[7,7],[8,8]]}},
]);
</code></pre>
<h2 id="aggregation-stage-examples-execution"><a class="header" href="#aggregation-stage-examples-execution">Aggregation Stage Examples Execution</a></h2>
<blockquote>
<p><em>If you are running a MongoDB version earlier than 6.0, and if you have not configured an <a href="appendices/cheatsheet-source.html#configuring-the-required-atlas-search-index">Atlas Search index</a>, some of the unsupported stages in the code you execute below will show an error or empty result. A code comment marks each stage with the minimum necessary MongoDB version and indicates if the stage requires an Atlas Search index.</em></p>
</blockquote>
<pre><code class="language-javascript">// $addFields  (v3.4)
db.shapes.aggregate([
  {&quot;$addFields&quot;: {&quot;z&quot;: &quot;●&quot;}}
]);


// $bucket  (v3.4)
db.shapes.aggregate([
  {&quot;$bucket&quot;: {
    &quot;groupBy&quot;: &quot;$val&quot;, &quot;boundaries&quot;: [0, 25, 50, 75, 100], &quot;default&quot;: &quot;Other&quot;
  }}
]);


// $bucketAuto  (v3.4)
db.shapes.aggregate([
  {&quot;$bucketAuto&quot;: {&quot;groupBy&quot;: &quot;$val&quot;, &quot;buckets&quot;: 3}}
]);


// $count  (v3.4)
db.shapes.aggregate([
  {&quot;$count&quot;: &quot;amount&quot;}
]);


// $densify  (v5.1)
db.shapes.aggregate([
  {&quot;$densify&quot;: {
    &quot;field&quot;: &quot;val&quot;, 
    &quot;partitionByFields&quot;: [&quot;x&quot;], 
    &quot;range&quot;: {&quot;bounds&quot;: &quot;full&quot;, &quot;step&quot;: 25}
  }}
]);


// $documents  (v5.1)
db.aggregate([
  {&quot;$documents&quot;: [
    {&quot;p&quot;: &quot;▭&quot;, &quot;q&quot;: &quot;▯&quot;},
    {&quot;p&quot;: &quot;▯&quot;, &quot;q&quot;: &quot;▭&quot;},
  ]}
]);


// $facet  (v3.4)
db.shapes.aggregate([
  {&quot;$facet&quot;: {
    &quot;X_CIRCLE_FACET&quot;: [{&quot;$match&quot;: {&quot;x&quot;: &quot;●&quot;}}],
    &quot;FIRST_TWO_FACET&quot; : [{&quot;$limit&quot;: 2}],
  }}
]);


// $fill  (v5.3)
db.shapes.aggregate([
  {&quot;$fill&quot;: {
    &quot;sortBy&quot;: {&quot;val&quot;: 1},        
    &quot;output&quot;: {
      &quot;ord&quot;: {&quot;method&quot;: &quot;linear&quot;}
    }
  }}
]);


// $geoNear  (v2.2)
db.places.aggregate([
  {&quot;$geoNear&quot;: {
    &quot;near&quot;: {&quot;type&quot;: &quot;Point&quot;, &quot;coordinates&quot;: [9,9]}, &quot;distanceField&quot;: &quot;distance&quot;
  }}
]);


// $graphLookup  (v3.4)
db.shapes.aggregate([
  {&quot;$graphLookup&quot;: {
    &quot;from&quot;: &quot;shapes&quot;,
    &quot;startWith&quot;: &quot;$x&quot;,
    &quot;connectFromField&quot;: &quot;x&quot;,
    &quot;connectToField&quot;: &quot;y&quot;,
    &quot;depthField&quot;: &quot;depth&quot;,
    &quot;as&quot;: &quot;connections&quot;,
  }},
  {&quot;$project&quot;: {&quot;connections_count&quot;: {&quot;$size&quot;: &quot;$connections&quot;}}}
]);


// $group  (v2.2)
db.shapes.aggregate([
  {&quot;$group&quot;: {&quot;_id&quot;: &quot;$x&quot;, &quot;ylist&quot;: {&quot;$push&quot;: &quot;$y&quot;}}}
]);


// $limit  (v2.2)
db.shapes.aggregate([
  {&quot;$limit&quot;: 2}
]);


// $lookup  (v3.2)
db.shapes.aggregate([
  {&quot;$lookup&quot;: {
    &quot;from&quot;: &quot;lists&quot;,
    &quot;localField&quot;: &quot;y&quot;,
    &quot;foreignField&quot;: &quot;a&quot;,
    &quot;as&quot;: &quot;refs&quot;,
  }}
]);


// $match  (v2.2)
db.shapes.aggregate([
  {&quot;$match&quot;: {&quot;y&quot;: &quot;▲&quot;}  
}]);


// $merge  (v4.2)
db.results.drop();
db.shapes.aggregate([
  {&quot;$merge&quot;: {&quot;into&quot;: &quot;results&quot;}}
]);
db.results.find();


// $out  (v2.6)
db.results.drop();
db.shapes.aggregate([
  {&quot;$out&quot;: &quot;results&quot;}
]);
db.results.find();


// $project  (v2.2)
db.shapes.aggregate([
  {&quot;$project&quot;: {&quot;x&quot;: 1}}
]);


// $redact  (v2.6)
db.places.aggregate([
  {&quot;$redact&quot;: {&quot;$cond&quot;: {
    &quot;if&quot;  : {&quot;$eq&quot;: [&quot;$type&quot;, &quot;LineString&quot;]},
    &quot;then&quot;: &quot;$$PRUNE&quot;,
    &quot;else&quot;: &quot;$$DESCEND&quot;
  }}}
]);


// $replaceRoot  (v3.4)
db.lists.aggregate([
  {&quot;$replaceRoot&quot;: {&quot;newRoot&quot;: {&quot;first&quot;: {&quot;$first&quot;: &quot;$b&quot;}, &quot;last&quot;: {&quot;$last&quot;: &quot;$b&quot;}}}}
]);


// $replaceWith  (v4.2)
db.lists.aggregate([
  {&quot;$replaceWith&quot;: {&quot;first&quot;: {&quot;$first&quot;: &quot;$b&quot;}, &quot;last&quot;: {&quot;$last&quot;: &quot;$b&quot;}}}
]);


// $sample  (v3.2)
db.shapes.aggregate([
  {&quot;$sample&quot;: {&quot;size&quot;: 3}}
]);


// $search  (v4.2 - requires an Atlas Search index)
db.places.aggregate([
  {&quot;$search&quot;: {
    &quot;text&quot;: {
      &quot;path&quot;: &quot;_id&quot;,
      &quot;query&quot;: &quot;Bigtown Happytown&quot;,
    }
  }}
]);


// $searchMeta  (v4.2 - requires an Atlas Search index)
db.places.aggregate([
  {&quot;$searchMeta&quot;: {
   &quot;facet&quot;: {
      &quot;operator&quot;: {
        &quot;exists&quot;: {
          &quot;path&quot;: &quot;_id&quot;
        }      
      },   
      &quot;facets&quot;: {        
        &quot;geotypes&quot;: {
          &quot;type&quot; : &quot;string&quot;,
          &quot;path&quot; : &quot;loc.type&quot;,
          &quot;numBuckets&quot; : 2,
        }            
      }        
    }             
  }}
]);


// $set  (v4.2)
db.shapes.aggregate([
  {&quot;$set&quot;: {&quot;y&quot;: &quot;▲&quot;}}
]);


// $setWindowFields  (v5.0)
db.shapes.aggregate([
  {&quot;$setWindowFields&quot;: {
    &quot;partitionBy&quot;: &quot;$x&quot;,
    &quot;sortBy&quot;: {&quot;_id&quot;: 1},    
    &quot;output&quot;: {
      &quot;cumulativeValShapeX&quot;: {
        &quot;$sum&quot;: &quot;$val&quot;,
        &quot;window&quot;: {
          &quot;documents&quot;: [&quot;unbounded&quot;, &quot;current&quot;]
        }
      }
    }
  }}
]);


// $skip  (v2.2)
db.shapes.aggregate([
  {&quot;$skip&quot;: 5}
]);


// $sort  (v2.2)
db.shapes.aggregate([
  {&quot;$sort&quot;: {&quot;x&quot;: 1, &quot;y&quot;: 1}}
]);


// $sortByCount  (v3.4)
db.shapes.aggregate([
  {&quot;$sortByCount&quot;: &quot;$x&quot;}
]);


// $unionWith  (v4.4)
db.shapes.aggregate([
  {&quot;$unionWith&quot;: {&quot;coll&quot;: &quot;lists&quot;}}
]);


// $unset  (v4.2)
db.shapes.aggregate([
  {&quot;$unset&quot;: [&quot;x&quot;]}
]);


// $unwind  (v2.2)
db.lists.aggregate([
  {&quot;$unwind&quot;: {&quot;path&quot;: &quot;$b&quot;}}
]);
</code></pre>
<h2 id="configuring-the-required-atlas-search-index"><a class="header" href="#configuring-the-required-atlas-search-index">Configuring The Required Atlas Search Index</a></h2>
<p>The <code>$search</code> and <code>$searchMeta</code> stages require you to first configure an <a href="https://www.mongodb.com/docs/atlas/atlas-search/atlas-search-overview/">Atlas Search</a> index. Follow the procedure described in the <a href="appendices/./create-search-index.html">Create Atlas Search Index</a> appendix to define a search index for the collection <strong>cheatsheet.places</strong>, with the following JSON configuration:</p>
<pre><code class="language-javascript">{
  &quot;mappings&quot;: {
    &quot;dynamic&quot;: true,
    &quot;fields&quot;: {
      &quot;loc&quot;: {
        &quot;fields&quot;: {
          &quot;type&quot;: {
            &quot;type&quot;: &quot;stringFacet&quot;
          }
        },
        &quot;type&quot;: &quot;document&quot;
      }
    }
  }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="create-atlas-search-index"><a class="header" href="#create-atlas-search-index">Create Atlas Search Index</a></h1>
<p>For this book's <a href="appendices/../examples/full-text-search/full-text-search.html">Full-Text Search Examples</a>, you need to use an Atlas Cluster rather than a self-managed MongoDB deployment. The simplest way to provision an Atlas Cluster is to <a href="https://www.mongodb.com/cloud/atlas">create a Free Tier Cluster</a>. Once created, use the steps below whenever a <em>Full-Text Search Examples</em> chapter asks you to create a search index. </p>
<hr />
<p> </p>
<p>First, in the <a href="https://cloud.mongodb.com/">Atlas console</a> for your database cluster, click the <strong>Search tab</strong> and then click <strong>Create Search Index</strong>:</p>
<p><img src="appendices/./pics/create-search-index.png" alt="Atlas Search - Create Index" /></p>
<p> </p>
<p>Select <strong>JSON Editor</strong> and then click <strong>Next</strong>:</p>
<p><img src="appendices/./pics/editor.png" alt="Atlas Search - JSON Editor" /></p>
<p> </p>
<p>Leave <strong>Index Name</strong> as &quot;default&quot;, select the <strong>database</strong> and <strong>collection</strong> you require, paste in your JSON index definition and then click <strong>Next</strong>:</p>
<p><img src="appendices/./pics/json-def.png" alt="Atlas Search - JSON definition" /></p>
<p> </p>
<p>In the review screen, click <strong>Create Search Index</strong> to finish:</p>
<p><img src="appendices/./pics/review.png" alt="Atlas Search - Initial Review" /></p>
<p>It may take a few minutes for the system to generate the text search index.</p>
<p> </p>
<blockquote>
<p><em>Note that it is also possible for you to automate creating a search index by using the <a href="https://www.mongodb.com/docs/atlas/reference/api/fts-indexes-create-one/#examples">Atlas Admin API</a> in MongoDB version 4.2 and greater, or by using the <strong>createSearchIndexes command</strong> in MongoDB version 7.0 and greater.</em></p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="book-version-history"><a class="header" href="#book-version-history">Book Version History</a></h1>
<p>A summary of the significant additions in each major version of this book.</p>
<p><strong>Version 7.0 (released in July 2023)</strong></p>
<ul>
<li><a href="appendices/../examples/array-manipulations/array-fields-joining.html">Array Fields Joining</a> example chapter</li>
<li><a href="appendices/../examples/securing-data/role-programmatic-restricted-view.html">Role Programmatic Restricted View</a> example chapter</li>
</ul>
<p><em>No version 6.0 existed - skipped straight to version 7.0 to align with the latest MongoDB version</em></p>
<p><strong>Version 5.0 (released in November 2022)</strong></p>
<ul>
<li><a href="appendices/../examples/time-series/state-change-boundaries.html">State Change Boundaries</a> example chapter</li>
<li><a href="appendices/../examples/array-manipulations/array-element-grouping.html">Array Element Grouping</a> example chapter</li>
<li><a href="appendices/../examples/array-manipulations/comparison-of-two-arrays.html">Comparison Of Two Arrays</a> example chapter</li>
</ul>
<p><strong>Version 4.0 (released in June 2022)</strong></p>
<ul>
<li><a href="appendices/../examples/full-text-search/compound-text-search.html">Compound Text Search Criteria</a> example chapter</li>
<li><a href="appendices/../examples/full-text-search/facets-and-counts-text-search.html">Facets And Counts Text Search</a> example chapter</li>
<li><a href="appendices/create-search-index.html">Create Atlas Search Index</a> appendix</li>
</ul>
<p><strong>Version 3.0 (released in October 2021)</strong></p>
<ul>
<li><a href="appendices/../guides/advanced-arrays.html">Advanced Use Of Expressions For Array Processing</a> guide chapter</li>
<li><a href="appendices/../examples/array-manipulations/array-high-low-avg.html">Summarising Arrays For First, Last, Min, Max &amp; Average</a> example chapter</li>
<li><a href="appendices/../examples/array-manipulations/pivot-array-items.html">Pivot Array Items By A Key</a> example chapter</li>
<li><a href="appendices/../examples/array-manipulations/array-sort-percentiles.html">Array Sorting &amp; Percentiles</a> example chapter</li>
</ul>
<p><strong>Version 2.0 (released in July 2021)</strong></p>
<ul>
<li><a href="appendices/../guides/sharding.html">Sharding Considerations</a> guide chapter</li>
<li><a href="appendices/../examples/foundational/distinct-values.html">Distinct List Of Values</a> example chapter</li>
<li><a href="appendices/../examples/time-series/iot-power-consumption.html">IoT Power Consumption</a> example chapter</li>
<li><a href="appendices/cheatsheet.html">Stages Cheatsheet</a> appendix</li>
<li><a href="appendices/cheatsheet-source.html">Stages Cheatsheet Source</a> appendix</li>
<li><a href="appendices/book-history.html">Book Version History</a> appendix</li>
</ul>
<p><strong>Version 1.0 (released in May 2021)</strong></p>
<ul>
<li><a href="appendices/../intro/introducing-aggregations.html">Introducing MongoDB Aggregations</a> chapter</li>
<li><a href="appendices/../intro/history.html">History Of MongoDB Aggregations</a> chapter</li>
<li><a href="appendices/../intro/getting-started.html">Getting Started</a> chapter</li>
<li><a href="appendices/../intro/getting-help.html">Getting Help</a> chapter</li>
<li><a href="appendices/../guides/composibility.html">Embrace Composability For Increased Productivity</a> guide chapter</li>
<li><a href="appendices/../guides/project.html">Better Alternatives To A Project Stage</a> guide chapter</li>
<li><a href="appendices/../guides/explain.html">Using Explain Plans</a> guide chapter</li>
<li><a href="appendices/../guides/performance.html">Pipeline Performance Considerations</a> guide chapter</li>
<li><a href="appendices/../guides/expressions.html">Expressions Explained guide</a> chapter</li>
<li><a href="appendices/../examples/foundational/filtered-top-subset.html">Filtered Top Subset</a> example chapter</li>
<li><a href="appendices/../examples/foundational/group-and-total.html">Group &amp; Total</a> example chapter</li>
<li><a href="appendices/../examples/foundational/unpack-array-group-differently.html">Unpack Arrays &amp; Group Differently</a> example chapter</li>
<li><a href="appendices/../examples/joining/one-to-one-join.html">One-to-One Join</a> example chapter</li>
<li><a href="appendices/../examples/joining/multi-one-to-many.html">Multi-Field Join &amp; One-to-Many</a> example chapter</li>
<li><a href="appendices/../examples/type-convert/convert-to-strongly-typed.html">Strongly-Typed Conversion</a> example chapter</li>
<li><a href="appendices/../examples/type-convert/convert-incomplete-dates.html">Convert Incomplete Date Strings</a> example chapter</li>
<li><a href="appendices/../examples/trend-analysis/faceted-classifications.html">Faceted Classification</a> example chapter</li>
<li><a href="appendices/../examples/trend-analysis/largest-graph-network.html">Largest Graph Network</a> example chapter</li>
<li><a href="appendices/../examples/trend-analysis/incremental-analytics.html">Incremental Analytics</a> example chapter</li>
<li><a href="appendices/../examples/securing-data/redacted-view.html">Redacted View</a> example chapter</li>
<li><a href="appendices/../examples/securing-data/mask-sensitive-fields.html">Mask Sensitive Fields</a> example chapter</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p><img src="./pics/back-cover.png" alt="Practical MongoDB Aggregations book back cover" /></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
